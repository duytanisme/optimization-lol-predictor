{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c10b8874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1686b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = Path(\"..\")\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "SOURCE_DIR = DATA_DIR / \"source\"\n",
    "OUTPUT_DIR = DATA_DIR / \"output\"\n",
    "INPUT_PATH = SOURCE_DIR / \"OE_Public_Match_Data_cleaned.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d20d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>firsttothreetowers</th>\n",
       "      <th>firstbaron</th>\n",
       "      <th>xpdiffat</th>\n",
       "      <th>firstmidtower</th>\n",
       "      <th>golddiffat</th>\n",
       "      <th>diffkillsat</th>\n",
       "      <th>firstdragon</th>\n",
       "      <th>diffdeathsat</th>\n",
       "      <th>firsttower</th>\n",
       "      <th>firstblood</th>\n",
       "      <th>...</th>\n",
       "      <th>side</th>\n",
       "      <th>diffassistsat</th>\n",
       "      <th>split_Regular</th>\n",
       "      <th>split_Special</th>\n",
       "      <th>playoffs</th>\n",
       "      <th>firstherald</th>\n",
       "      <th>game</th>\n",
       "      <th>patch</th>\n",
       "      <th>minute</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-463.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-119.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1272.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>825.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2036.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4919.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4854.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   firsttothreetowers  firstbaron  xpdiffat  firstmidtower  golddiffat  \\\n",
       "0                 1.0         1.0    -463.0            1.0      -119.0   \n",
       "1                 1.0         1.0      63.0            1.0      1272.0   \n",
       "2                 1.0         1.0     825.0            1.0      2036.0   \n",
       "3                 1.0         1.0    4919.0            1.0      4854.0   \n",
       "4                 0.0         0.0     463.0            0.0       119.0   \n",
       "\n",
       "   diffkillsat  firstdragon  diffdeathsat  firsttower  firstblood  ...  side  \\\n",
       "0          1.0          0.0          -1.0         1.0         1.0  ...     1   \n",
       "1          2.0          0.0          -2.0         1.0         1.0  ...     1   \n",
       "2          2.0          0.0          -2.0         1.0         1.0  ...     1   \n",
       "3          6.0          0.0          -6.0         1.0         1.0  ...     1   \n",
       "4         -1.0          1.0           1.0         0.0         0.0  ...     0   \n",
       "\n",
       "   diffassistsat  split_Regular  split_Special  playoffs  firstherald  game  \\\n",
       "0            3.0              1              0         0          0.0   1.0   \n",
       "1            4.0              1              0         0          0.0   1.0   \n",
       "2            4.0              1              0         0          0.0   1.0   \n",
       "3           17.0              1              0         0          0.0   1.0   \n",
       "4           -3.0              1              0         0          0.0   1.0   \n",
       "\n",
       "   patch  minute  result  \n",
       "0   3.15      10       1  \n",
       "1   3.15      15       1  \n",
       "2   3.15      20       1  \n",
       "3   3.15      25       1  \n",
       "4   3.15      10       0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(INPUT_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32901ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (731240, 19)\n",
      "Target shape: (731240,)\n",
      "Weights shape: (731240,)\n",
      "Target distribution: [365792 365448]\n",
      "Weights stats - min: 10, max: 25, mean: 17.50\n",
      "Train set: (438744, 19), (438744,), weights: (438744,)\n",
      "Validation set: (146248, 19), (146248,), weights: (146248,)\n",
      "Test set: (146248, 19), (146248,), weights: (146248,)\n",
      "Train set: (438744, 19), (438744,), weights: (438744,)\n",
      "Validation set: (146248, 19), (146248,), weights: (146248,)\n",
      "Test set: (146248, 19), (146248,), weights: (146248,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [col for col in df.columns if col not in ['result', 'minute']]\n",
    "X = df[feature_columns].values\n",
    "y = df['result'].values\n",
    "weights = df['minute'].values  # Extract weights from minute column\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Weights shape: {weights.shape}\")\n",
    "print(f\"Target distribution: {np.bincount(y)}\")\n",
    "print(f\"Weights stats - min: {weights.min()}, max: {weights.max()}, mean: {weights.mean():.2f}\")\n",
    "\n",
    "# Split data including weights\n",
    "X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(\n",
    "    X, y, weights, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train, X_val, y_train, y_val, weights_train, weights_val = train_test_split(\n",
    "    X_train, y_train, weights_train, test_size=0.25, random_state=42, stratify=y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train set: {X_train_scaled.shape}, {y_train.shape}, weights: {weights_train.shape}\")\n",
    "print(f\"Validation set: {X_val_scaled.shape}, {y_val.shape}, weights: {weights_val.shape}\")\n",
    "print(f\"Test set: {X_test_scaled.shape}, {y_test.shape}, weights: {weights_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb562f0",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4393ce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression with weighted loss function.\n",
    "    \n",
    "    Implements: WeightedLogLoss(y, p, α) = - (1/Σαᵢ) Σ αᵢ(yᵢlog(pᵢ) + (1-yᵢ)log(1-pᵢ))\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        z = np.clip(z, -250, 250)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _compute_loss(self, X, y, sample_weights=None):\n",
    "        \"\"\"\n",
    "        Compute weighted logistic loss.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "            y: True labels\n",
    "            sample_weights: Sample weights (α in the formula)\n",
    "        \"\"\"\n",
    "        z = X @ self.weights + self.bias\n",
    "        predictions = self._sigmoid(z)\n",
    "        predictions = np.clip(predictions, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        if sample_weights is None:\n",
    "            # Standard unweighted loss\n",
    "            loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "        else:\n",
    "            # Weighted loss: - (1/Σαᵢ) Σ αᵢ(yᵢlog(pᵢ) + (1-yᵢ)log(1-pᵢ))\n",
    "            weighted_log_loss = sample_weights * (y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "            loss = -np.sum(weighted_log_loss) / np.sum(sample_weights)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def _compute_gradient(self, X, y, sample_weights=None):\n",
    "        \"\"\"\n",
    "        Compute gradients for weighted logistic regression.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "            y: True labels \n",
    "            sample_weights: Sample weights (α in the formula)\n",
    "        \"\"\"\n",
    "        z = X @ self.weights + self.bias\n",
    "        predictions = self._sigmoid(z)\n",
    "        \n",
    "        if sample_weights is None:\n",
    "            # Standard unweighted gradients\n",
    "            dw = (1 / len(y)) * X.T @ (predictions - y)\n",
    "            db = (1 / len(y)) * np.sum(predictions - y)\n",
    "        else:\n",
    "            # Weighted gradients: (1/Σαᵢ) Σ αᵢ(pᵢ - yᵢ)\n",
    "            weighted_errors = sample_weights * (predictions - y)\n",
    "            weight_sum = np.sum(sample_weights)\n",
    "            dw = X.T @ weighted_errors / weight_sum\n",
    "            db = np.sum(weighted_errors) / weight_sum\n",
    "            \n",
    "        return dw, db\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        z = X @ self.weights + self.bias\n",
    "        return self._sigmoid(z)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc17e61",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "228e4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 20000\n",
    "EARLY_STOP_THRESHOLD = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445e67b",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a4f802",
   "metadata": {},
   "source": [
    "## SGD fixed learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91541a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDOptimizer:    \n",
    "    def __init__(self, learning_rate=0.01, max_epochs=MAX_EPOCHS, early_stop_patience=50, \n",
    "                 early_stop_threshold=EARLY_STOP_THRESHOLD, log_interval=10, verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize SGD optimizer.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate (float): Fixed learning rate for gradient descent\n",
    "            max_epochs (int): Maximum number of training epochs\n",
    "            early_stop_patience (int): Number of epochs to wait for improvement before stopping\n",
    "            early_stop_threshold (float): Minimum improvement threshold for early stopping\n",
    "            log_interval (int): Log metrics every n epochs\n",
    "            verbose (bool): Whether to print training progress\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "        self.early_stop_patience = early_stop_patience\n",
    "        self.early_stop_threshold = early_stop_threshold\n",
    "        self.log_interval = log_interval\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Training history\n",
    "        self.loss_history = []\n",
    "        self.epoch_history = []\n",
    "        self.stopped_early = False\n",
    "        self.final_epoch = 0\n",
    "        \n",
    "    def optimize(self, model, X_train, y_train, X_val=None, y_val=None, \n",
    "                weights_train=None, weights_val=None):\n",
    "        \"\"\"\n",
    "        Optimize model parameters using SGD with early stopping.\n",
    "        \n",
    "        Args:\n",
    "            model: Model object with _compute_loss and _compute_gradient methods\n",
    "            X_train: Training features\n",
    "            y_train: Training targets\n",
    "            X_val: Validation features (optional)\n",
    "            y_val: Validation targets (optional)\n",
    "            weights_train: Training sample weights (optional)\n",
    "            weights_val: Validation sample weights (optional)\n",
    "            \n",
    "        Returns:\n",
    "            dict: Training history and statistics\n",
    "        \"\"\"\n",
    "        # Initialize weights if not already done\n",
    "        if model.weights is None:\n",
    "            model.weights = np.random.normal(0, 0.01, X_train.shape[1])\n",
    "            model.bias = 0.0\n",
    "            \n",
    "        # Track best loss for early stopping\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Starting SGD training with learning rate: {self.learning_rate}\")\n",
    "            print(f\"Early stopping: patience={self.early_stop_patience}, threshold={self.early_stop_threshold}\")\n",
    "            if weights_train is not None:\n",
    "                print(f\"Using weighted loss with training weights shape: {weights_train.shape}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.max_epochs):\n",
    "            # Compute loss and gradients (weighted if weights provided)\n",
    "            train_loss = model._compute_loss(X_train, y_train, weights_train)\n",
    "            dw, db = model._compute_gradient(X_train, y_train, weights_train)\n",
    "            \n",
    "            # Update parameters\n",
    "            model.weights -= self.learning_rate * dw\n",
    "            model.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Store history\n",
    "            self.loss_history.append(train_loss)\n",
    "            self.epoch_history.append(epoch)\n",
    "            \n",
    "            # Check for early stopping\n",
    "            if train_loss < best_loss - self.early_stop_threshold:\n",
    "                best_loss = train_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            # Log progress\n",
    "            if (epoch + 1) % self.log_interval == 0 and self.verbose:\n",
    "                val_loss_str = \"\"\n",
    "                if X_val is not None and y_val is not None:\n",
    "                    val_loss = model._compute_loss(X_val, y_val, weights_val)\n",
    "                    val_loss_str = f\", Val Loss: {val_loss:.6f}\"\n",
    "                    \n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(f\"Epoch {epoch+1:4d} | Train Loss: {train_loss:.6f}{val_loss_str} | \"\n",
    "                      f\"LR: {self.learning_rate:.4f} | Patience: {patience_counter:2d}/{self.early_stop_patience} | \"\n",
    "                      f\"Time: {elapsed_time:.2f}s\")\n",
    "                \n",
    "            # Early stopping check\n",
    "            if patience_counter >= self.early_stop_patience:\n",
    "                self.stopped_early = True\n",
    "                self.final_epoch = epoch + 1\n",
    "                if self.verbose:\n",
    "                    print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "                    print(f\"No improvement in loss for {self.early_stop_patience} epochs \"\n",
    "                          f\"(threshold: {self.early_stop_threshold})\")\n",
    "                break\n",
    "                \n",
    "        if not self.stopped_early:\n",
    "            self.final_epoch = self.max_epochs\n",
    "            if self.verbose:\n",
    "                print(f\"\\nTraining completed after {self.max_epochs} epochs\")\n",
    "                \n",
    "        training_time = time.time() - start_time\n",
    "        final_loss = self.loss_history[-1]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Final loss: {final_loss:.6f}\")\n",
    "            print(f\"Total training time: {training_time:.2f}s\")\n",
    "            print(f\"Average time per epoch: {training_time/self.final_epoch:.4f}s\")\n",
    "            \n",
    "        return {\n",
    "            'loss_history': self.loss_history.copy(),\n",
    "            'epoch_history': self.epoch_history.copy(),\n",
    "            'final_loss': final_loss,\n",
    "            'final_epoch': self.final_epoch,\n",
    "            'stopped_early': self.stopped_early,\n",
    "            'training_time': training_time,\n",
    "            'learning_rate': self.learning_rate\n",
    "        }\n",
    "    \n",
    "    def plot_loss_curve(self, title=\"SGD Loss Curve\"):\n",
    "        \"\"\"Plot the loss curve from training history.\"\"\"\n",
    "        if not self.loss_history:\n",
    "            print(\"No training history available. Run optimize() first.\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.epoch_history, self.loss_history, 'b-', linewidth=2, label='Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'{title} (LR: {self.learning_rate})')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        if self.stopped_early:\n",
    "            plt.axvline(x=self.final_epoch-1, color='r', linestyle='--', alpha=0.7, \n",
    "                       label=f'Early Stop (Epoch {self.final_epoch})')\n",
    "            plt.legend()\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a06679",
   "metadata": {},
   "source": [
    "## SGD backtracking line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8ede8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDBacktrackingOptimizer:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent with Backtracking Line Search.\n",
    "    \n",
    "    Features:\n",
    "    - Adaptive step size using Armijo line search condition\n",
    "    - Backtracking algorithm with configurable parameters\n",
    "    - Early stopping based on loss improvement threshold\n",
    "    - Configurable logging intervals\n",
    "    - Step size history tracking\n",
    "    - Support for weighted loss computation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_learning_rate=1.0, c1=1e-4, rho=0.5, max_epochs=MAX_EPOCHS, \n",
    "                 early_stop_patience=50, early_stop_threshold=EARLY_STOP_THRESHOLD, log_interval=10, \n",
    "                 max_backtrack_iter=20, verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize SGD Backtracking optimizer.\n",
    "        \n",
    "        Args:\n",
    "            initial_learning_rate (float): Initial step size for backtracking\n",
    "            c1 (float): Armijo condition parameter (0 < c1 < 1)\n",
    "            rho (float): Step size reduction factor (0 < rho < 1)\n",
    "            max_epochs (int): Maximum number of training epochs\n",
    "            early_stop_patience (int): Number of epochs to wait for improvement before stopping\n",
    "            early_stop_threshold (float): Minimum improvement threshold for early stopping\n",
    "            log_interval (int): Log metrics every n epochs\n",
    "            max_backtrack_iter (int): Maximum backtracking iterations per epoch\n",
    "            verbose (bool): Whether to print training progress\n",
    "        \"\"\"\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.c1 = c1  # Armijo parameter\n",
    "        self.rho = rho  # Step reduction factor\n",
    "        self.max_epochs = max_epochs\n",
    "        self.early_stop_patience = early_stop_patience\n",
    "        self.early_stop_threshold = early_stop_threshold\n",
    "        self.log_interval = log_interval\n",
    "        self.max_backtrack_iter = max_backtrack_iter\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Training history\n",
    "        self.loss_history = []\n",
    "        self.epoch_history = []\n",
    "        self.step_size_history = []\n",
    "        self.backtrack_iterations_history = []\n",
    "        self.stopped_early = False\n",
    "        self.final_epoch = 0\n",
    "        \n",
    "    def _armijo_condition(self, f_current, f_new, grad_dot_direction, step_size):\n",
    "        \"\"\"\n",
    "        Check Armijo sufficient decrease condition.\n",
    "        \n",
    "        Args:\n",
    "            f_current: Current function value\n",
    "            f_new: New function value after step\n",
    "            grad_dot_direction: Gradient dot product with search direction\n",
    "            step_size: Current step size\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if Armijo condition is satisfied\n",
    "        \"\"\"\n",
    "        return f_new <= f_current + self.c1 * step_size * grad_dot_direction\n",
    "    \n",
    "    def _backtracking_line_search(self, model, X, y, current_weights, current_bias, \n",
    "                                  dw, db, current_loss, sample_weights=None):\n",
    "        \"\"\"\n",
    "        Perform backtracking line search to find suitable step size.\n",
    "        \n",
    "        Args:\n",
    "            model: Model object\n",
    "            X: Input features\n",
    "            y: Target values\n",
    "            current_weights: Current weight parameters\n",
    "            current_bias: Current bias parameter\n",
    "            dw: Weight gradients\n",
    "            db: Bias gradient\n",
    "            current_loss: Current loss value\n",
    "            sample_weights: Sample weights for weighted loss computation\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (step_size, backtrack_iterations)\n",
    "        \"\"\"\n",
    "        step_size = self.initial_learning_rate\n",
    "        \n",
    "        # Compute gradient norm squared (for descent direction)\n",
    "        grad_norm_sq = np.sum(dw**2) + db**2\n",
    "        \n",
    "        # Search direction is negative gradient\n",
    "        direction_dot_grad = -grad_norm_sq\n",
    "        \n",
    "        backtrack_iter = 0\n",
    "        \n",
    "        for i in range(self.max_backtrack_iter):\n",
    "            # Try step with current step size\n",
    "            new_weights = current_weights - step_size * dw\n",
    "            new_bias = current_bias - step_size * db\n",
    "            \n",
    "            # Temporarily update model parameters to compute new loss\n",
    "            old_weights = model.weights.copy()\n",
    "            old_bias = model.bias\n",
    "            \n",
    "            model.weights = new_weights\n",
    "            model.bias = new_bias\n",
    "            \n",
    "            new_loss = model._compute_loss(X, y, sample_weights)\n",
    "            \n",
    "            # Restore original parameters\n",
    "            model.weights = old_weights\n",
    "            model.bias = old_bias\n",
    "            \n",
    "            # Check Armijo condition\n",
    "            if self._armijo_condition(current_loss, new_loss, direction_dot_grad, step_size):\n",
    "                break\n",
    "                \n",
    "            # Reduce step size\n",
    "            step_size *= self.rho\n",
    "            backtrack_iter += 1\n",
    "            \n",
    "        return step_size, backtrack_iter\n",
    "        \n",
    "    def optimize(self, model, X_train, y_train, X_val=None, y_val=None,\n",
    "                weights_train=None, weights_val=None):\n",
    "        \"\"\"\n",
    "        Optimize model parameters using SGD with backtracking line search.\n",
    "        \n",
    "        Args:\n",
    "            model: Model object with _compute_loss and _compute_gradient methods\n",
    "            X_train: Training features\n",
    "            y_train: Training targets\n",
    "            X_val: Validation features (optional)\n",
    "            y_val: Validation targets (optional)\n",
    "            weights_train: Training sample weights (optional)\n",
    "            weights_val: Validation sample weights (optional)\n",
    "            \n",
    "        Returns:\n",
    "            dict: Training history and statistics\n",
    "        \"\"\"\n",
    "        # Initialize weights if not already done\n",
    "        if model.weights is None:\n",
    "            model.weights = np.random.normal(0, 0.01, X_train.shape[1])\n",
    "            model.bias = 0.0\n",
    "            \n",
    "        # Track best loss for early stopping\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Starting SGD with Backtracking Line Search\")\n",
    "            print(f\"Initial step size: {self.initial_learning_rate}, c1: {self.c1}, rho: {self.rho}\")\n",
    "            print(f\"Early stopping: patience={self.early_stop_patience}, threshold={self.early_stop_threshold}\")\n",
    "            if weights_train is not None:\n",
    "                print(f\"Using weighted loss with training weights shape: {weights_train.shape}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.max_epochs):\n",
    "            # Compute loss and gradients (weighted if weights provided)\n",
    "            train_loss = model._compute_loss(X_train, y_train, weights_train)\n",
    "            dw, db = model._compute_gradient(X_train, y_train, weights_train)\n",
    "            \n",
    "            # Perform backtracking line search\n",
    "            step_size, backtrack_iter = self._backtracking_line_search(\n",
    "                model, X_train, y_train, model.weights, model.bias, dw, db, train_loss, weights_train\n",
    "            )\n",
    "            \n",
    "            # Update parameters with found step size\n",
    "            model.weights -= step_size * dw\n",
    "            model.bias -= step_size * db\n",
    "            \n",
    "            # Store history\n",
    "            self.loss_history.append(train_loss)\n",
    "            self.epoch_history.append(epoch)\n",
    "            self.step_size_history.append(step_size)\n",
    "            self.backtrack_iterations_history.append(backtrack_iter)\n",
    "            \n",
    "            # Check for early stopping\n",
    "            if train_loss < best_loss - self.early_stop_threshold:\n",
    "                best_loss = train_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            # Log progress\n",
    "            if (epoch + 1) % self.log_interval == 0 and self.verbose:\n",
    "                val_loss_str = \"\"\n",
    "                if X_val is not None and y_val is not None:\n",
    "                    val_loss = model._compute_loss(X_val, y_val, weights_val)\n",
    "                    val_loss_str = f\", Val Loss: {val_loss:.6f}\"\n",
    "                    \n",
    "                elapsed_time = time.time() - start_time\n",
    "                avg_step_size = np.mean(self.step_size_history[-self.log_interval:])\n",
    "                avg_backtrack = np.mean(self.backtrack_iterations_history[-self.log_interval:])\n",
    "                \n",
    "                print(f\"Epoch {epoch+1:4d} | Train Loss: {train_loss:.6f}{val_loss_str} | \"\n",
    "                      f\"Step Size: {step_size:.6f} | Avg Step: {avg_step_size:.6f} | \"\n",
    "                      f\"Backtrack: {backtrack_iter:2d} | Avg BT: {avg_backtrack:.1f} | \"\n",
    "                      f\"Patience: {patience_counter:2d}/{self.early_stop_patience} | Time: {elapsed_time:.2f}s\")\n",
    "                \n",
    "            # Early stopping check\n",
    "            if patience_counter >= self.early_stop_patience:\n",
    "                self.stopped_early = True\n",
    "                self.final_epoch = epoch + 1\n",
    "                if self.verbose:\n",
    "                    print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "                    print(f\"No improvement in loss for {self.early_stop_patience} epochs \"\n",
    "                          f\"(threshold: {self.early_stop_threshold})\")\n",
    "                break\n",
    "                \n",
    "        if not self.stopped_early:\n",
    "            self.final_epoch = self.max_epochs\n",
    "            if self.verbose:\n",
    "                print(f\"\\nTraining completed after {self.max_epochs} epochs\")\n",
    "                \n",
    "        training_time = time.time() - start_time\n",
    "        final_loss = self.loss_history[-1]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Final loss: {final_loss:.6f}\")\n",
    "            print(f\"Total training time: {training_time:.2f}s\")\n",
    "            print(f\"Average time per epoch: {training_time/self.final_epoch:.4f}s\")\n",
    "            print(f\"Average step size: {np.mean(self.step_size_history):.6f}\")\n",
    "            print(f\"Final step size: {self.step_size_history[-1]:.6f}\")\n",
    "            print(f\"Average backtrack iterations: {np.mean(self.backtrack_iterations_history):.2f}\")\n",
    "            \n",
    "        return {\n",
    "            'loss_history': self.loss_history.copy(),\n",
    "            'epoch_history': self.epoch_history.copy(),\n",
    "            'step_size_history': self.step_size_history.copy(),\n",
    "            'backtrack_iterations_history': self.backtrack_iterations_history.copy(),\n",
    "            'final_loss': final_loss,\n",
    "            'final_epoch': self.final_epoch,\n",
    "            'stopped_early': self.stopped_early,\n",
    "            'training_time': training_time,\n",
    "            'initial_learning_rate': self.initial_learning_rate,\n",
    "            'avg_step_size': np.mean(self.step_size_history),\n",
    "            'final_step_size': self.step_size_history[-1],\n",
    "            'avg_backtrack_iterations': np.mean(self.backtrack_iterations_history)\n",
    "        }\n",
    "    \n",
    "    def plot_training_curves(self, title=\"SGD Backtracking Training Curves\"):\n",
    "        \"\"\"Plot loss curve and step size evolution.\"\"\"\n",
    "        if not self.loss_history:\n",
    "            print(\"No training history available. Run optimize() first.\")\n",
    "            return\n",
    "            \n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10))\n",
    "        \n",
    "        # Plot 1: Loss curve\n",
    "        ax1.plot(self.epoch_history, self.loss_history, 'b-', linewidth=2, label='Training Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title(f'{title} - Loss Evolution')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend()\n",
    "        \n",
    "        if self.stopped_early:\n",
    "            ax1.axvline(x=self.final_epoch-1, color='r', linestyle='--', alpha=0.7, \n",
    "                       label=f'Early Stop (Epoch {self.final_epoch})')\n",
    "            ax1.legend()\n",
    "        \n",
    "        # Plot 2: Step size evolution\n",
    "        ax2.plot(self.epoch_history, self.step_size_history, 'g-', linewidth=2, label='Step Size')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Step Size')\n",
    "        ax2.set_title('Step Size Evolution')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "        ax2.set_yscale('log')  # Log scale for better visualization\n",
    "        \n",
    "        # Plot 3: Backtrack iterations\n",
    "        ax3.plot(self.epoch_history, self.backtrack_iterations_history, 'r-', linewidth=2, \n",
    "                label='Backtrack Iterations')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Backtrack Iterations')\n",
    "        ax3.set_title('Backtracking Iterations per Epoch')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.legend()\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa18ba9",
   "metadata": {},
   "source": [
    "## SGD iterations (mini-batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a09f685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDIterationsOptimizer:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent optimizer that processes data by iterations (mini-batches).\n",
    "    \n",
    "    Features:\n",
    "    - Configurable batch size for mini-batch processing\n",
    "    - Iteration-based training (not epoch-based)\n",
    "    - Random sampling of batches from training data\n",
    "    - Early stopping based on loss improvement threshold\n",
    "    - Configurable logging intervals\n",
    "    - Support for weighted loss computation\n",
    "    - Maintains data integrity with proper indexing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, batch_size=32, max_iterations=10000, \n",
    "                 early_stop_patience=100, early_stop_threshold=EARLY_STOP_THRESHOLD, \n",
    "                 log_interval=100, verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize SGD Iterations optimizer.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate (float): Fixed learning rate for gradient descent\n",
    "            batch_size (int): Size of mini-batches for each iteration\n",
    "            max_iterations (int): Maximum number of training iterations\n",
    "            early_stop_patience (int): Number of iterations to wait for improvement before stopping\n",
    "            early_stop_threshold (float): Minimum improvement threshold for early stopping\n",
    "            log_interval (int): Log metrics every n iterations\n",
    "            verbose (bool): Whether to print training progress\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iterations = max_iterations\n",
    "        self.early_stop_patience = early_stop_patience\n",
    "        self.early_stop_threshold = early_stop_threshold\n",
    "        self.log_interval = log_interval\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Training history\n",
    "        self.loss_history = []\n",
    "        self.iteration_history = []\n",
    "        self.stopped_early = False\n",
    "        self.final_iteration = 0\n",
    "        \n",
    "    def _generate_batch(self, X, y, weights=None):\n",
    "        \"\"\"\n",
    "        Generate a random mini-batch from the training data.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "            y: Target values\n",
    "            weights: Sample weights (optional)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (X_batch, y_batch, weights_batch) where weights_batch is None if weights is None\n",
    "        \"\"\"\n",
    "        n_samples = len(X)\n",
    "        batch_indices = np.random.choice(n_samples, size=min(self.batch_size, n_samples), replace=False)\n",
    "        \n",
    "        X_batch = X[batch_indices]\n",
    "        y_batch = y[batch_indices]\n",
    "        weights_batch = weights[batch_indices] if weights is not None else None\n",
    "        \n",
    "        return X_batch, y_batch, weights_batch\n",
    "    \n",
    "    def _compute_full_loss(self, model, X, y, weights=None):\n",
    "        \"\"\"\n",
    "        Compute loss on the full dataset for monitoring purposes.\n",
    "        \n",
    "        Args:\n",
    "            model: Model object\n",
    "            X: Full input features\n",
    "            y: Full target values\n",
    "            weights: Full sample weights (optional)\n",
    "            \n",
    "        Returns:\n",
    "            float: Loss value on full dataset\n",
    "        \"\"\"\n",
    "        return model._compute_loss(X, y, weights)\n",
    "        \n",
    "    def optimize(self, model, X_train, y_train, X_val=None, y_val=None,\n",
    "                weights_train=None, weights_val=None):\n",
    "        \"\"\"\n",
    "        Optimize model parameters using SGD with mini-batch iterations.\n",
    "        \n",
    "        Args:\n",
    "            model: Model object with _compute_loss and _compute_gradient methods\n",
    "            X_train: Training features\n",
    "            y_train: Training targets\n",
    "            X_val: Validation features (optional)\n",
    "            y_val: Validation targets (optional)\n",
    "            weights_train: Training sample weights (optional)\n",
    "            weights_val: Validation sample weights (optional)\n",
    "            \n",
    "        Returns:\n",
    "            dict: Training history and statistics\n",
    "        \"\"\"\n",
    "        # Initialize weights if not already done\n",
    "        if model.weights is None:\n",
    "            model.weights = np.random.normal(0, 0.01, X_train.shape[1])\n",
    "            model.bias = 0.0\n",
    "            \n",
    "        # Track best loss for early stopping (use full training set loss)\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Compute initial full dataset loss\n",
    "        initial_loss = self._compute_full_loss(model, X_train, y_train, weights_train)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Starting SGD Iterations training\")\n",
    "            print(f\"Learning rate: {self.learning_rate}, Batch size: {self.batch_size}\")\n",
    "            print(f\"Max iterations: {self.max_iterations}\")\n",
    "            print(f\"Early stopping: patience={self.early_stop_patience}, threshold={self.early_stop_threshold}\")\n",
    "            if weights_train is not None:\n",
    "                print(f\"Using weighted loss with training weights shape: {weights_train.shape}\")\n",
    "            print(f\"Initial full dataset loss: {initial_loss:.6f}\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            # Generate random mini-batch\n",
    "            X_batch, y_batch, weights_batch = self._generate_batch(X_train, y_train, weights_train)\n",
    "            \n",
    "            # Compute gradients on mini-batch\n",
    "            dw, db = model._compute_gradient(X_batch, y_batch, weights_batch)\n",
    "            \n",
    "            # Update parameters\n",
    "            model.weights -= self.learning_rate * dw\n",
    "            model.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Compute full dataset loss for monitoring (every few iterations to save computation)\n",
    "            if iteration % max(1, self.log_interval // 4) == 0:\n",
    "                full_train_loss = self._compute_full_loss(model, X_train, y_train, weights_train)\n",
    "                \n",
    "                # Store history\n",
    "                self.loss_history.append(full_train_loss)\n",
    "                self.iteration_history.append(iteration)\n",
    "                \n",
    "                # Check for early stopping\n",
    "                if full_train_loss < best_loss - self.early_stop_threshold:\n",
    "                    best_loss = full_train_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "            \n",
    "            # Log progress\n",
    "            if (iteration + 1) % self.log_interval == 0 and self.verbose:\n",
    "                # Compute current full losses for logging\n",
    "                current_train_loss = self._compute_full_loss(model, X_train, y_train, weights_train)\n",
    "                \n",
    "                val_loss_str = \"\"\n",
    "                if X_val is not None and y_val is not None:\n",
    "                    val_loss = self._compute_full_loss(model, X_val, y_val, weights_val)\n",
    "                    val_loss_str = f\", Val Loss: {val_loss:.6f}\"\n",
    "                    \n",
    "                elapsed_time = time.time() - start_time\n",
    "                iterations_per_sec = (iteration + 1) / elapsed_time\n",
    "                \n",
    "                print(f\"Iter {iteration+1:5d} | Train Loss: {current_train_loss:.6f}{val_loss_str} | \"\n",
    "                      f\"LR: {self.learning_rate:.4f} | Batch: {self.batch_size:3d} | \"\n",
    "                      f\"Patience: {patience_counter:3d}/{self.early_stop_patience} | \"\n",
    "                      f\"Rate: {iterations_per_sec:.1f} iter/s | Time: {elapsed_time:.2f}s\")\n",
    "                \n",
    "            # Early stopping check (check every few iterations)\n",
    "            if iteration % max(1, self.log_interval // 4) == 0 and patience_counter >= self.early_stop_patience:\n",
    "                self.stopped_early = True\n",
    "                self.final_iteration = iteration + 1\n",
    "                if self.verbose:\n",
    "                    print(f\"\\nEarly stopping triggered at iteration {iteration+1}\")\n",
    "                    print(f\"No improvement in loss for {self.early_stop_patience} checks \"\n",
    "                          f\"(threshold: {self.early_stop_threshold})\")\n",
    "                break\n",
    "                \n",
    "        if not self.stopped_early:\n",
    "            self.final_iteration = self.max_iterations\n",
    "            if self.verbose:\n",
    "                print(f\"\\nTraining completed after {self.max_iterations} iterations\")\n",
    "                \n",
    "        training_time = time.time() - start_time\n",
    "        final_loss = self._compute_full_loss(model, X_train, y_train, weights_train)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Final loss: {final_loss:.6f}\")\n",
    "            print(f\"Total training time: {training_time:.2f}s\")\n",
    "            print(f\"Average time per iteration: {training_time/self.final_iteration:.4f}s\")\n",
    "            print(f\"Total iterations: {self.final_iteration}\")\n",
    "            print(f\"Approximate epochs: {(self.final_iteration * self.batch_size) / len(X_train):.2f}\")\n",
    "            \n",
    "        return {\n",
    "            'loss_history': self.loss_history.copy(),\n",
    "            'iteration_history': self.iteration_history.copy(),\n",
    "            'final_loss': final_loss,\n",
    "            'final_iteration': self.final_iteration,\n",
    "            'stopped_early': self.stopped_early,\n",
    "            'training_time': training_time,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'batch_size': self.batch_size,\n",
    "            'approximate_epochs': (self.final_iteration * self.batch_size) / len(X_train)\n",
    "        }\n",
    "    \n",
    "    def plot_loss_curve(self, title=\"SGD Iterations Loss Curve\"):\n",
    "        \"\"\"Plot the loss curve from training history.\"\"\"\n",
    "        if not self.loss_history:\n",
    "            print(\"No training history available. Run optimize() first.\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.iteration_history, self.loss_history, 'b-', linewidth=2, label='Training Loss')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'{title} (LR: {self.learning_rate}, Batch: {self.batch_size})')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        if self.stopped_early:\n",
    "            plt.axvline(x=self.final_iteration-1, color='r', linestyle='--', alpha=0.7, \n",
    "                       label=f'Early Stop (Iter {self.final_iteration})')\n",
    "            plt.legend()\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fa8683f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SGD Iterations training\n",
      "Learning rate: 0.01, Batch size: 1\n",
      "Max iterations: 20000\n",
      "Early stopping: patience=100, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "Initial full dataset loss: 0.703001\n",
      "----------------------------------------------------------------------\n",
      "Iter    20 | Train Loss: 0.632041, Val Loss: 0.632106 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 103.4 iter/s | Time: 0.19s\n",
      "Iter    40 | Train Loss: 0.562751, Val Loss: 0.562518 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 105.0 iter/s | Time: 0.38s\n",
      "Iter    60 | Train Loss: 0.552025, Val Loss: 0.551588 | LR: 0.0100 | Batch:   1 | Patience:   2/100 | Rate: 102.7 iter/s | Time: 0.58s\n",
      "Iter    40 | Train Loss: 0.562751, Val Loss: 0.562518 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 105.0 iter/s | Time: 0.38s\n",
      "Iter    60 | Train Loss: 0.552025, Val Loss: 0.551588 | LR: 0.0100 | Batch:   1 | Patience:   2/100 | Rate: 102.7 iter/s | Time: 0.58s\n",
      "Iter    80 | Train Loss: 0.524386, Val Loss: 0.524098 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 101.4 iter/s | Time: 0.79s\n",
      "Iter    80 | Train Loss: 0.524386, Val Loss: 0.524098 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 101.4 iter/s | Time: 0.79s\n",
      "Iter   100 | Train Loss: 0.498769, Val Loss: 0.498441 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 100.8 iter/s | Time: 0.99s\n",
      "Iter   120 | Train Loss: 0.478382, Val Loss: 0.478022 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 100.8 iter/s | Time: 1.19s\n",
      "Iter   100 | Train Loss: 0.498769, Val Loss: 0.498441 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 100.8 iter/s | Time: 0.99s\n",
      "Iter   120 | Train Loss: 0.478382, Val Loss: 0.478022 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 100.8 iter/s | Time: 1.19s\n",
      "Iter   140 | Train Loss: 0.462169, Val Loss: 0.461629 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 98.3 iter/s | Time: 1.42s\n",
      "Iter   160 | Train Loss: 0.457684, Val Loss: 0.457339 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 98.8 iter/s | Time: 1.62s\n",
      "Iter   140 | Train Loss: 0.462169, Val Loss: 0.461629 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 98.3 iter/s | Time: 1.42s\n",
      "Iter   160 | Train Loss: 0.457684, Val Loss: 0.457339 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 98.8 iter/s | Time: 1.62s\n",
      "Iter   180 | Train Loss: 0.449846, Val Loss: 0.449447 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 98.7 iter/s | Time: 1.82s\n",
      "Iter   180 | Train Loss: 0.449846, Val Loss: 0.449447 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 98.7 iter/s | Time: 1.82s\n",
      "Iter   200 | Train Loss: 0.437478, Val Loss: 0.437024 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 97.4 iter/s | Time: 2.05s\n",
      "Iter   200 | Train Loss: 0.437478, Val Loss: 0.437024 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 97.4 iter/s | Time: 2.05s\n",
      "Iter   220 | Train Loss: 0.433359, Val Loss: 0.432711 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 93.5 iter/s | Time: 2.35s\n",
      "Iter   240 | Train Loss: 0.434276, Val Loss: 0.433591 | LR: 0.0100 | Batch:   1 | Patience:   4/100 | Rate: 94.3 iter/s | Time: 2.55s\n",
      "Iter   220 | Train Loss: 0.433359, Val Loss: 0.432711 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 93.5 iter/s | Time: 2.35s\n",
      "Iter   240 | Train Loss: 0.434276, Val Loss: 0.433591 | LR: 0.0100 | Batch:   1 | Patience:   4/100 | Rate: 94.3 iter/s | Time: 2.55s\n",
      "Iter   260 | Train Loss: 0.433992, Val Loss: 0.433426 | LR: 0.0100 | Batch:   1 | Patience:   8/100 | Rate: 94.8 iter/s | Time: 2.74s\n",
      "Iter   260 | Train Loss: 0.433992, Val Loss: 0.433426 | LR: 0.0100 | Batch:   1 | Patience:   8/100 | Rate: 94.8 iter/s | Time: 2.74s\n",
      "Iter   280 | Train Loss: 0.429303, Val Loss: 0.428614 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.1 iter/s | Time: 2.94s\n",
      "Iter   300 | Train Loss: 0.429187, Val Loss: 0.428588 | LR: 0.0100 | Batch:   1 | Patience:   3/100 | Rate: 95.4 iter/s | Time: 3.14s\n",
      "Iter   280 | Train Loss: 0.429303, Val Loss: 0.428614 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.1 iter/s | Time: 2.94s\n",
      "Iter   300 | Train Loss: 0.429187, Val Loss: 0.428588 | LR: 0.0100 | Batch:   1 | Patience:   3/100 | Rate: 95.4 iter/s | Time: 3.14s\n",
      "Iter   320 | Train Loss: 0.424375, Val Loss: 0.423669 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.5 iter/s | Time: 3.35s\n",
      "Iter   320 | Train Loss: 0.424375, Val Loss: 0.423669 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.5 iter/s | Time: 3.35s\n",
      "Iter   340 | Train Loss: 0.421172, Val Loss: 0.420447 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.7 iter/s | Time: 3.55s\n",
      "Iter   340 | Train Loss: 0.421172, Val Loss: 0.420447 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.7 iter/s | Time: 3.55s\n",
      "Iter   360 | Train Loss: 0.418555, Val Loss: 0.417924 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.4 iter/s | Time: 3.78s\n",
      "Iter   360 | Train Loss: 0.418555, Val Loss: 0.417924 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.4 iter/s | Time: 3.78s\n",
      "Iter   380 | Train Loss: 0.418750, Val Loss: 0.418218 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 93.9 iter/s | Time: 4.05s\n",
      "Iter   400 | Train Loss: 0.416536, Val Loss: 0.415912 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 94.3 iter/s | Time: 4.24s\n",
      "Iter   380 | Train Loss: 0.418750, Val Loss: 0.418218 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 93.9 iter/s | Time: 4.05s\n",
      "Iter   400 | Train Loss: 0.416536, Val Loss: 0.415912 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 94.3 iter/s | Time: 4.24s\n",
      "Iter   420 | Train Loss: 0.415057, Val Loss: 0.414508 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 94.2 iter/s | Time: 4.46s\n",
      "Iter   420 | Train Loss: 0.415057, Val Loss: 0.414508 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 94.2 iter/s | Time: 4.46s\n",
      "Iter   440 | Train Loss: 0.415069, Val Loss: 0.414605 | LR: 0.0100 | Batch:   1 | Patience:   2/100 | Rate: 94.3 iter/s | Time: 4.67s\n",
      "Iter   440 | Train Loss: 0.415069, Val Loss: 0.414605 | LR: 0.0100 | Batch:   1 | Patience:   2/100 | Rate: 94.3 iter/s | Time: 4.67s\n",
      "Iter   460 | Train Loss: 0.411632, Val Loss: 0.410945 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 94.2 iter/s | Time: 4.88s\n",
      "Iter   460 | Train Loss: 0.411632, Val Loss: 0.410945 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 94.2 iter/s | Time: 4.88s\n",
      "Iter   480 | Train Loss: 0.411409, Val Loss: 0.410789 | LR: 0.0100 | Batch:   1 | Patience:   1/100 | Rate: 94.4 iter/s | Time: 5.09s\n",
      "Iter   500 | Train Loss: 0.410277, Val Loss: 0.409615 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 94.6 iter/s | Time: 5.28s\n",
      "Iter   480 | Train Loss: 0.411409, Val Loss: 0.410789 | LR: 0.0100 | Batch:   1 | Patience:   1/100 | Rate: 94.4 iter/s | Time: 5.09s\n",
      "Iter   500 | Train Loss: 0.410277, Val Loss: 0.409615 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 94.6 iter/s | Time: 5.28s\n",
      "Iter   520 | Train Loss: 0.408680, Val Loss: 0.407818 | LR: 0.0100 | Batch:   1 | Patience:   1/100 | Rate: 94.4 iter/s | Time: 5.51s\n",
      "Iter   540 | Train Loss: 0.406443, Val Loss: 0.405610 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 94.7 iter/s | Time: 5.70s\n",
      "Iter   520 | Train Loss: 0.408680, Val Loss: 0.407818 | LR: 0.0100 | Batch:   1 | Patience:   1/100 | Rate: 94.4 iter/s | Time: 5.51s\n",
      "Iter   540 | Train Loss: 0.406443, Val Loss: 0.405610 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 94.7 iter/s | Time: 5.70s\n",
      "Iter   560 | Train Loss: 0.406514, Val Loss: 0.405799 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 94.8 iter/s | Time: 5.90s\n",
      "Iter   580 | Train Loss: 0.405889, Val Loss: 0.405195 | LR: 0.0100 | Batch:   1 | Patience:   2/100 | Rate: 95.1 iter/s | Time: 6.10s\n",
      "Iter   560 | Train Loss: 0.406514, Val Loss: 0.405799 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 94.8 iter/s | Time: 5.90s\n",
      "Iter   580 | Train Loss: 0.405889, Val Loss: 0.405195 | LR: 0.0100 | Batch:   1 | Patience:   2/100 | Rate: 95.1 iter/s | Time: 6.10s\n",
      "Iter   600 | Train Loss: 0.405708, Val Loss: 0.404951 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.3 iter/s | Time: 6.30s\n",
      "Iter   600 | Train Loss: 0.405708, Val Loss: 0.404951 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.3 iter/s | Time: 6.30s\n",
      "Iter   620 | Train Loss: 0.404493, Val Loss: 0.403890 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.1 iter/s | Time: 6.52s\n",
      "Iter   620 | Train Loss: 0.404493, Val Loss: 0.403890 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.1 iter/s | Time: 6.52s\n",
      "Iter   640 | Train Loss: 0.405399, Val Loss: 0.404691 | LR: 0.0100 | Batch:   1 | Patience:   3/100 | Rate: 95.1 iter/s | Time: 6.73s\n",
      "Iter   640 | Train Loss: 0.405399, Val Loss: 0.404691 | LR: 0.0100 | Batch:   1 | Patience:   3/100 | Rate: 95.1 iter/s | Time: 6.73s\n",
      "Iter   660 | Train Loss: 0.404435, Val Loss: 0.403703 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.0 iter/s | Time: 6.95s\n",
      "Iter   680 | Train Loss: 0.403812, Val Loss: 0.403239 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.1 iter/s | Time: 7.15s\n",
      "Iter   660 | Train Loss: 0.404435, Val Loss: 0.403703 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.0 iter/s | Time: 6.95s\n",
      "Iter   680 | Train Loss: 0.403812, Val Loss: 0.403239 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.1 iter/s | Time: 7.15s\n",
      "Iter   700 | Train Loss: 0.402676, Val Loss: 0.402018 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.3 iter/s | Time: 7.35s\n",
      "Iter   700 | Train Loss: 0.402676, Val Loss: 0.402018 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.3 iter/s | Time: 7.35s\n",
      "Iter   720 | Train Loss: 0.402138, Val Loss: 0.401355 | LR: 0.0100 | Batch:   1 | Patience:   1/100 | Rate: 95.4 iter/s | Time: 7.55s\n",
      "Iter   720 | Train Loss: 0.402138, Val Loss: 0.401355 | LR: 0.0100 | Batch:   1 | Patience:   1/100 | Rate: 95.4 iter/s | Time: 7.55s\n",
      "Iter   740 | Train Loss: 0.401634, Val Loss: 0.400821 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.4 iter/s | Time: 7.76s\n",
      "Iter   740 | Train Loss: 0.401634, Val Loss: 0.400821 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.4 iter/s | Time: 7.76s\n",
      "Iter   760 | Train Loss: 0.401059, Val Loss: 0.400269 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.1 iter/s | Time: 7.99s\n",
      "Iter   780 | Train Loss: 0.400812, Val Loss: 0.399938 | LR: 0.0100 | Batch:   1 | Patience:   1/100 | Rate: 95.3 iter/s | Time: 8.19s\n",
      "Iter   760 | Train Loss: 0.401059, Val Loss: 0.400269 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.1 iter/s | Time: 7.99s\n",
      "Iter   780 | Train Loss: 0.400812, Val Loss: 0.399938 | LR: 0.0100 | Batch:   1 | Patience:   1/100 | Rate: 95.3 iter/s | Time: 8.19s\n",
      "Iter   800 | Train Loss: 0.400311, Val Loss: 0.399462 | LR: 0.0100 | Batch:   1 | Patience:   1/100 | Rate: 95.4 iter/s | Time: 8.39s\n",
      "Iter   800 | Train Loss: 0.400311, Val Loss: 0.399462 | LR: 0.0100 | Batch:   1 | Patience:   1/100 | Rate: 95.4 iter/s | Time: 8.39s\n",
      "Iter   820 | Train Loss: 0.400891, Val Loss: 0.399927 | LR: 0.0100 | Batch:   1 | Patience:   2/100 | Rate: 95.4 iter/s | Time: 8.60s\n",
      "Iter   820 | Train Loss: 0.400891, Val Loss: 0.399927 | LR: 0.0100 | Batch:   1 | Patience:   2/100 | Rate: 95.4 iter/s | Time: 8.60s\n",
      "Iter   840 | Train Loss: 0.400556, Val Loss: 0.399669 | LR: 0.0100 | Batch:   1 | Patience:   6/100 | Rate: 95.2 iter/s | Time: 8.82s\n",
      "Iter   860 | Train Loss: 0.400199, Val Loss: 0.399355 | LR: 0.0100 | Batch:   1 | Patience:   1/100 | Rate: 95.3 iter/s | Time: 9.02s\n",
      "Iter   840 | Train Loss: 0.400556, Val Loss: 0.399669 | LR: 0.0100 | Batch:   1 | Patience:   6/100 | Rate: 95.2 iter/s | Time: 8.82s\n",
      "Iter   860 | Train Loss: 0.400199, Val Loss: 0.399355 | LR: 0.0100 | Batch:   1 | Patience:   1/100 | Rate: 95.3 iter/s | Time: 9.02s\n",
      "Iter   880 | Train Loss: 0.399980, Val Loss: 0.399174 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.4 iter/s | Time: 9.23s\n",
      "Iter   880 | Train Loss: 0.399980, Val Loss: 0.399174 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.4 iter/s | Time: 9.23s\n",
      "Iter   900 | Train Loss: 0.400794, Val Loss: 0.399937 | LR: 0.0100 | Batch:   1 | Patience:   4/100 | Rate: 95.3 iter/s | Time: 9.45s\n",
      "Iter   900 | Train Loss: 0.400794, Val Loss: 0.399937 | LR: 0.0100 | Batch:   1 | Patience:   4/100 | Rate: 95.3 iter/s | Time: 9.45s\n",
      "Iter   920 | Train Loss: 0.401123, Val Loss: 0.400242 | LR: 0.0100 | Batch:   1 | Patience:   8/100 | Rate: 95.1 iter/s | Time: 9.67s\n",
      "Iter   920 | Train Loss: 0.401123, Val Loss: 0.400242 | LR: 0.0100 | Batch:   1 | Patience:   8/100 | Rate: 95.1 iter/s | Time: 9.67s\n",
      "Iter   940 | Train Loss: 0.400986, Val Loss: 0.400174 | LR: 0.0100 | Batch:   1 | Patience:  12/100 | Rate: 95.0 iter/s | Time: 9.89s\n",
      "Iter   940 | Train Loss: 0.400986, Val Loss: 0.400174 | LR: 0.0100 | Batch:   1 | Patience:  12/100 | Rate: 95.0 iter/s | Time: 9.89s\n",
      "Iter   960 | Train Loss: 0.400779, Val Loss: 0.399836 | LR: 0.0100 | Batch:   1 | Patience:  16/100 | Rate: 94.9 iter/s | Time: 10.11s\n",
      "Iter   980 | Train Loss: 0.399903, Val Loss: 0.399074 | LR: 0.0100 | Batch:   1 | Patience:  20/100 | Rate: 95.0 iter/s | Time: 10.31s\n",
      "Iter   960 | Train Loss: 0.400779, Val Loss: 0.399836 | LR: 0.0100 | Batch:   1 | Patience:  16/100 | Rate: 94.9 iter/s | Time: 10.11s\n",
      "Iter   980 | Train Loss: 0.399903, Val Loss: 0.399074 | LR: 0.0100 | Batch:   1 | Patience:  20/100 | Rate: 95.0 iter/s | Time: 10.31s\n",
      "Iter  1000 | Train Loss: 0.400328, Val Loss: 0.399430 | LR: 0.0100 | Batch:   1 | Patience:  24/100 | Rate: 95.1 iter/s | Time: 10.51s\n",
      "Iter  1020 | Train Loss: 0.400898, Val Loss: 0.399970 | LR: 0.0100 | Batch:   1 | Patience:  28/100 | Rate: 95.2 iter/s | Time: 10.71s\n",
      "Iter  1000 | Train Loss: 0.400328, Val Loss: 0.399430 | LR: 0.0100 | Batch:   1 | Patience:  24/100 | Rate: 95.1 iter/s | Time: 10.51s\n",
      "Iter  1020 | Train Loss: 0.400898, Val Loss: 0.399970 | LR: 0.0100 | Batch:   1 | Patience:  28/100 | Rate: 95.2 iter/s | Time: 10.71s\n",
      "Iter  1040 | Train Loss: 0.400215, Val Loss: 0.399323 | LR: 0.0100 | Batch:   1 | Patience:  32/100 | Rate: 95.2 iter/s | Time: 10.92s\n",
      "Iter  1040 | Train Loss: 0.400215, Val Loss: 0.399323 | LR: 0.0100 | Batch:   1 | Patience:  32/100 | Rate: 95.2 iter/s | Time: 10.92s\n",
      "Iter  1060 | Train Loss: 0.400886, Val Loss: 0.399982 | LR: 0.0100 | Batch:   1 | Patience:  36/100 | Rate: 95.2 iter/s | Time: 11.13s\n",
      "Iter  1080 | Train Loss: 0.399571, Val Loss: 0.398749 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.3 iter/s | Time: 11.33s\n",
      "Iter  1060 | Train Loss: 0.400886, Val Loss: 0.399982 | LR: 0.0100 | Batch:   1 | Patience:  36/100 | Rate: 95.2 iter/s | Time: 11.13s\n",
      "Iter  1080 | Train Loss: 0.399571, Val Loss: 0.398749 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.3 iter/s | Time: 11.33s\n",
      "Iter  1100 | Train Loss: 0.399088, Val Loss: 0.398487 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.4 iter/s | Time: 11.53s\n",
      "Iter  1100 | Train Loss: 0.399088, Val Loss: 0.398487 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.4 iter/s | Time: 11.53s\n",
      "Iter  1120 | Train Loss: 0.399160, Val Loss: 0.398449 | LR: 0.0100 | Batch:   1 | Patience:   4/100 | Rate: 95.4 iter/s | Time: 11.74s\n",
      "Iter  1120 | Train Loss: 0.399160, Val Loss: 0.398449 | LR: 0.0100 | Batch:   1 | Patience:   4/100 | Rate: 95.4 iter/s | Time: 11.74s\n",
      "Iter  1140 | Train Loss: 0.399290, Val Loss: 0.398594 | LR: 0.0100 | Batch:   1 | Patience:   8/100 | Rate: 95.4 iter/s | Time: 11.94s\n",
      "Iter  1140 | Train Loss: 0.399290, Val Loss: 0.398594 | LR: 0.0100 | Batch:   1 | Patience:   8/100 | Rate: 95.4 iter/s | Time: 11.94s\n",
      "Iter  1160 | Train Loss: 0.400329, Val Loss: 0.399707 | LR: 0.0100 | Batch:   1 | Patience:  12/100 | Rate: 95.5 iter/s | Time: 12.15s\n",
      "Iter  1180 | Train Loss: 0.400493, Val Loss: 0.399781 | LR: 0.0100 | Batch:   1 | Patience:  16/100 | Rate: 95.6 iter/s | Time: 12.35s\n",
      "Iter  1160 | Train Loss: 0.400329, Val Loss: 0.399707 | LR: 0.0100 | Batch:   1 | Patience:  12/100 | Rate: 95.5 iter/s | Time: 12.15s\n",
      "Iter  1180 | Train Loss: 0.400493, Val Loss: 0.399781 | LR: 0.0100 | Batch:   1 | Patience:  16/100 | Rate: 95.6 iter/s | Time: 12.35s\n",
      "Iter  1200 | Train Loss: 0.400306, Val Loss: 0.399534 | LR: 0.0100 | Batch:   1 | Patience:  20/100 | Rate: 95.6 iter/s | Time: 12.55s\n",
      "Iter  1220 | Train Loss: 0.400396, Val Loss: 0.399629 | LR: 0.0100 | Batch:   1 | Patience:  24/100 | Rate: 95.7 iter/s | Time: 12.74s\n",
      "Iter  1200 | Train Loss: 0.400306, Val Loss: 0.399534 | LR: 0.0100 | Batch:   1 | Patience:  20/100 | Rate: 95.6 iter/s | Time: 12.55s\n",
      "Iter  1220 | Train Loss: 0.400396, Val Loss: 0.399629 | LR: 0.0100 | Batch:   1 | Patience:  24/100 | Rate: 95.7 iter/s | Time: 12.74s\n",
      "Iter  1240 | Train Loss: 0.401482, Val Loss: 0.400582 | LR: 0.0100 | Batch:   1 | Patience:  28/100 | Rate: 95.7 iter/s | Time: 12.95s\n",
      "Iter  1240 | Train Loss: 0.401482, Val Loss: 0.400582 | LR: 0.0100 | Batch:   1 | Patience:  28/100 | Rate: 95.7 iter/s | Time: 12.95s\n",
      "Iter  1260 | Train Loss: 0.400454, Val Loss: 0.399648 | LR: 0.0100 | Batch:   1 | Patience:  32/100 | Rate: 95.6 iter/s | Time: 13.17s\n",
      "Iter  1280 | Train Loss: 0.399749, Val Loss: 0.398913 | LR: 0.0100 | Batch:   1 | Patience:  36/100 | Rate: 95.7 iter/s | Time: 13.37s\n",
      "Iter  1260 | Train Loss: 0.400454, Val Loss: 0.399648 | LR: 0.0100 | Batch:   1 | Patience:  32/100 | Rate: 95.6 iter/s | Time: 13.17s\n",
      "Iter  1280 | Train Loss: 0.399749, Val Loss: 0.398913 | LR: 0.0100 | Batch:   1 | Patience:  36/100 | Rate: 95.7 iter/s | Time: 13.37s\n",
      "Iter  1300 | Train Loss: 0.398370, Val Loss: 0.397653 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.8 iter/s | Time: 13.58s\n",
      "Iter  1300 | Train Loss: 0.398370, Val Loss: 0.397653 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.8 iter/s | Time: 13.58s\n",
      "Iter  1320 | Train Loss: 0.397742, Val Loss: 0.397002 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.8 iter/s | Time: 13.78s\n",
      "Iter  1320 | Train Loss: 0.397742, Val Loss: 0.397002 | LR: 0.0100 | Batch:   1 | Patience:   0/100 | Rate: 95.8 iter/s | Time: 13.78s\n",
      "Iter  1340 | Train Loss: 0.398356, Val Loss: 0.397508 | LR: 0.0100 | Batch:   1 | Patience:   2/100 | Rate: 95.6 iter/s | Time: 14.01s\n",
      "Iter  1340 | Train Loss: 0.398356, Val Loss: 0.397508 | LR: 0.0100 | Batch:   1 | Patience:   2/100 | Rate: 95.6 iter/s | Time: 14.01s\n",
      "Iter  1360 | Train Loss: 0.398187, Val Loss: 0.397282 | LR: 0.0100 | Batch:   1 | Patience:   6/100 | Rate: 95.7 iter/s | Time: 14.22s\n",
      "Iter  1380 | Train Loss: 0.398697, Val Loss: 0.397667 | LR: 0.0100 | Batch:   1 | Patience:  10/100 | Rate: 95.7 iter/s | Time: 14.42s\n",
      "Iter  1360 | Train Loss: 0.398187, Val Loss: 0.397282 | LR: 0.0100 | Batch:   1 | Patience:   6/100 | Rate: 95.7 iter/s | Time: 14.22s\n",
      "Iter  1380 | Train Loss: 0.398697, Val Loss: 0.397667 | LR: 0.0100 | Batch:   1 | Patience:  10/100 | Rate: 95.7 iter/s | Time: 14.42s\n",
      "Iter  1400 | Train Loss: 0.398131, Val Loss: 0.397037 | LR: 0.0100 | Batch:   1 | Patience:  14/100 | Rate: 95.7 iter/s | Time: 14.62s\n",
      "Iter  1420 | Train Loss: 0.398373, Val Loss: 0.397251 | LR: 0.0100 | Batch:   1 | Patience:  18/100 | Rate: 95.8 iter/s | Time: 14.82s\n",
      "Iter  1400 | Train Loss: 0.398131, Val Loss: 0.397037 | LR: 0.0100 | Batch:   1 | Patience:  14/100 | Rate: 95.7 iter/s | Time: 14.62s\n",
      "Iter  1420 | Train Loss: 0.398373, Val Loss: 0.397251 | LR: 0.0100 | Batch:   1 | Patience:  18/100 | Rate: 95.8 iter/s | Time: 14.82s\n",
      "Iter  1440 | Train Loss: 0.398292, Val Loss: 0.397232 | LR: 0.0100 | Batch:   1 | Patience:  22/100 | Rate: 95.6 iter/s | Time: 15.07s\n",
      "Iter  1440 | Train Loss: 0.398292, Val Loss: 0.397232 | LR: 0.0100 | Batch:   1 | Patience:  22/100 | Rate: 95.6 iter/s | Time: 15.07s\n",
      "Iter  1460 | Train Loss: 0.398525, Val Loss: 0.397436 | LR: 0.0100 | Batch:   1 | Patience:  26/100 | Rate: 95.6 iter/s | Time: 15.27s\n",
      "Iter  1460 | Train Loss: 0.398525, Val Loss: 0.397436 | LR: 0.0100 | Batch:   1 | Patience:  26/100 | Rate: 95.6 iter/s | Time: 15.27s\n",
      "Iter  1480 | Train Loss: 0.399931, Val Loss: 0.398723 | LR: 0.0100 | Batch:   1 | Patience:  30/100 | Rate: 95.7 iter/s | Time: 15.47s\n",
      "Iter  1500 | Train Loss: 0.401214, Val Loss: 0.399935 | LR: 0.0100 | Batch:   1 | Patience:  34/100 | Rate: 95.7 iter/s | Time: 15.67s\n",
      "Iter  1480 | Train Loss: 0.399931, Val Loss: 0.398723 | LR: 0.0100 | Batch:   1 | Patience:  30/100 | Rate: 95.7 iter/s | Time: 15.47s\n",
      "Iter  1500 | Train Loss: 0.401214, Val Loss: 0.399935 | LR: 0.0100 | Batch:   1 | Patience:  34/100 | Rate: 95.7 iter/s | Time: 15.67s\n",
      "Iter  1520 | Train Loss: 0.401527, Val Loss: 0.400244 | LR: 0.0100 | Batch:   1 | Patience:  38/100 | Rate: 95.8 iter/s | Time: 15.87s\n",
      "Iter  1520 | Train Loss: 0.401527, Val Loss: 0.400244 | LR: 0.0100 | Batch:   1 | Patience:  38/100 | Rate: 95.8 iter/s | Time: 15.87s\n",
      "Iter  1540 | Train Loss: 0.402642, Val Loss: 0.401297 | LR: 0.0100 | Batch:   1 | Patience:  42/100 | Rate: 95.8 iter/s | Time: 16.08s\n",
      "Iter  1540 | Train Loss: 0.402642, Val Loss: 0.401297 | LR: 0.0100 | Batch:   1 | Patience:  42/100 | Rate: 95.8 iter/s | Time: 16.08s\n",
      "Iter  1560 | Train Loss: 0.400943, Val Loss: 0.399741 | LR: 0.0100 | Batch:   1 | Patience:  46/100 | Rate: 95.8 iter/s | Time: 16.28s\n",
      "Iter  1580 | Train Loss: 0.399663, Val Loss: 0.398501 | LR: 0.0100 | Batch:   1 | Patience:  50/100 | Rate: 95.8 iter/s | Time: 16.48s\n",
      "Iter  1560 | Train Loss: 0.400943, Val Loss: 0.399741 | LR: 0.0100 | Batch:   1 | Patience:  46/100 | Rate: 95.8 iter/s | Time: 16.28s\n",
      "Iter  1580 | Train Loss: 0.399663, Val Loss: 0.398501 | LR: 0.0100 | Batch:   1 | Patience:  50/100 | Rate: 95.8 iter/s | Time: 16.48s\n",
      "Iter  1600 | Train Loss: 0.400084, Val Loss: 0.398839 | LR: 0.0100 | Batch:   1 | Patience:  54/100 | Rate: 95.9 iter/s | Time: 16.68s\n",
      "Iter  1600 | Train Loss: 0.400084, Val Loss: 0.398839 | LR: 0.0100 | Batch:   1 | Patience:  54/100 | Rate: 95.9 iter/s | Time: 16.68s\n",
      "Iter  1620 | Train Loss: 0.399833, Val Loss: 0.398552 | LR: 0.0100 | Batch:   1 | Patience:  58/100 | Rate: 95.9 iter/s | Time: 16.89s\n",
      "Iter  1620 | Train Loss: 0.399833, Val Loss: 0.398552 | LR: 0.0100 | Batch:   1 | Patience:  58/100 | Rate: 95.9 iter/s | Time: 16.89s\n",
      "Iter  1640 | Train Loss: 0.399641, Val Loss: 0.398443 | LR: 0.0100 | Batch:   1 | Patience:  62/100 | Rate: 95.9 iter/s | Time: 17.09s\n",
      "Iter  1660 | Train Loss: 0.400741, Val Loss: 0.399500 | LR: 0.0100 | Batch:   1 | Patience:  66/100 | Rate: 96.0 iter/s | Time: 17.29s\n",
      "Iter  1640 | Train Loss: 0.399641, Val Loss: 0.398443 | LR: 0.0100 | Batch:   1 | Patience:  62/100 | Rate: 95.9 iter/s | Time: 17.09s\n",
      "Iter  1660 | Train Loss: 0.400741, Val Loss: 0.399500 | LR: 0.0100 | Batch:   1 | Patience:  66/100 | Rate: 96.0 iter/s | Time: 17.29s\n",
      "Iter  1680 | Train Loss: 0.400922, Val Loss: 0.399684 | LR: 0.0100 | Batch:   1 | Patience:  70/100 | Rate: 96.0 iter/s | Time: 17.49s\n",
      "Iter  1700 | Train Loss: 0.400114, Val Loss: 0.399234 | LR: 0.0100 | Batch:   1 | Patience:  74/100 | Rate: 96.1 iter/s | Time: 17.69s\n",
      "Iter  1680 | Train Loss: 0.400922, Val Loss: 0.399684 | LR: 0.0100 | Batch:   1 | Patience:  70/100 | Rate: 96.0 iter/s | Time: 17.49s\n",
      "Iter  1700 | Train Loss: 0.400114, Val Loss: 0.399234 | LR: 0.0100 | Batch:   1 | Patience:  74/100 | Rate: 96.1 iter/s | Time: 17.69s\n",
      "Iter  1720 | Train Loss: 0.401741, Val Loss: 0.400919 | LR: 0.0100 | Batch:   1 | Patience:  78/100 | Rate: 96.1 iter/s | Time: 17.89s\n",
      "Iter  1720 | Train Loss: 0.401741, Val Loss: 0.400919 | LR: 0.0100 | Batch:   1 | Patience:  78/100 | Rate: 96.1 iter/s | Time: 17.89s\n",
      "Iter  1740 | Train Loss: 0.401586, Val Loss: 0.400804 | LR: 0.0100 | Batch:   1 | Patience:  82/100 | Rate: 96.2 iter/s | Time: 18.09s\n",
      "Iter  1760 | Train Loss: 0.401598, Val Loss: 0.400841 | LR: 0.0100 | Batch:   1 | Patience:  86/100 | Rate: 96.2 iter/s | Time: 18.29s\n",
      "Iter  1740 | Train Loss: 0.401586, Val Loss: 0.400804 | LR: 0.0100 | Batch:   1 | Patience:  82/100 | Rate: 96.2 iter/s | Time: 18.09s\n",
      "Iter  1760 | Train Loss: 0.401598, Val Loss: 0.400841 | LR: 0.0100 | Batch:   1 | Patience:  86/100 | Rate: 96.2 iter/s | Time: 18.29s\n",
      "Iter  1780 | Train Loss: 0.401450, Val Loss: 0.400658 | LR: 0.0100 | Batch:   1 | Patience:  90/100 | Rate: 96.3 iter/s | Time: 18.49s\n",
      "Iter  1800 | Train Loss: 0.402702, Val Loss: 0.402005 | LR: 0.0100 | Batch:   1 | Patience:  94/100 | Rate: 96.3 iter/s | Time: 18.69s\n",
      "Iter  1780 | Train Loss: 0.401450, Val Loss: 0.400658 | LR: 0.0100 | Batch:   1 | Patience:  90/100 | Rate: 96.3 iter/s | Time: 18.49s\n",
      "Iter  1800 | Train Loss: 0.402702, Val Loss: 0.402005 | LR: 0.0100 | Batch:   1 | Patience:  94/100 | Rate: 96.3 iter/s | Time: 18.69s\n",
      "Iter  1820 | Train Loss: 0.403574, Val Loss: 0.402820 | LR: 0.0100 | Batch:   1 | Patience:  98/100 | Rate: 96.3 iter/s | Time: 18.89s\n",
      "\n",
      "Early stopping triggered at iteration 1826\n",
      "No improvement in loss for 100 checks (threshold: 1e-06)\n",
      "Final loss: 0.403137\n",
      "Total training time: 18.96s\n",
      "Average time per iteration: 0.0104s\n",
      "Total iterations: 1826\n",
      "Approximate epochs: 0.00\n",
      "\n",
      "✅ SGD Iterations test completed!\n",
      "Final loss: 0.403137\n",
      "Total iterations: 1826\n",
      "Approximate epochs: 0.00\n",
      "Iter  1820 | Train Loss: 0.403574, Val Loss: 0.402820 | LR: 0.0100 | Batch:   1 | Patience:  98/100 | Rate: 96.3 iter/s | Time: 18.89s\n",
      "\n",
      "Early stopping triggered at iteration 1826\n",
      "No improvement in loss for 100 checks (threshold: 1e-06)\n",
      "Final loss: 0.403137\n",
      "Total training time: 18.96s\n",
      "Average time per iteration: 0.0104s\n",
      "Total iterations: 1826\n",
      "Approximate epochs: 0.00\n",
      "\n",
      "✅ SGD Iterations test completed!\n",
      "Final loss: 0.403137\n",
      "Total iterations: 1826\n",
      "Approximate epochs: 0.00\n"
     ]
    }
   ],
   "source": [
    "test_model = WeightedLogisticRegression()\n",
    "test_optimizer = SGDIterationsOptimizer(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=1,\n",
    "    max_iterations=20000,\n",
    "    log_interval=20,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "test_result = test_optimizer.optimize(\n",
    "    test_model,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    X_val_scaled,\n",
    "    y_val,\n",
    "    weights_train=weights_train,\n",
    "    weights_val=weights_val\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ SGD Iterations test completed!\")\n",
    "print(f\"Final loss: {test_result['final_loss']:.6f}\")\n",
    "print(f\"Total iterations: {test_result['final_iteration']}\")\n",
    "print(f\"Approximate epochs: {test_result['approximate_epochs']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8742570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the plotting functionality\n",
    "test_optimizer.plot_loss_curve(\"SGD Iterations Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4bb950",
   "metadata": {},
   "source": [
    "## AGD fixed learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4310c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGDOptimizer:\n",
    "    \"\"\"\n",
    "    Accelerated Gradient Descent (Nesterov's method) optimizer with early stopping and logging capabilities.\n",
    "    \n",
    "    Features:\n",
    "    - Fixed learning rate with momentum acceleration\n",
    "    - Nesterov momentum for improved convergence\n",
    "    - Early stopping based on loss improvement threshold\n",
    "    - Configurable logging intervals\n",
    "    - Loss and momentum history tracking\n",
    "    - Support for weighted loss computation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9, max_epochs=MAX_EPOCHS, early_stop_patience=50, \n",
    "                 early_stop_threshold=EARLY_STOP_THRESHOLD, log_interval=10, verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize AGD optimizer.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate (float): Fixed learning rate for gradient descent\n",
    "            momentum (float): Momentum coefficient (0 <= momentum < 1)\n",
    "            max_epochs (int): Maximum number of training epochs\n",
    "            early_stop_patience (int): Number of epochs to wait for improvement before stopping\n",
    "            early_stop_threshold (float): Minimum improvement threshold for early stopping\n",
    "            log_interval (int): Log metrics every n epochs\n",
    "            verbose (bool): Whether to print training progress\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.max_epochs = max_epochs\n",
    "        self.early_stop_patience = early_stop_patience\n",
    "        self.early_stop_threshold = early_stop_threshold\n",
    "        self.log_interval = log_interval\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Training history\n",
    "        self.loss_history = []\n",
    "        self.epoch_history = []\n",
    "        self.stopped_early = False\n",
    "        self.final_epoch = 0\n",
    "        \n",
    "        # Momentum variables\n",
    "        self.velocity_w = None\n",
    "        self.velocity_b = None\n",
    "        \n",
    "    def optimize(self, model, X_train, y_train, X_val=None, y_val=None,\n",
    "                weights_train=None, weights_val=None):\n",
    "        \"\"\"\n",
    "        Optimize model parameters using AGD with Nesterov momentum.\n",
    "        \n",
    "        Args:\n",
    "            model: Model object with _compute_loss and _compute_gradient methods\n",
    "            X_train: Training features\n",
    "            y_train: Training targets\n",
    "            X_val: Validation features (optional)\n",
    "            y_val: Validation targets (optional)\n",
    "            weights_train: Training sample weights (optional)\n",
    "            weights_val: Validation sample weights (optional)\n",
    "            \n",
    "        Returns:\n",
    "            dict: Training history and statistics\n",
    "        \"\"\"\n",
    "        # Initialize weights if not already done\n",
    "        if model.weights is None:\n",
    "            model.weights = np.random.normal(0, 0.01, X_train.shape[1])\n",
    "            model.bias = 0.0\n",
    "            \n",
    "        # Initialize momentum variables\n",
    "        self.velocity_w = np.zeros_like(model.weights)\n",
    "        self.velocity_b = 0.0\n",
    "            \n",
    "        # Track best loss for early stopping\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Starting AGD training with learning rate: {self.learning_rate}, momentum: {self.momentum}\")\n",
    "            print(f\"Early stopping: patience={self.early_stop_patience}, threshold={self.early_stop_threshold}\")\n",
    "            if weights_train is not None:\n",
    "                print(f\"Using weighted loss with training weights shape: {weights_train.shape}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.max_epochs):\n",
    "            # Nesterov momentum: compute gradients at the lookahead position\n",
    "            # Store current parameters\n",
    "            current_weights = model.weights.copy()\n",
    "            current_bias = model.bias\n",
    "            \n",
    "            # Move to lookahead position\n",
    "            model.weights = current_weights + self.momentum * self.velocity_w\n",
    "            model.bias = current_bias + self.momentum * self.velocity_b\n",
    "            \n",
    "            # Compute loss at current position (before update) for monitoring\n",
    "            model.weights = current_weights\n",
    "            model.bias = current_bias\n",
    "            train_loss = model._compute_loss(X_train, y_train, weights_train)\n",
    "            \n",
    "            # Compute gradients at lookahead position (weighted if weights provided)\n",
    "            model.weights = current_weights + self.momentum * self.velocity_w\n",
    "            model.bias = current_bias + self.momentum * self.velocity_b\n",
    "            dw, db = model._compute_gradient(X_train, y_train, weights_train)\n",
    "            \n",
    "            # Update velocity\n",
    "            self.velocity_w = self.momentum * self.velocity_w - self.learning_rate * dw\n",
    "            self.velocity_b = self.momentum * self.velocity_b - self.learning_rate * db\n",
    "            \n",
    "            # Update parameters\n",
    "            model.weights = current_weights + self.velocity_w\n",
    "            model.bias = current_bias + self.velocity_b\n",
    "            \n",
    "            # Store history\n",
    "            self.loss_history.append(train_loss)\n",
    "            self.epoch_history.append(epoch)\n",
    "            \n",
    "            # Check for early stopping\n",
    "            if train_loss < best_loss - self.early_stop_threshold:\n",
    "                best_loss = train_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            # Log progress\n",
    "            if (epoch + 1) % self.log_interval == 0 and self.verbose:\n",
    "                val_loss_str = \"\"\n",
    "                if X_val is not None and y_val is not None:\n",
    "                    val_loss = model._compute_loss(X_val, y_val, weights_val)\n",
    "                    val_loss_str = f\", Val Loss: {val_loss:.6f}\"\n",
    "                    \n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(f\"Epoch {epoch+1:4d} | Train Loss: {train_loss:.6f}{val_loss_str} | \"\n",
    "                      f\"LR: {self.learning_rate:.4f} | Momentum: {self.momentum:.3f} | \"\n",
    "                      f\"Patience: {patience_counter:2d}/{self.early_stop_patience} | \"\n",
    "                      f\"Time: {elapsed_time:.2f}s\")\n",
    "                \n",
    "            # Early stopping check\n",
    "            if patience_counter >= self.early_stop_patience:\n",
    "                self.stopped_early = True\n",
    "                self.final_epoch = epoch + 1\n",
    "                if self.verbose:\n",
    "                    print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "                    print(f\"No improvement in loss for {self.early_stop_patience} epochs \"\n",
    "                          f\"(threshold: {self.early_stop_threshold})\")\n",
    "                break\n",
    "                \n",
    "        if not self.stopped_early:\n",
    "            self.final_epoch = self.max_epochs\n",
    "            if self.verbose:\n",
    "                print(f\"\\nTraining completed after {self.max_epochs} epochs\")\n",
    "                \n",
    "        training_time = time.time() - start_time\n",
    "        final_loss = self.loss_history[-1]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Final loss: {final_loss:.6f}\")\n",
    "            print(f\"Total training time: {training_time:.2f}s\")\n",
    "            print(f\"Average time per epoch: {training_time/self.final_epoch:.4f}s\")\n",
    "            \n",
    "        return {\n",
    "            'loss_history': self.loss_history.copy(),\n",
    "            'epoch_history': self.epoch_history.copy(),\n",
    "            'final_loss': final_loss,\n",
    "            'final_epoch': self.final_epoch,\n",
    "            'stopped_early': self.stopped_early,\n",
    "            'training_time': training_time,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'momentum': self.momentum\n",
    "        }\n",
    "    \n",
    "    def plot_loss_curve(self, title=\"AGD Loss Curve\"):\n",
    "        \"\"\"Plot the loss curve from training history.\"\"\"\n",
    "        if not self.loss_history:\n",
    "            print(\"No training history available. Run optimize() first.\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.epoch_history, self.loss_history, 'b-', linewidth=2, label='Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'{title} (LR: {self.learning_rate}, Momentum: {self.momentum})')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        if self.stopped_early:\n",
    "            plt.axvline(x=self.final_epoch-1, color='r', linestyle='--', alpha=0.7, \n",
    "                       label=f'Early Stop (Epoch {self.final_epoch})')\n",
    "            plt.legend()\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2c3f5c",
   "metadata": {},
   "source": [
    "## AGD backtracking line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33576f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGDBacktrackingOptimizer:\n",
    "    \"\"\"\n",
    "    Accelerated Gradient Descent with Backtracking Line Search.\n",
    "    \n",
    "    Features:\n",
    "    - Nesterov momentum acceleration\n",
    "    - Adaptive step size using Armijo line search condition\n",
    "    - Backtracking algorithm with configurable parameters\n",
    "    - Early stopping based on loss improvement threshold\n",
    "    - Configurable logging intervals\n",
    "    - Step size and momentum history tracking\n",
    "    - Support for weighted loss computation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_learning_rate=1.0, momentum=0.9, c1=1e-4, rho=0.5, \n",
    "                 max_epochs=MAX_EPOCHS, early_stop_patience=50, early_stop_threshold=EARLY_STOP_THRESHOLD, \n",
    "                 log_interval=10, max_backtrack_iter=20, verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize AGD Backtracking optimizer.\n",
    "        \n",
    "        Args:\n",
    "            initial_learning_rate (float): Initial step size for backtracking\n",
    "            momentum (float): Momentum coefficient (0 <= momentum < 1)\n",
    "            c1 (float): Armijo condition parameter (0 < c1 < 1)\n",
    "            rho (float): Step size reduction factor (0 < rho < 1)\n",
    "            max_epochs (int): Maximum number of training epochs\n",
    "            early_stop_patience (int): Number of epochs to wait for improvement before stopping\n",
    "            early_stop_threshold (float): Minimum improvement threshold for early stopping\n",
    "            log_interval (int): Log metrics every n epochs\n",
    "            max_backtrack_iter (int): Maximum backtracking iterations per epoch\n",
    "            verbose (bool): Whether to print training progress\n",
    "        \"\"\"\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.c1 = c1  # Armijo parameter\n",
    "        self.rho = rho  # Step reduction factor\n",
    "        self.max_epochs = max_epochs\n",
    "        self.early_stop_patience = early_stop_patience\n",
    "        self.early_stop_threshold = early_stop_threshold\n",
    "        self.log_interval = log_interval\n",
    "        self.max_backtrack_iter = max_backtrack_iter\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Training history\n",
    "        self.loss_history = []\n",
    "        self.epoch_history = []\n",
    "        self.step_size_history = []\n",
    "        self.backtrack_iterations_history = []\n",
    "        self.stopped_early = False\n",
    "        self.final_epoch = 0\n",
    "        \n",
    "        # Momentum variables\n",
    "        self.velocity_w = None\n",
    "        self.velocity_b = None\n",
    "        \n",
    "    def _armijo_condition(self, f_current, f_new, grad_dot_direction, step_size):\n",
    "        \"\"\"\n",
    "        Check Armijo sufficient decrease condition.\n",
    "        \n",
    "        Args:\n",
    "            f_current: Current function value\n",
    "            f_new: New function value after step\n",
    "            grad_dot_direction: Gradient dot product with search direction\n",
    "            step_size: Current step size\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if Armijo condition is satisfied\n",
    "        \"\"\"\n",
    "        return f_new <= f_current + self.c1 * step_size * grad_dot_direction\n",
    "    \n",
    "    def _backtracking_line_search_agd(self, model, X, y, current_weights, current_bias,\n",
    "                                      velocity_w, velocity_b, dw, db, current_loss, sample_weights=None):\n",
    "        \"\"\"\n",
    "        Perform backtracking line search for AGD to find suitable step size.\n",
    "        \n",
    "        Args:\n",
    "            model: Model object\n",
    "            X: Input features\n",
    "            y: Target values\n",
    "            current_weights: Current weight parameters\n",
    "            current_bias: Current bias parameter\n",
    "            velocity_w: Current weight velocity\n",
    "            velocity_b: Current bias velocity\n",
    "            dw: Weight gradients\n",
    "            db: Bias gradient\n",
    "            current_loss: Current loss value\n",
    "            sample_weights: Sample weights for weighted loss computation\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (step_size, backtrack_iterations)\n",
    "        \"\"\"\n",
    "        step_size = self.initial_learning_rate\n",
    "        \n",
    "        # Compute gradient norm squared (for descent direction)\n",
    "        grad_norm_sq = np.sum(dw**2) + db**2\n",
    "        \n",
    "        # Search direction is negative gradient (velocity update direction)\n",
    "        direction_dot_grad = -grad_norm_sq\n",
    "        \n",
    "        backtrack_iter = 0\n",
    "        \n",
    "        for i in range(self.max_backtrack_iter):\n",
    "            # Try step with current step size\n",
    "            # Update velocity\n",
    "            new_velocity_w = self.momentum * velocity_w - step_size * dw\n",
    "            new_velocity_b = self.momentum * velocity_b - step_size * db\n",
    "            \n",
    "            # Update parameters\n",
    "            new_weights = current_weights + new_velocity_w\n",
    "            new_bias = current_bias + new_velocity_b\n",
    "            \n",
    "            # Temporarily update model parameters to compute new loss\n",
    "            old_weights = model.weights.copy()\n",
    "            old_bias = model.bias\n",
    "            \n",
    "            model.weights = new_weights\n",
    "            model.bias = new_bias\n",
    "            \n",
    "            new_loss = model._compute_loss(X, y, sample_weights)\n",
    "            \n",
    "            # Restore original parameters\n",
    "            model.weights = old_weights\n",
    "            model.bias = old_bias\n",
    "            \n",
    "            # Check Armijo condition\n",
    "            if self._armijo_condition(current_loss, new_loss, direction_dot_grad, step_size):\n",
    "                break\n",
    "                \n",
    "            # Reduce step size\n",
    "            step_size *= self.rho\n",
    "            backtrack_iter += 1\n",
    "            \n",
    "        return step_size, backtrack_iter\n",
    "        \n",
    "    def optimize(self, model, X_train, y_train, X_val=None, y_val=None,\n",
    "                weights_train=None, weights_val=None):\n",
    "        \"\"\"\n",
    "        Optimize model parameters using AGD with backtracking line search.\n",
    "        \n",
    "        Args:\n",
    "            model: Model object with _compute_loss and _compute_gradient methods\n",
    "            X_train: Training features\n",
    "            y_train: Training targets\n",
    "            X_val: Validation features (optional)\n",
    "            y_val: Validation targets (optional)\n",
    "            weights_train: Training sample weights (optional)\n",
    "            weights_val: Validation sample weights (optional)\n",
    "            \n",
    "        Returns:\n",
    "            dict: Training history and statistics\n",
    "        \"\"\"\n",
    "        # Initialize weights if not already done\n",
    "        if model.weights is None:\n",
    "            model.weights = np.random.normal(0, 0.01, X_train.shape[1])\n",
    "            model.bias = 0.0\n",
    "            \n",
    "        # Initialize momentum variables\n",
    "        self.velocity_w = np.zeros_like(model.weights)\n",
    "        self.velocity_b = 0.0\n",
    "            \n",
    "        # Track best loss for early stopping\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Starting AGD with Backtracking Line Search\")\n",
    "            print(f\"Initial step size: {self.initial_learning_rate}, momentum: {self.momentum}\")\n",
    "            print(f\"c1: {self.c1}, rho: {self.rho}\")\n",
    "            print(f\"Early stopping: patience={self.early_stop_patience}, threshold={self.early_stop_threshold}\")\n",
    "            if weights_train is not None:\n",
    "                print(f\"Using weighted loss with training weights shape: {weights_train.shape}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.max_epochs):\n",
    "            # Nesterov momentum: compute gradients at the lookahead position\n",
    "            # Store current parameters\n",
    "            current_weights = model.weights.copy()\n",
    "            current_bias = model.bias\n",
    "            \n",
    "            # Move to lookahead position for gradient computation\n",
    "            lookahead_weights = current_weights + self.momentum * self.velocity_w\n",
    "            lookahead_bias = current_bias + self.momentum * self.velocity_b\n",
    "            \n",
    "            # Compute loss at current position (before update) for monitoring\n",
    "            train_loss = model._compute_loss(X_train, y_train, weights_train)\n",
    "            \n",
    "            # Compute gradients at lookahead position (weighted if weights provided)\n",
    "            model.weights = lookahead_weights\n",
    "            model.bias = lookahead_bias\n",
    "            dw, db = model._compute_gradient(X_train, y_train, weights_train)\n",
    "            \n",
    "            # Restore current position\n",
    "            model.weights = current_weights\n",
    "            model.bias = current_bias\n",
    "            \n",
    "            # Perform backtracking line search\n",
    "            step_size, backtrack_iter = self._backtracking_line_search_agd(\n",
    "                model, X_train, y_train, current_weights, current_bias,\n",
    "                self.velocity_w, self.velocity_b, dw, db, train_loss, weights_train\n",
    "            )\n",
    "            \n",
    "            # Update velocity with found step size\n",
    "            self.velocity_w = self.momentum * self.velocity_w - step_size * dw\n",
    "            self.velocity_b = self.momentum * self.velocity_b - step_size * db\n",
    "            \n",
    "            # Update parameters\n",
    "            model.weights = current_weights + self.velocity_w\n",
    "            model.bias = current_bias + self.velocity_b\n",
    "            \n",
    "            # Store history\n",
    "            self.loss_history.append(train_loss)\n",
    "            self.epoch_history.append(epoch)\n",
    "            self.step_size_history.append(step_size)\n",
    "            self.backtrack_iterations_history.append(backtrack_iter)\n",
    "            \n",
    "            # Check for early stopping\n",
    "            if train_loss < best_loss - self.early_stop_threshold:\n",
    "                best_loss = train_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            # Log progress\n",
    "            if (epoch + 1) % self.log_interval == 0 and self.verbose:\n",
    "                val_loss_str = \"\"\n",
    "                if X_val is not None and y_val is not None:\n",
    "                    val_loss = model._compute_loss(X_val, y_val, weights_val)\n",
    "                    val_loss_str = f\", Val Loss: {val_loss:.6f}\"\n",
    "                    \n",
    "                elapsed_time = time.time() - start_time\n",
    "                avg_step_size = np.mean(self.step_size_history[-self.log_interval:])\n",
    "                avg_backtrack = np.mean(self.backtrack_iterations_history[-self.log_interval:])\n",
    "                \n",
    "                print(f\"Epoch {epoch+1:4d} | Train Loss: {train_loss:.6f}{val_loss_str} | \"\n",
    "                      f\"Step Size: {step_size:.6f} | Avg Step: {avg_step_size:.6f} | \"\n",
    "                      f\"Momentum: {self.momentum:.3f} | Backtrack: {backtrack_iter:2d} | \"\n",
    "                      f\"Avg BT: {avg_backtrack:.1f} | Patience: {patience_counter:2d}/{self.early_stop_patience} | \"\n",
    "                      f\"Time: {elapsed_time:.2f}s\")\n",
    "                \n",
    "            # Early stopping check\n",
    "            if patience_counter >= self.early_stop_patience:\n",
    "                self.stopped_early = True\n",
    "                self.final_epoch = epoch + 1\n",
    "                if self.verbose:\n",
    "                    print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "                    print(f\"No improvement in loss for {self.early_stop_patience} epochs \"\n",
    "                          f\"(threshold: {self.early_stop_threshold})\")\n",
    "                break\n",
    "                \n",
    "        if not self.stopped_early:\n",
    "            self.final_epoch = self.max_epochs\n",
    "            if self.verbose:\n",
    "                print(f\"\\nTraining completed after {self.max_epochs} epochs\")\n",
    "                \n",
    "        training_time = time.time() - start_time\n",
    "        final_loss = self.loss_history[-1]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Final loss: {final_loss:.6f}\")\n",
    "            print(f\"Total training time: {training_time:.2f}s\")\n",
    "            print(f\"Average time per epoch: {training_time/self.final_epoch:.4f}s\")\n",
    "            print(f\"Average step size: {np.mean(self.step_size_history):.6f}\")\n",
    "            print(f\"Final step size: {self.step_size_history[-1]:.6f}\")\n",
    "            print(f\"Average backtrack iterations: {np.mean(self.backtrack_iterations_history):.2f}\")\n",
    "            \n",
    "        return {\n",
    "            'loss_history': self.loss_history.copy(),\n",
    "            'epoch_history': self.epoch_history.copy(),\n",
    "            'step_size_history': self.step_size_history.copy(),\n",
    "            'backtrack_iterations_history': self.backtrack_iterations_history.copy(),\n",
    "            'final_loss': final_loss,\n",
    "            'final_epoch': self.final_epoch,\n",
    "            'stopped_early': self.stopped_early,\n",
    "            'training_time': training_time,\n",
    "            'initial_learning_rate': self.initial_learning_rate,\n",
    "            'momentum': self.momentum,\n",
    "            'avg_step_size': np.mean(self.step_size_history),\n",
    "            'final_step_size': self.step_size_history[-1],\n",
    "            'avg_backtrack_iterations': np.mean(self.backtrack_iterations_history)\n",
    "        }\n",
    "    \n",
    "    def plot_training_curves(self, title=\"AGD Backtracking Training Curves\"):\n",
    "        \"\"\"Plot loss curve and step size evolution.\"\"\"\n",
    "        if not self.loss_history:\n",
    "            print(\"No training history available. Run optimize() first.\")\n",
    "            return\n",
    "            \n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10))\n",
    "        \n",
    "        # Plot 1: Loss curve\n",
    "        ax1.plot(self.epoch_history, self.loss_history, 'b-', linewidth=2, label='Training Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title(f'{title} - Loss Evolution (Momentum: {self.momentum})')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend()\n",
    "        \n",
    "        if self.stopped_early:\n",
    "            ax1.axvline(x=self.final_epoch-1, color='r', linestyle='--', alpha=0.7, \n",
    "                       label=f'Early Stop (Epoch {self.final_epoch})')\n",
    "            ax1.legend()\n",
    "        \n",
    "        # Plot 2: Step size evolution\n",
    "        ax2.plot(self.epoch_history, self.step_size_history, 'g-', linewidth=2, label='Step Size')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Step Size')\n",
    "        ax2.set_title('Step Size Evolution')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "        ax2.set_yscale('log')  # Log scale for better visualization\n",
    "        \n",
    "        # Plot 3: Backtrack iterations\n",
    "        ax3.plot(self.epoch_history, self.backtrack_iterations_history, 'r-', linewidth=2, \n",
    "                label='Backtrack Iterations')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Backtrack Iterations')\n",
    "        ax3.set_title('Backtracking Iterations per Epoch')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.legend()\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec5f7e0",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a882067",
   "metadata": {},
   "source": [
    "## SGD fixed learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3d48359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SGD training with learning rate: 0.01\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.597363 | LR: 0.0100 | Patience:  0/50 | Time: 0.54s\n",
      "Epoch   40 | Train Loss: 0.541136 | LR: 0.0100 | Patience:  0/50 | Time: 1.05s\n",
      "Epoch   60 | Train Loss: 0.508028 | LR: 0.0100 | Patience:  0/50 | Time: 1.56s\n",
      "Epoch   80 | Train Loss: 0.486606 | LR: 0.0100 | Patience:  0/50 | Time: 2.08s\n",
      "Epoch  100 | Train Loss: 0.471761 | LR: 0.0100 | Patience:  0/50 | Time: 2.61s\n",
      "Epoch  120 | Train Loss: 0.460933 | LR: 0.0100 | Patience:  0/50 | Time: 3.11s\n",
      "Epoch  140 | Train Loss: 0.452719 | LR: 0.0100 | Patience:  0/50 | Time: 3.65s\n",
      "Epoch  160 | Train Loss: 0.446285 | LR: 0.0100 | Patience:  0/50 | Time: 4.20s\n",
      "Epoch  180 | Train Loss: 0.441114 | LR: 0.0100 | Patience:  0/50 | Time: 4.73s\n",
      "Epoch  200 | Train Loss: 0.436865 | LR: 0.0100 | Patience:  0/50 | Time: 5.24s\n",
      "Epoch  220 | Train Loss: 0.433308 | LR: 0.0100 | Patience:  0/50 | Time: 5.77s\n",
      "Epoch  240 | Train Loss: 0.430281 | LR: 0.0100 | Patience:  0/50 | Time: 6.29s\n",
      "Epoch  260 | Train Loss: 0.427670 | LR: 0.0100 | Patience:  0/50 | Time: 6.79s\n",
      "Epoch  280 | Train Loss: 0.425388 | LR: 0.0100 | Patience:  0/50 | Time: 7.31s\n",
      "Epoch  300 | Train Loss: 0.423374 | LR: 0.0100 | Patience:  0/50 | Time: 7.88s\n",
      "Epoch  320 | Train Loss: 0.421578 | LR: 0.0100 | Patience:  0/50 | Time: 8.41s\n",
      "Epoch  340 | Train Loss: 0.419965 | LR: 0.0100 | Patience:  0/50 | Time: 8.92s\n",
      "Epoch  360 | Train Loss: 0.418504 | LR: 0.0100 | Patience:  0/50 | Time: 9.44s\n",
      "Epoch  380 | Train Loss: 0.417172 | LR: 0.0100 | Patience:  0/50 | Time: 9.99s\n",
      "Epoch  400 | Train Loss: 0.415952 | LR: 0.0100 | Patience:  0/50 | Time: 10.50s\n",
      "Epoch  420 | Train Loss: 0.414829 | LR: 0.0100 | Patience:  0/50 | Time: 11.03s\n",
      "Epoch  440 | Train Loss: 0.413789 | LR: 0.0100 | Patience:  0/50 | Time: 11.52s\n",
      "Epoch  460 | Train Loss: 0.412824 | LR: 0.0100 | Patience:  0/50 | Time: 12.01s\n",
      "Epoch  480 | Train Loss: 0.411925 | LR: 0.0100 | Patience:  0/50 | Time: 12.56s\n",
      "Epoch  500 | Train Loss: 0.411085 | LR: 0.0100 | Patience:  0/50 | Time: 13.06s\n",
      "Epoch  520 | Train Loss: 0.410298 | LR: 0.0100 | Patience:  0/50 | Time: 13.60s\n",
      "Epoch  540 | Train Loss: 0.409558 | LR: 0.0100 | Patience:  0/50 | Time: 14.09s\n",
      "Epoch  560 | Train Loss: 0.408862 | LR: 0.0100 | Patience:  0/50 | Time: 14.58s\n",
      "Epoch  580 | Train Loss: 0.408206 | LR: 0.0100 | Patience:  0/50 | Time: 15.08s\n",
      "Epoch  600 | Train Loss: 0.407587 | LR: 0.0100 | Patience:  0/50 | Time: 15.56s\n",
      "Epoch  620 | Train Loss: 0.407001 | LR: 0.0100 | Patience:  0/50 | Time: 16.04s\n",
      "Epoch  640 | Train Loss: 0.406447 | LR: 0.0100 | Patience:  0/50 | Time: 16.54s\n",
      "Epoch  660 | Train Loss: 0.405921 | LR: 0.0100 | Patience:  0/50 | Time: 17.03s\n",
      "Epoch  680 | Train Loss: 0.405422 | LR: 0.0100 | Patience:  0/50 | Time: 17.52s\n",
      "Epoch  700 | Train Loss: 0.404948 | LR: 0.0100 | Patience:  0/50 | Time: 18.00s\n",
      "Epoch  720 | Train Loss: 0.404497 | LR: 0.0100 | Patience:  0/50 | Time: 18.51s\n",
      "Epoch  740 | Train Loss: 0.404068 | LR: 0.0100 | Patience:  0/50 | Time: 18.98s\n",
      "Epoch  760 | Train Loss: 0.403660 | LR: 0.0100 | Patience:  0/50 | Time: 19.51s\n",
      "Epoch  780 | Train Loss: 0.403271 | LR: 0.0100 | Patience:  0/50 | Time: 20.02s\n",
      "Epoch  800 | Train Loss: 0.402901 | LR: 0.0100 | Patience:  0/50 | Time: 20.52s\n",
      "Epoch  820 | Train Loss: 0.402547 | LR: 0.0100 | Patience:  0/50 | Time: 21.14s\n",
      "Epoch  840 | Train Loss: 0.402210 | LR: 0.0100 | Patience:  0/50 | Time: 21.64s\n",
      "Epoch  860 | Train Loss: 0.401888 | LR: 0.0100 | Patience:  0/50 | Time: 22.19s\n",
      "Epoch  880 | Train Loss: 0.401580 | LR: 0.0100 | Patience:  0/50 | Time: 22.77s\n",
      "Epoch  900 | Train Loss: 0.401286 | LR: 0.0100 | Patience:  0/50 | Time: 23.39s\n",
      "Epoch  920 | Train Loss: 0.401005 | LR: 0.0100 | Patience:  0/50 | Time: 23.96s\n",
      "Epoch  940 | Train Loss: 0.400737 | LR: 0.0100 | Patience:  0/50 | Time: 24.49s\n",
      "Epoch  960 | Train Loss: 0.400480 | LR: 0.0100 | Patience:  0/50 | Time: 25.00s\n",
      "Epoch  980 | Train Loss: 0.400234 | LR: 0.0100 | Patience:  0/50 | Time: 25.50s\n",
      "Epoch 1000 | Train Loss: 0.399999 | LR: 0.0100 | Patience:  0/50 | Time: 26.06s\n",
      "Epoch 1020 | Train Loss: 0.399774 | LR: 0.0100 | Patience:  0/50 | Time: 26.58s\n",
      "Epoch 1040 | Train Loss: 0.399559 | LR: 0.0100 | Patience:  0/50 | Time: 27.12s\n",
      "Epoch 1060 | Train Loss: 0.399352 | LR: 0.0100 | Patience:  0/50 | Time: 27.73s\n",
      "Epoch 1080 | Train Loss: 0.399155 | LR: 0.0100 | Patience:  0/50 | Time: 28.30s\n",
      "Epoch 1100 | Train Loss: 0.398965 | LR: 0.0100 | Patience:  0/50 | Time: 28.82s\n",
      "Epoch 1120 | Train Loss: 0.398783 | LR: 0.0100 | Patience:  0/50 | Time: 29.36s\n",
      "Epoch 1140 | Train Loss: 0.398609 | LR: 0.0100 | Patience:  0/50 | Time: 29.92s\n",
      "Epoch 1160 | Train Loss: 0.398442 | LR: 0.0100 | Patience:  0/50 | Time: 30.45s\n",
      "Epoch 1180 | Train Loss: 0.398282 | LR: 0.0100 | Patience:  0/50 | Time: 30.97s\n",
      "Epoch 1200 | Train Loss: 0.398129 | LR: 0.0100 | Patience:  0/50 | Time: 31.50s\n",
      "Epoch 1220 | Train Loss: 0.397981 | LR: 0.0100 | Patience:  0/50 | Time: 32.07s\n",
      "Epoch 1240 | Train Loss: 0.397840 | LR: 0.0100 | Patience:  0/50 | Time: 32.61s\n",
      "Epoch 1260 | Train Loss: 0.397704 | LR: 0.0100 | Patience:  0/50 | Time: 33.23s\n",
      "Epoch 1280 | Train Loss: 0.397574 | LR: 0.0100 | Patience:  0/50 | Time: 33.77s\n",
      "Epoch 1300 | Train Loss: 0.397449 | LR: 0.0100 | Patience:  0/50 | Time: 34.29s\n",
      "Epoch 1320 | Train Loss: 0.397329 | LR: 0.0100 | Patience:  0/50 | Time: 34.82s\n",
      "Epoch 1340 | Train Loss: 0.397213 | LR: 0.0100 | Patience:  0/50 | Time: 35.39s\n",
      "Epoch 1360 | Train Loss: 0.397103 | LR: 0.0100 | Patience:  0/50 | Time: 35.95s\n",
      "Epoch 1380 | Train Loss: 0.396996 | LR: 0.0100 | Patience:  0/50 | Time: 36.52s\n",
      "Epoch 1400 | Train Loss: 0.396894 | LR: 0.0100 | Patience:  0/50 | Time: 37.07s\n",
      "Epoch 1420 | Train Loss: 0.396796 | LR: 0.0100 | Patience:  0/50 | Time: 37.64s\n",
      "Epoch 1440 | Train Loss: 0.396701 | LR: 0.0100 | Patience:  0/50 | Time: 38.19s\n",
      "Epoch 1460 | Train Loss: 0.396610 | LR: 0.0100 | Patience:  0/50 | Time: 38.72s\n",
      "Epoch 1480 | Train Loss: 0.396523 | LR: 0.0100 | Patience:  0/50 | Time: 39.26s\n",
      "Epoch 1500 | Train Loss: 0.396439 | LR: 0.0100 | Patience:  0/50 | Time: 39.77s\n",
      "Epoch 1520 | Train Loss: 0.396358 | LR: 0.0100 | Patience:  0/50 | Time: 40.31s\n",
      "Epoch 1540 | Train Loss: 0.396280 | LR: 0.0100 | Patience:  0/50 | Time: 40.87s\n",
      "Epoch 1560 | Train Loss: 0.396205 | LR: 0.0100 | Patience:  0/50 | Time: 41.44s\n",
      "Epoch 1580 | Train Loss: 0.396133 | LR: 0.0100 | Patience:  0/50 | Time: 41.99s\n",
      "Epoch 1600 | Train Loss: 0.396064 | LR: 0.0100 | Patience:  0/50 | Time: 42.61s\n",
      "Epoch 1620 | Train Loss: 0.395997 | LR: 0.0100 | Patience:  0/50 | Time: 43.21s\n",
      "Epoch 1640 | Train Loss: 0.395933 | LR: 0.0100 | Patience:  0/50 | Time: 43.76s\n",
      "Epoch 1660 | Train Loss: 0.395871 | LR: 0.0100 | Patience:  0/50 | Time: 44.30s\n",
      "Epoch 1680 | Train Loss: 0.395812 | LR: 0.0100 | Patience:  0/50 | Time: 44.87s\n",
      "Epoch 1700 | Train Loss: 0.395755 | LR: 0.0100 | Patience:  0/50 | Time: 45.46s\n",
      "Epoch 1720 | Train Loss: 0.395699 | LR: 0.0100 | Patience:  0/50 | Time: 45.99s\n",
      "Epoch 1740 | Train Loss: 0.395646 | LR: 0.0100 | Patience:  0/50 | Time: 46.59s\n",
      "Epoch 1760 | Train Loss: 0.395595 | LR: 0.0100 | Patience:  0/50 | Time: 47.39s\n",
      "Epoch 1780 | Train Loss: 0.395546 | LR: 0.0100 | Patience:  0/50 | Time: 47.90s\n",
      "Epoch 1800 | Train Loss: 0.395498 | LR: 0.0100 | Patience:  0/50 | Time: 48.52s\n",
      "Epoch 1820 | Train Loss: 0.395452 | LR: 0.0100 | Patience:  0/50 | Time: 49.09s\n",
      "Epoch 1840 | Train Loss: 0.395408 | LR: 0.0100 | Patience:  0/50 | Time: 49.60s\n",
      "Epoch 1860 | Train Loss: 0.395366 | LR: 0.0100 | Patience:  0/50 | Time: 50.12s\n",
      "Epoch 1880 | Train Loss: 0.395325 | LR: 0.0100 | Patience:  0/50 | Time: 50.61s\n",
      "Epoch 1900 | Train Loss: 0.395285 | LR: 0.0100 | Patience:  0/50 | Time: 51.18s\n",
      "Epoch 1920 | Train Loss: 0.395247 | LR: 0.0100 | Patience:  0/50 | Time: 51.70s\n",
      "Epoch 1940 | Train Loss: 0.395210 | LR: 0.0100 | Patience:  0/50 | Time: 52.24s\n",
      "Epoch 1960 | Train Loss: 0.395175 | LR: 0.0100 | Patience:  0/50 | Time: 52.77s\n",
      "Epoch 1980 | Train Loss: 0.395140 | LR: 0.0100 | Patience:  0/50 | Time: 53.39s\n",
      "Epoch 2000 | Train Loss: 0.395107 | LR: 0.0100 | Patience:  0/50 | Time: 53.92s\n",
      "Epoch 2020 | Train Loss: 0.395075 | LR: 0.0100 | Patience:  0/50 | Time: 54.51s\n",
      "Epoch 2040 | Train Loss: 0.395045 | LR: 0.0100 | Patience:  0/50 | Time: 55.01s\n",
      "Epoch 2060 | Train Loss: 0.395015 | LR: 0.0100 | Patience:  0/50 | Time: 55.57s\n",
      "Epoch 2080 | Train Loss: 0.394986 | LR: 0.0100 | Patience:  0/50 | Time: 56.09s\n",
      "Epoch 2100 | Train Loss: 0.394959 | LR: 0.0100 | Patience:  0/50 | Time: 56.64s\n",
      "Epoch 2120 | Train Loss: 0.394932 | LR: 0.0100 | Patience:  0/50 | Time: 57.16s\n",
      "Epoch 2140 | Train Loss: 0.394906 | LR: 0.0100 | Patience:  0/50 | Time: 57.71s\n",
      "Epoch 2160 | Train Loss: 0.394881 | LR: 0.0100 | Patience:  0/50 | Time: 58.29s\n",
      "Epoch 2180 | Train Loss: 0.394857 | LR: 0.0100 | Patience:  0/50 | Time: 58.93s\n",
      "Epoch 2200 | Train Loss: 0.394834 | LR: 0.0100 | Patience:  0/50 | Time: 59.61s\n",
      "Epoch 2220 | Train Loss: 0.394811 | LR: 0.0100 | Patience:  0/50 | Time: 60.14s\n",
      "Epoch 2240 | Train Loss: 0.394790 | LR: 0.0100 | Patience:  0/50 | Time: 60.68s\n",
      "Epoch 2260 | Train Loss: 0.394769 | LR: 0.0100 | Patience:  0/50 | Time: 61.20s\n",
      "Epoch 2280 | Train Loss: 0.394749 | LR: 0.0100 | Patience:  1/50 | Time: 61.76s\n",
      "Epoch 2300 | Train Loss: 0.394729 | LR: 0.0100 | Patience:  1/50 | Time: 62.27s\n",
      "Epoch 2320 | Train Loss: 0.394710 | LR: 0.0100 | Patience:  1/50 | Time: 62.78s\n",
      "Epoch 2340 | Train Loss: 0.394692 | LR: 0.0100 | Patience:  1/50 | Time: 63.29s\n",
      "Epoch 2360 | Train Loss: 0.394674 | LR: 0.0100 | Patience:  1/50 | Time: 63.90s\n",
      "Epoch 2380 | Train Loss: 0.394657 | LR: 0.0100 | Patience:  1/50 | Time: 64.47s\n",
      "Epoch 2400 | Train Loss: 0.394641 | LR: 0.0100 | Patience:  1/50 | Time: 65.05s\n",
      "Epoch 2420 | Train Loss: 0.394625 | LR: 0.0100 | Patience:  1/50 | Time: 65.64s\n",
      "Epoch 2440 | Train Loss: 0.394609 | LR: 0.0100 | Patience:  1/50 | Time: 66.14s\n",
      "Epoch 2460 | Train Loss: 0.394594 | LR: 0.0100 | Patience:  1/50 | Time: 66.63s\n",
      "Epoch 2480 | Train Loss: 0.394580 | LR: 0.0100 | Patience:  1/50 | Time: 67.27s\n",
      "Epoch 2500 | Train Loss: 0.394566 | LR: 0.0100 | Patience:  1/50 | Time: 67.79s\n",
      "Epoch 2520 | Train Loss: 0.394552 | LR: 0.0100 | Patience:  1/50 | Time: 68.31s\n",
      "Epoch 2540 | Train Loss: 0.394539 | LR: 0.0100 | Patience:  1/50 | Time: 68.82s\n",
      "Epoch 2560 | Train Loss: 0.394527 | LR: 0.0100 | Patience:  1/50 | Time: 69.42s\n",
      "Epoch 2580 | Train Loss: 0.394514 | LR: 0.0100 | Patience:  1/50 | Time: 70.09s\n",
      "Epoch 2600 | Train Loss: 0.394502 | LR: 0.0100 | Patience:  1/50 | Time: 70.63s\n",
      "Epoch 2620 | Train Loss: 0.394491 | LR: 0.0100 | Patience:  1/50 | Time: 71.16s\n",
      "Epoch 2640 | Train Loss: 0.394480 | LR: 0.0100 | Patience:  1/50 | Time: 71.75s\n",
      "Epoch 2660 | Train Loss: 0.394469 | LR: 0.0100 | Patience:  1/50 | Time: 72.24s\n",
      "Epoch 2680 | Train Loss: 0.394459 | LR: 0.0100 | Patience:  1/50 | Time: 72.74s\n",
      "Epoch 2700 | Train Loss: 0.394449 | LR: 0.0100 | Patience:  0/50 | Time: 73.30s\n",
      "Epoch 2720 | Train Loss: 0.394439 | LR: 0.0100 | Patience:  2/50 | Time: 73.88s\n",
      "Epoch 2740 | Train Loss: 0.394429 | LR: 0.0100 | Patience:  1/50 | Time: 74.53s\n",
      "Epoch 2760 | Train Loss: 0.394420 | LR: 0.0100 | Patience:  0/50 | Time: 75.20s\n",
      "Epoch 2780 | Train Loss: 0.394411 | LR: 0.0100 | Patience:  2/50 | Time: 75.77s\n",
      "Epoch 2800 | Train Loss: 0.394403 | LR: 0.0100 | Patience:  1/50 | Time: 76.29s\n",
      "Epoch 2820 | Train Loss: 0.394394 | LR: 0.0100 | Patience:  0/50 | Time: 76.84s\n",
      "Epoch 2840 | Train Loss: 0.394386 | LR: 0.0100 | Patience:  2/50 | Time: 77.39s\n",
      "Epoch 2860 | Train Loss: 0.394378 | LR: 0.0100 | Patience:  1/50 | Time: 77.97s\n",
      "Epoch 2880 | Train Loss: 0.394371 | LR: 0.0100 | Patience:  0/50 | Time: 78.56s\n",
      "Epoch 2900 | Train Loss: 0.394363 | LR: 0.0100 | Patience:  2/50 | Time: 79.16s\n",
      "Epoch 2920 | Train Loss: 0.394356 | LR: 0.0100 | Patience:  1/50 | Time: 79.65s\n",
      "Epoch 2940 | Train Loss: 0.394349 | LR: 0.0100 | Patience:  0/50 | Time: 80.15s\n",
      "Epoch 2960 | Train Loss: 0.394342 | LR: 0.0100 | Patience:  2/50 | Time: 80.66s\n",
      "Epoch 2980 | Train Loss: 0.394336 | LR: 0.0100 | Patience:  2/50 | Time: 81.23s\n",
      "Epoch 3000 | Train Loss: 0.394329 | LR: 0.0100 | Patience:  2/50 | Time: 81.74s\n",
      "Epoch 3020 | Train Loss: 0.394323 | LR: 0.0100 | Patience:  2/50 | Time: 82.24s\n",
      "Epoch 3040 | Train Loss: 0.394317 | LR: 0.0100 | Patience:  2/50 | Time: 82.79s\n",
      "Epoch 3060 | Train Loss: 0.394311 | LR: 0.0100 | Patience:  2/50 | Time: 83.37s\n",
      "Epoch 3080 | Train Loss: 0.394306 | LR: 0.0100 | Patience:  2/50 | Time: 83.95s\n",
      "Epoch 3100 | Train Loss: 0.394300 | LR: 0.0100 | Patience:  2/50 | Time: 84.51s\n",
      "Epoch 3120 | Train Loss: 0.394295 | LR: 0.0100 | Patience:  2/50 | Time: 85.08s\n",
      "Epoch 3140 | Train Loss: 0.394290 | LR: 0.0100 | Patience:  2/50 | Time: 85.68s\n",
      "Epoch 3160 | Train Loss: 0.394285 | LR: 0.0100 | Patience:  1/50 | Time: 86.37s\n",
      "Epoch 3180 | Train Loss: 0.394280 | LR: 0.0100 | Patience:  1/50 | Time: 86.97s\n",
      "Epoch 3200 | Train Loss: 0.394275 | LR: 0.0100 | Patience:  1/50 | Time: 87.54s\n",
      "Epoch 3220 | Train Loss: 0.394271 | LR: 0.0100 | Patience:  1/50 | Time: 88.17s\n",
      "Epoch 3240 | Train Loss: 0.394266 | LR: 0.0100 | Patience:  1/50 | Time: 88.77s\n",
      "Epoch 3260 | Train Loss: 0.394262 | LR: 0.0100 | Patience:  1/50 | Time: 89.32s\n",
      "Epoch 3280 | Train Loss: 0.394258 | LR: 0.0100 | Patience:  1/50 | Time: 89.87s\n",
      "Epoch 3300 | Train Loss: 0.394253 | LR: 0.0100 | Patience:  1/50 | Time: 90.44s\n",
      "Epoch 3320 | Train Loss: 0.394249 | LR: 0.0100 | Patience:  5/50 | Time: 91.00s\n",
      "Epoch 3340 | Train Loss: 0.394246 | LR: 0.0100 | Patience:  1/50 | Time: 91.67s\n",
      "Epoch 3360 | Train Loss: 0.394242 | LR: 0.0100 | Patience:  3/50 | Time: 92.32s\n",
      "Epoch 3380 | Train Loss: 0.394238 | LR: 0.0100 | Patience:  5/50 | Time: 92.89s\n",
      "Epoch 3400 | Train Loss: 0.394234 | LR: 0.0100 | Patience:  1/50 | Time: 93.46s\n",
      "Epoch 3420 | Train Loss: 0.394231 | LR: 0.0100 | Patience:  3/50 | Time: 94.00s\n",
      "Epoch 3440 | Train Loss: 0.394228 | LR: 0.0100 | Patience:  5/50 | Time: 94.54s\n",
      "Epoch 3460 | Train Loss: 0.394224 | LR: 0.0100 | Patience:  6/50 | Time: 95.06s\n",
      "Epoch 3480 | Train Loss: 0.394221 | LR: 0.0100 | Patience:  5/50 | Time: 95.57s\n",
      "Epoch 3500 | Train Loss: 0.394218 | LR: 0.0100 | Patience:  4/50 | Time: 96.19s\n",
      "Epoch 3520 | Train Loss: 0.394215 | LR: 0.0100 | Patience:  3/50 | Time: 96.79s\n",
      "Epoch 3540 | Train Loss: 0.394212 | LR: 0.0100 | Patience:  2/50 | Time: 97.36s\n",
      "Epoch 3560 | Train Loss: 0.394209 | LR: 0.0100 | Patience:  1/50 | Time: 97.93s\n",
      "Epoch 3580 | Train Loss: 0.394206 | LR: 0.0100 | Patience:  6/50 | Time: 98.46s\n",
      "Epoch 3600 | Train Loss: 0.394203 | LR: 0.0100 | Patience:  2/50 | Time: 98.96s\n",
      "Epoch 3620 | Train Loss: 0.394201 | LR: 0.0100 | Patience:  6/50 | Time: 99.47s\n",
      "Epoch 3640 | Train Loss: 0.394198 | LR: 0.0100 | Patience:  2/50 | Time: 99.99s\n",
      "Epoch 3660 | Train Loss: 0.394196 | LR: 0.0100 | Patience:  6/50 | Time: 100.54s\n",
      "Epoch 3680 | Train Loss: 0.394193 | LR: 0.0100 | Patience:  1/50 | Time: 101.08s\n",
      "Epoch 3700 | Train Loss: 0.394191 | LR: 0.0100 | Patience:  3/50 | Time: 101.67s\n",
      "Epoch 3720 | Train Loss: 0.394188 | LR: 0.0100 | Patience:  5/50 | Time: 102.21s\n",
      "Epoch 3740 | Train Loss: 0.394186 | LR: 0.0100 | Patience:  7/50 | Time: 102.75s\n",
      "Epoch 3760 | Train Loss: 0.394184 | LR: 0.0100 | Patience:  0/50 | Time: 103.32s\n",
      "Epoch 3780 | Train Loss: 0.394181 | LR: 0.0100 | Patience:  1/50 | Time: 103.95s\n",
      "Epoch 3800 | Train Loss: 0.394179 | LR: 0.0100 | Patience:  1/50 | Time: 104.51s\n",
      "Epoch 3820 | Train Loss: 0.394177 | LR: 0.0100 | Patience:  1/50 | Time: 105.07s\n",
      "Epoch 3840 | Train Loss: 0.394175 | LR: 0.0100 | Patience:  1/50 | Time: 105.69s\n",
      "Epoch 3860 | Train Loss: 0.394173 | LR: 0.0100 | Patience:  1/50 | Time: 106.29s\n",
      "Epoch 3880 | Train Loss: 0.394171 | LR: 0.0100 | Patience: 10/50 | Time: 106.78s\n",
      "Epoch 3900 | Train Loss: 0.394169 | LR: 0.0100 | Patience:  8/50 | Time: 107.45s\n",
      "Epoch 3920 | Train Loss: 0.394167 | LR: 0.0100 | Patience:  6/50 | Time: 107.96s\n",
      "Epoch 3940 | Train Loss: 0.394165 | LR: 0.0100 | Patience:  4/50 | Time: 108.50s\n",
      "Epoch 3960 | Train Loss: 0.394164 | LR: 0.0100 | Patience:  0/50 | Time: 109.04s\n",
      "Epoch 3980 | Train Loss: 0.394162 | LR: 0.0100 | Patience:  8/50 | Time: 109.55s\n",
      "Epoch 4000 | Train Loss: 0.394160 | LR: 0.0100 | Patience:  4/50 | Time: 110.06s\n",
      "Epoch 4020 | Train Loss: 0.394158 | LR: 0.0100 | Patience:  0/50 | Time: 110.64s\n",
      "Epoch 4040 | Train Loss: 0.394157 | LR: 0.0100 | Patience:  7/50 | Time: 111.14s\n",
      "Epoch 4060 | Train Loss: 0.394155 | LR: 0.0100 | Patience:  1/50 | Time: 111.63s\n",
      "Epoch 4080 | Train Loss: 0.394154 | LR: 0.0100 | Patience:  8/50 | Time: 112.15s\n",
      "Epoch 4100 | Train Loss: 0.394152 | LR: 0.0100 | Patience:  2/50 | Time: 112.66s\n",
      "Epoch 4120 | Train Loss: 0.394151 | LR: 0.0100 | Patience:  8/50 | Time: 113.16s\n",
      "Epoch 4140 | Train Loss: 0.394149 | LR: 0.0100 | Patience:  0/50 | Time: 113.67s\n",
      "Epoch 4160 | Train Loss: 0.394148 | LR: 0.0100 | Patience:  6/50 | Time: 114.26s\n",
      "Epoch 4180 | Train Loss: 0.394146 | LR: 0.0100 | Patience: 12/50 | Time: 114.87s\n",
      "Epoch 4200 | Train Loss: 0.394145 | LR: 0.0100 | Patience:  2/50 | Time: 115.52s\n",
      "Epoch 4220 | Train Loss: 0.394143 | LR: 0.0100 | Patience:  7/50 | Time: 116.13s\n",
      "Epoch 4240 | Train Loss: 0.394142 | LR: 0.0100 | Patience: 12/50 | Time: 116.71s\n",
      "Epoch 4260 | Train Loss: 0.394141 | LR: 0.0100 | Patience:  1/50 | Time: 117.30s\n",
      "Epoch 4280 | Train Loss: 0.394139 | LR: 0.0100 | Patience:  5/50 | Time: 118.04s\n",
      "Epoch 4300 | Train Loss: 0.394138 | LR: 0.0100 | Patience:  9/50 | Time: 118.66s\n",
      "Epoch 4320 | Train Loss: 0.394137 | LR: 0.0100 | Patience: 13/50 | Time: 119.21s\n",
      "Epoch 4340 | Train Loss: 0.394136 | LR: 0.0100 | Patience: 16/50 | Time: 119.81s\n",
      "Epoch 4360 | Train Loss: 0.394134 | LR: 0.0100 | Patience:  2/50 | Time: 120.49s\n",
      "Epoch 4380 | Train Loss: 0.394133 | LR: 0.0100 | Patience:  5/50 | Time: 121.00s\n",
      "Epoch 4400 | Train Loss: 0.394132 | LR: 0.0100 | Patience:  7/50 | Time: 121.51s\n",
      "Epoch 4420 | Train Loss: 0.394131 | LR: 0.0100 | Patience:  9/50 | Time: 122.06s\n",
      "Epoch 4440 | Train Loss: 0.394130 | LR: 0.0100 | Patience: 11/50 | Time: 122.63s\n",
      "Epoch 4460 | Train Loss: 0.394129 | LR: 0.0100 | Patience: 13/50 | Time: 123.13s\n",
      "Epoch 4480 | Train Loss: 0.394128 | LR: 0.0100 | Patience: 14/50 | Time: 123.63s\n",
      "Epoch 4500 | Train Loss: 0.394127 | LR: 0.0100 | Patience: 15/50 | Time: 124.15s\n",
      "Epoch 4520 | Train Loss: 0.394126 | LR: 0.0100 | Patience: 16/50 | Time: 124.72s\n",
      "Epoch 4540 | Train Loss: 0.394125 | LR: 0.0100 | Patience: 16/50 | Time: 125.28s\n",
      "Epoch 4560 | Train Loss: 0.394123 | LR: 0.0100 | Patience: 16/50 | Time: 125.83s\n",
      "Epoch 4580 | Train Loss: 0.394122 | LR: 0.0100 | Patience: 16/50 | Time: 126.39s\n",
      "Epoch 4600 | Train Loss: 0.394121 | LR: 0.0100 | Patience: 15/50 | Time: 127.11s\n",
      "Epoch 4620 | Train Loss: 0.394121 | LR: 0.0100 | Patience: 14/50 | Time: 127.66s\n",
      "Epoch 4640 | Train Loss: 0.394120 | LR: 0.0100 | Patience: 13/50 | Time: 128.19s\n",
      "Epoch 4660 | Train Loss: 0.394119 | LR: 0.0100 | Patience: 11/50 | Time: 128.68s\n",
      "Epoch 4680 | Train Loss: 0.394118 | LR: 0.0100 | Patience:  9/50 | Time: 129.23s\n",
      "Epoch 4700 | Train Loss: 0.394117 | LR: 0.0100 | Patience:  7/50 | Time: 129.76s\n",
      "Epoch 4720 | Train Loss: 0.394116 | LR: 0.0100 | Patience:  4/50 | Time: 130.30s\n",
      "Epoch 4740 | Train Loss: 0.394115 | LR: 0.0100 | Patience:  1/50 | Time: 130.88s\n",
      "Epoch 4760 | Train Loss: 0.394114 | LR: 0.0100 | Patience: 21/50 | Time: 131.41s\n",
      "Epoch 4780 | Train Loss: 0.394113 | LR: 0.0100 | Patience: 18/50 | Time: 131.94s\n",
      "Epoch 4800 | Train Loss: 0.394112 | LR: 0.0100 | Patience: 14/50 | Time: 132.44s\n",
      "Epoch 4820 | Train Loss: 0.394112 | LR: 0.0100 | Patience: 10/50 | Time: 132.94s\n",
      "Epoch 4840 | Train Loss: 0.394111 | LR: 0.0100 | Patience:  5/50 | Time: 133.59s\n",
      "Epoch 4860 | Train Loss: 0.394110 | LR: 0.0100 | Patience:  0/50 | Time: 134.15s\n",
      "Epoch 4880 | Train Loss: 0.394109 | LR: 0.0100 | Patience: 20/50 | Time: 134.75s\n",
      "Epoch 4900 | Train Loss: 0.394108 | LR: 0.0100 | Patience: 15/50 | Time: 135.30s\n",
      "Epoch 4920 | Train Loss: 0.394108 | LR: 0.0100 | Patience:  9/50 | Time: 135.84s\n",
      "Epoch 4940 | Train Loss: 0.394107 | LR: 0.0100 | Patience:  3/50 | Time: 136.44s\n",
      "Epoch 4960 | Train Loss: 0.394106 | LR: 0.0100 | Patience: 23/50 | Time: 136.99s\n",
      "Epoch 4980 | Train Loss: 0.394105 | LR: 0.0100 | Patience: 16/50 | Time: 137.63s\n",
      "Epoch 5000 | Train Loss: 0.394104 | LR: 0.0100 | Patience:  9/50 | Time: 138.15s\n",
      "Epoch 5020 | Train Loss: 0.394104 | LR: 0.0100 | Patience:  2/50 | Time: 138.68s\n",
      "Epoch 5040 | Train Loss: 0.394103 | LR: 0.0100 | Patience: 22/50 | Time: 139.25s\n",
      "Epoch 5060 | Train Loss: 0.394102 | LR: 0.0100 | Patience: 14/50 | Time: 139.91s\n",
      "Epoch 5080 | Train Loss: 0.394102 | LR: 0.0100 | Patience:  6/50 | Time: 140.46s\n",
      "Epoch 5100 | Train Loss: 0.394101 | LR: 0.0100 | Patience: 26/50 | Time: 141.07s\n",
      "Epoch 5120 | Train Loss: 0.394100 | LR: 0.0100 | Patience: 17/50 | Time: 141.65s\n",
      "Epoch 5140 | Train Loss: 0.394099 | LR: 0.0100 | Patience:  8/50 | Time: 142.26s\n",
      "Epoch 5160 | Train Loss: 0.394099 | LR: 0.0100 | Patience: 28/50 | Time: 142.87s\n",
      "Epoch 5180 | Train Loss: 0.394098 | LR: 0.0100 | Patience: 18/50 | Time: 143.43s\n",
      "Epoch 5200 | Train Loss: 0.394097 | LR: 0.0100 | Patience:  8/50 | Time: 144.05s\n",
      "Epoch 5220 | Train Loss: 0.394097 | LR: 0.0100 | Patience: 28/50 | Time: 144.70s\n",
      "Epoch 5240 | Train Loss: 0.394096 | LR: 0.0100 | Patience: 17/50 | Time: 145.24s\n",
      "Epoch 5260 | Train Loss: 0.394096 | LR: 0.0100 | Patience:  6/50 | Time: 145.75s\n",
      "Epoch 5280 | Train Loss: 0.394095 | LR: 0.0100 | Patience: 26/50 | Time: 146.32s\n",
      "Epoch 5300 | Train Loss: 0.394094 | LR: 0.0100 | Patience: 14/50 | Time: 146.99s\n",
      "Epoch 5320 | Train Loss: 0.394094 | LR: 0.0100 | Patience:  2/50 | Time: 147.63s\n",
      "Epoch 5340 | Train Loss: 0.394093 | LR: 0.0100 | Patience: 22/50 | Time: 148.26s\n",
      "Epoch 5360 | Train Loss: 0.394092 | LR: 0.0100 | Patience:  9/50 | Time: 148.78s\n",
      "Epoch 5380 | Train Loss: 0.394092 | LR: 0.0100 | Patience: 29/50 | Time: 149.40s\n",
      "Epoch 5400 | Train Loss: 0.394091 | LR: 0.0100 | Patience: 16/50 | Time: 149.93s\n",
      "Epoch 5420 | Train Loss: 0.394091 | LR: 0.0100 | Patience:  2/50 | Time: 150.44s\n",
      "Epoch 5440 | Train Loss: 0.394090 | LR: 0.0100 | Patience: 22/50 | Time: 150.93s\n",
      "Epoch 5460 | Train Loss: 0.394089 | LR: 0.0100 | Patience:  8/50 | Time: 151.64s\n",
      "Epoch 5480 | Train Loss: 0.394089 | LR: 0.0100 | Patience: 28/50 | Time: 152.24s\n",
      "Epoch 5500 | Train Loss: 0.394088 | LR: 0.0100 | Patience: 13/50 | Time: 152.80s\n",
      "Epoch 5520 | Train Loss: 0.394088 | LR: 0.0100 | Patience: 33/50 | Time: 153.44s\n",
      "Epoch 5540 | Train Loss: 0.394087 | LR: 0.0100 | Patience: 18/50 | Time: 153.98s\n",
      "Epoch 5560 | Train Loss: 0.394087 | LR: 0.0100 | Patience:  2/50 | Time: 154.49s\n",
      "Epoch 5580 | Train Loss: 0.394086 | LR: 0.0100 | Patience: 22/50 | Time: 155.02s\n",
      "Epoch 5600 | Train Loss: 0.394085 | LR: 0.0100 | Patience:  5/50 | Time: 155.53s\n",
      "Epoch 5620 | Train Loss: 0.394085 | LR: 0.0100 | Patience: 25/50 | Time: 156.10s\n",
      "Epoch 5640 | Train Loss: 0.394084 | LR: 0.0100 | Patience:  8/50 | Time: 156.63s\n",
      "Epoch 5660 | Train Loss: 0.394084 | LR: 0.0100 | Patience: 28/50 | Time: 157.14s\n",
      "Epoch 5680 | Train Loss: 0.394083 | LR: 0.0100 | Patience: 10/50 | Time: 157.71s\n",
      "Epoch 5700 | Train Loss: 0.394083 | LR: 0.0100 | Patience: 30/50 | Time: 158.33s\n",
      "Epoch 5720 | Train Loss: 0.394082 | LR: 0.0100 | Patience: 12/50 | Time: 158.89s\n",
      "Epoch 5740 | Train Loss: 0.394082 | LR: 0.0100 | Patience: 32/50 | Time: 159.38s\n",
      "Epoch 5760 | Train Loss: 0.394081 | LR: 0.0100 | Patience: 13/50 | Time: 159.92s\n",
      "Epoch 5780 | Train Loss: 0.394081 | LR: 0.0100 | Patience: 33/50 | Time: 160.49s\n",
      "Epoch 5800 | Train Loss: 0.394080 | LR: 0.0100 | Patience: 14/50 | Time: 161.13s\n",
      "Epoch 5820 | Train Loss: 0.394080 | LR: 0.0100 | Patience: 34/50 | Time: 161.66s\n",
      "Epoch 5840 | Train Loss: 0.394079 | LR: 0.0100 | Patience: 14/50 | Time: 162.25s\n",
      "Epoch 5860 | Train Loss: 0.394079 | LR: 0.0100 | Patience: 34/50 | Time: 162.83s\n",
      "Epoch 5880 | Train Loss: 0.394078 | LR: 0.0100 | Patience: 13/50 | Time: 163.45s\n",
      "Epoch 5900 | Train Loss: 0.394078 | LR: 0.0100 | Patience: 33/50 | Time: 164.16s\n",
      "Epoch 5920 | Train Loss: 0.394077 | LR: 0.0100 | Patience: 12/50 | Time: 164.84s\n",
      "Epoch 5940 | Train Loss: 0.394077 | LR: 0.0100 | Patience: 32/50 | Time: 165.63s\n",
      "Epoch 5960 | Train Loss: 0.394076 | LR: 0.0100 | Patience: 10/50 | Time: 166.21s\n",
      "Epoch 5980 | Train Loss: 0.394076 | LR: 0.0100 | Patience: 30/50 | Time: 166.77s\n",
      "Epoch 6000 | Train Loss: 0.394075 | LR: 0.0100 | Patience:  7/50 | Time: 167.39s\n",
      "Epoch 6020 | Train Loss: 0.394075 | LR: 0.0100 | Patience: 27/50 | Time: 167.93s\n",
      "Epoch 6040 | Train Loss: 0.394074 | LR: 0.0100 | Patience:  4/50 | Time: 168.47s\n",
      "Epoch 6060 | Train Loss: 0.394074 | LR: 0.0100 | Patience: 24/50 | Time: 169.11s\n",
      "Epoch 6080 | Train Loss: 0.394073 | LR: 0.0100 | Patience:  0/50 | Time: 169.75s\n",
      "Epoch 6100 | Train Loss: 0.394073 | LR: 0.0100 | Patience: 20/50 | Time: 170.30s\n",
      "Epoch 6120 | Train Loss: 0.394073 | LR: 0.0100 | Patience: 40/50 | Time: 170.84s\n",
      "Epoch 6140 | Train Loss: 0.394072 | LR: 0.0100 | Patience: 16/50 | Time: 171.43s\n",
      "Epoch 6160 | Train Loss: 0.394072 | LR: 0.0100 | Patience: 36/50 | Time: 171.94s\n",
      "Epoch 6180 | Train Loss: 0.394071 | LR: 0.0100 | Patience: 11/50 | Time: 172.48s\n",
      "Epoch 6200 | Train Loss: 0.394071 | LR: 0.0100 | Patience: 31/50 | Time: 172.98s\n",
      "Epoch 6220 | Train Loss: 0.394070 | LR: 0.0100 | Patience:  5/50 | Time: 173.66s\n",
      "Epoch 6240 | Train Loss: 0.394070 | LR: 0.0100 | Patience: 25/50 | Time: 174.26s\n",
      "Epoch 6260 | Train Loss: 0.394069 | LR: 0.0100 | Patience: 45/50 | Time: 174.82s\n",
      "Epoch 6280 | Train Loss: 0.394069 | LR: 0.0100 | Patience: 19/50 | Time: 175.50s\n",
      "Epoch 6300 | Train Loss: 0.394069 | LR: 0.0100 | Patience: 39/50 | Time: 176.04s\n",
      "Epoch 6320 | Train Loss: 0.394068 | LR: 0.0100 | Patience: 12/50 | Time: 176.66s\n",
      "Epoch 6340 | Train Loss: 0.394068 | LR: 0.0100 | Patience: 32/50 | Time: 177.23s\n",
      "Epoch 6360 | Train Loss: 0.394067 | LR: 0.0100 | Patience:  4/50 | Time: 177.89s\n",
      "Epoch 6380 | Train Loss: 0.394067 | LR: 0.0100 | Patience: 24/50 | Time: 178.39s\n",
      "Epoch 6400 | Train Loss: 0.394066 | LR: 0.0100 | Patience: 44/50 | Time: 178.98s\n",
      "Epoch 6420 | Train Loss: 0.394066 | LR: 0.0100 | Patience: 16/50 | Time: 179.59s\n",
      "Epoch 6440 | Train Loss: 0.394066 | LR: 0.0100 | Patience: 36/50 | Time: 180.16s\n",
      "Epoch 6460 | Train Loss: 0.394065 | LR: 0.0100 | Patience:  7/50 | Time: 180.76s\n",
      "Epoch 6480 | Train Loss: 0.394065 | LR: 0.0100 | Patience: 27/50 | Time: 181.61s\n",
      "Epoch 6500 | Train Loss: 0.394064 | LR: 0.0100 | Patience: 47/50 | Time: 182.24s\n",
      "Epoch 6520 | Train Loss: 0.394064 | LR: 0.0100 | Patience: 17/50 | Time: 182.84s\n",
      "Epoch 6540 | Train Loss: 0.394064 | LR: 0.0100 | Patience: 37/50 | Time: 183.37s\n",
      "Epoch 6560 | Train Loss: 0.394063 | LR: 0.0100 | Patience:  7/50 | Time: 183.99s\n",
      "Epoch 6580 | Train Loss: 0.394063 | LR: 0.0100 | Patience: 27/50 | Time: 184.60s\n",
      "Epoch 6600 | Train Loss: 0.394062 | LR: 0.0100 | Patience: 47/50 | Time: 185.12s\n",
      "\n",
      "Early stopping triggered at epoch 6603\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.394062\n",
      "Total training time: 185.20s\n",
      "Average time per epoch: 0.0280s\n",
      "Weighted final loss:   0.394062\n",
      "Weighted final epoch:   6603\n",
      "Starting SGD training with learning rate: 0.05\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.394061 | LR: 0.0500 | Patience:  8/50 | Time: 0.58s\n",
      "Epoch   40 | Train Loss: 0.394059 | LR: 0.0500 | Patience:  6/50 | Time: 1.11s\n",
      "Epoch   60 | Train Loss: 0.394057 | LR: 0.0500 | Patience:  4/50 | Time: 1.69s\n",
      "Epoch   80 | Train Loss: 0.394055 | LR: 0.0500 | Patience:  0/50 | Time: 2.19s\n",
      "Epoch  100 | Train Loss: 0.394053 | LR: 0.0500 | Patience:  8/50 | Time: 2.72s\n",
      "Epoch  120 | Train Loss: 0.394052 | LR: 0.0500 | Patience:  4/50 | Time: 3.23s\n",
      "Epoch  140 | Train Loss: 0.394050 | LR: 0.0500 | Patience: 12/50 | Time: 3.77s\n",
      "Epoch  160 | Train Loss: 0.394048 | LR: 0.0500 | Patience:  6/50 | Time: 4.32s\n",
      "Epoch  180 | Train Loss: 0.394047 | LR: 0.0500 | Patience:  0/50 | Time: 4.89s\n",
      "Epoch  200 | Train Loss: 0.394045 | LR: 0.0500 | Patience:  7/50 | Time: 5.46s\n",
      "Epoch  220 | Train Loss: 0.394044 | LR: 0.0500 | Patience: 13/50 | Time: 6.15s\n",
      "Epoch  240 | Train Loss: 0.394042 | LR: 0.0500 | Patience:  5/50 | Time: 6.78s\n",
      "Epoch  260 | Train Loss: 0.394041 | LR: 0.0500 | Patience: 11/50 | Time: 7.39s\n",
      "Epoch  280 | Train Loss: 0.394039 | LR: 0.0500 | Patience:  2/50 | Time: 7.89s\n",
      "Epoch  300 | Train Loss: 0.394038 | LR: 0.0500 | Patience:  7/50 | Time: 8.41s\n",
      "Epoch  320 | Train Loss: 0.394037 | LR: 0.0500 | Patience: 12/50 | Time: 8.96s\n",
      "Epoch  340 | Train Loss: 0.394035 | LR: 0.0500 | Patience:  1/50 | Time: 9.50s\n",
      "Epoch  360 | Train Loss: 0.394034 | LR: 0.0500 | Patience:  5/50 | Time: 10.03s\n",
      "Epoch  380 | Train Loss: 0.394033 | LR: 0.0500 | Patience:  9/50 | Time: 10.55s\n",
      "Epoch  400 | Train Loss: 0.394031 | LR: 0.0500 | Patience: 13/50 | Time: 11.13s\n",
      "Epoch  420 | Train Loss: 0.394030 | LR: 0.0500 | Patience: 16/50 | Time: 11.66s\n",
      "Epoch  440 | Train Loss: 0.394029 | LR: 0.0500 | Patience:  2/50 | Time: 12.23s\n",
      "Epoch  460 | Train Loss: 0.394028 | LR: 0.0500 | Patience:  5/50 | Time: 12.73s\n",
      "Epoch  480 | Train Loss: 0.394027 | LR: 0.0500 | Patience:  7/50 | Time: 13.27s\n",
      "Epoch  500 | Train Loss: 0.394025 | LR: 0.0500 | Patience:  9/50 | Time: 13.91s\n",
      "Epoch  520 | Train Loss: 0.394024 | LR: 0.0500 | Patience: 11/50 | Time: 14.50s\n",
      "Epoch  540 | Train Loss: 0.394023 | LR: 0.0500 | Patience: 12/50 | Time: 15.13s\n",
      "Epoch  560 | Train Loss: 0.394022 | LR: 0.0500 | Patience: 13/50 | Time: 15.64s\n",
      "Epoch  580 | Train Loss: 0.394021 | LR: 0.0500 | Patience: 13/50 | Time: 16.26s\n",
      "Epoch  600 | Train Loss: 0.394020 | LR: 0.0500 | Patience: 13/50 | Time: 16.81s\n",
      "Epoch  620 | Train Loss: 0.394019 | LR: 0.0500 | Patience: 13/50 | Time: 17.36s\n",
      "Epoch  640 | Train Loss: 0.394018 | LR: 0.0500 | Patience: 12/50 | Time: 17.93s\n",
      "Epoch  660 | Train Loss: 0.394017 | LR: 0.0500 | Patience: 11/50 | Time: 18.48s\n",
      "Epoch  680 | Train Loss: 0.394016 | LR: 0.0500 | Patience:  9/50 | Time: 19.01s\n",
      "Epoch  700 | Train Loss: 0.394015 | LR: 0.0500 | Patience:  7/50 | Time: 19.55s\n",
      "Epoch  720 | Train Loss: 0.394014 | LR: 0.0500 | Patience:  4/50 | Time: 20.06s\n",
      "Epoch  740 | Train Loss: 0.394014 | LR: 0.0500 | Patience:  1/50 | Time: 20.58s\n",
      "Epoch  760 | Train Loss: 0.394013 | LR: 0.0500 | Patience: 21/50 | Time: 21.10s\n",
      "Epoch  780 | Train Loss: 0.394012 | LR: 0.0500 | Patience: 17/50 | Time: 21.60s\n",
      "Epoch  800 | Train Loss: 0.394011 | LR: 0.0500 | Patience: 13/50 | Time: 22.10s\n",
      "Epoch  820 | Train Loss: 0.394010 | LR: 0.0500 | Patience:  8/50 | Time: 22.63s\n",
      "Epoch  840 | Train Loss: 0.394009 | LR: 0.0500 | Patience:  3/50 | Time: 23.33s\n",
      "Epoch  860 | Train Loss: 0.394009 | LR: 0.0500 | Patience: 23/50 | Time: 23.91s\n",
      "Epoch  880 | Train Loss: 0.394008 | LR: 0.0500 | Patience: 17/50 | Time: 24.43s\n",
      "Epoch  900 | Train Loss: 0.394007 | LR: 0.0500 | Patience: 10/50 | Time: 24.94s\n",
      "Epoch  920 | Train Loss: 0.394006 | LR: 0.0500 | Patience:  3/50 | Time: 25.45s\n",
      "Epoch  940 | Train Loss: 0.394006 | LR: 0.0500 | Patience: 23/50 | Time: 25.95s\n",
      "Epoch  960 | Train Loss: 0.394005 | LR: 0.0500 | Patience: 15/50 | Time: 26.49s\n",
      "Epoch  980 | Train Loss: 0.394004 | LR: 0.0500 | Patience:  6/50 | Time: 27.03s\n",
      "Epoch 1000 | Train Loss: 0.394004 | LR: 0.0500 | Patience: 26/50 | Time: 27.58s\n",
      "Epoch 1020 | Train Loss: 0.394003 | LR: 0.0500 | Patience: 17/50 | Time: 28.18s\n",
      "Epoch 1040 | Train Loss: 0.394002 | LR: 0.0500 | Patience:  7/50 | Time: 28.71s\n",
      "Epoch 1060 | Train Loss: 0.394002 | LR: 0.0500 | Patience: 27/50 | Time: 29.22s\n",
      "Epoch 1080 | Train Loss: 0.394001 | LR: 0.0500 | Patience: 16/50 | Time: 29.81s\n",
      "Epoch 1100 | Train Loss: 0.394000 | LR: 0.0500 | Patience:  4/50 | Time: 30.38s\n",
      "Epoch 1120 | Train Loss: 0.394000 | LR: 0.0500 | Patience: 24/50 | Time: 31.00s\n",
      "Epoch 1140 | Train Loss: 0.393999 | LR: 0.0500 | Patience: 11/50 | Time: 31.60s\n",
      "Epoch 1160 | Train Loss: 0.393998 | LR: 0.0500 | Patience: 31/50 | Time: 32.20s\n",
      "Epoch 1180 | Train Loss: 0.393998 | LR: 0.0500 | Patience: 17/50 | Time: 32.81s\n",
      "Epoch 1200 | Train Loss: 0.393997 | LR: 0.0500 | Patience:  2/50 | Time: 33.38s\n",
      "Epoch 1220 | Train Loss: 0.393997 | LR: 0.0500 | Patience: 22/50 | Time: 34.07s\n",
      "Epoch 1240 | Train Loss: 0.393996 | LR: 0.0500 | Patience:  6/50 | Time: 34.66s\n",
      "Epoch 1260 | Train Loss: 0.393996 | LR: 0.0500 | Patience: 26/50 | Time: 35.25s\n",
      "Epoch 1280 | Train Loss: 0.393995 | LR: 0.0500 | Patience:  9/50 | Time: 35.87s\n",
      "Epoch 1300 | Train Loss: 0.393995 | LR: 0.0500 | Patience: 29/50 | Time: 36.45s\n",
      "Epoch 1320 | Train Loss: 0.393994 | LR: 0.0500 | Patience: 10/50 | Time: 36.97s\n",
      "Epoch 1340 | Train Loss: 0.393994 | LR: 0.0500 | Patience: 30/50 | Time: 37.67s\n",
      "Epoch 1360 | Train Loss: 0.393993 | LR: 0.0500 | Patience: 10/50 | Time: 38.17s\n",
      "Epoch 1380 | Train Loss: 0.393993 | LR: 0.0500 | Patience: 30/50 | Time: 38.68s\n",
      "Epoch 1400 | Train Loss: 0.393992 | LR: 0.0500 | Patience:  8/50 | Time: 39.22s\n",
      "Epoch 1420 | Train Loss: 0.393992 | LR: 0.0500 | Patience: 28/50 | Time: 39.81s\n",
      "Epoch 1440 | Train Loss: 0.393991 | LR: 0.0500 | Patience:  5/50 | Time: 40.39s\n",
      "Epoch 1460 | Train Loss: 0.393991 | LR: 0.0500 | Patience: 25/50 | Time: 40.94s\n",
      "Epoch 1480 | Train Loss: 0.393990 | LR: 0.0500 | Patience:  0/50 | Time: 41.44s\n",
      "Epoch 1500 | Train Loss: 0.393990 | LR: 0.0500 | Patience: 20/50 | Time: 41.96s\n",
      "Epoch 1520 | Train Loss: 0.393989 | LR: 0.0500 | Patience: 40/50 | Time: 42.59s\n",
      "Epoch 1540 | Train Loss: 0.393989 | LR: 0.0500 | Patience: 13/50 | Time: 43.20s\n",
      "Epoch 1560 | Train Loss: 0.393989 | LR: 0.0500 | Patience: 33/50 | Time: 43.83s\n",
      "Epoch 1580 | Train Loss: 0.393988 | LR: 0.0500 | Patience:  4/50 | Time: 44.39s\n",
      "Epoch 1600 | Train Loss: 0.393988 | LR: 0.0500 | Patience: 24/50 | Time: 44.93s\n",
      "Epoch 1620 | Train Loss: 0.393987 | LR: 0.0500 | Patience: 44/50 | Time: 45.48s\n",
      "\n",
      "Early stopping triggered at epoch 1626\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393987\n",
      "Total training time: 45.65s\n",
      "Average time per epoch: 0.0281s\n",
      "Weighted final loss:   0.393987\n",
      "Weighted final epoch:   1626\n",
      "Starting SGD training with learning rate: 0.1\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.393987 | LR: 0.1000 | Patience: 19/50 | Time: 0.54s\n",
      "Epoch   40 | Train Loss: 0.393986 | LR: 0.1000 | Patience: 12/50 | Time: 1.10s\n",
      "Epoch   60 | Train Loss: 0.393985 | LR: 0.1000 | Patience:  4/50 | Time: 1.60s\n",
      "Epoch   80 | Train Loss: 0.393984 | LR: 0.1000 | Patience: 24/50 | Time: 2.21s\n",
      "Epoch  100 | Train Loss: 0.393984 | LR: 0.1000 | Patience: 14/50 | Time: 2.78s\n",
      "Epoch  120 | Train Loss: 0.393983 | LR: 0.1000 | Patience:  3/50 | Time: 3.41s\n",
      "Epoch  140 | Train Loss: 0.393982 | LR: 0.1000 | Patience: 23/50 | Time: 3.99s\n",
      "Epoch  160 | Train Loss: 0.393982 | LR: 0.1000 | Patience: 10/50 | Time: 4.50s\n",
      "Epoch  180 | Train Loss: 0.393981 | LR: 0.1000 | Patience: 30/50 | Time: 5.13s\n",
      "Epoch  200 | Train Loss: 0.393981 | LR: 0.1000 | Patience: 15/50 | Time: 5.65s\n",
      "Epoch  220 | Train Loss: 0.393980 | LR: 0.1000 | Patience: 35/50 | Time: 6.18s\n",
      "Epoch  240 | Train Loss: 0.393980 | LR: 0.1000 | Patience: 18/50 | Time: 6.67s\n",
      "Epoch  260 | Train Loss: 0.393979 | LR: 0.1000 | Patience: 38/50 | Time: 7.22s\n",
      "Epoch  280 | Train Loss: 0.393979 | LR: 0.1000 | Patience: 19/50 | Time: 7.96s\n",
      "Epoch  300 | Train Loss: 0.393978 | LR: 0.1000 | Patience: 39/50 | Time: 8.55s\n",
      "Epoch  320 | Train Loss: 0.393978 | LR: 0.1000 | Patience: 17/50 | Time: 9.15s\n",
      "Epoch  340 | Train Loss: 0.393977 | LR: 0.1000 | Patience: 37/50 | Time: 9.84s\n",
      "Epoch  360 | Train Loss: 0.393977 | LR: 0.1000 | Patience: 12/50 | Time: 10.37s\n",
      "Epoch  380 | Train Loss: 0.393976 | LR: 0.1000 | Patience: 32/50 | Time: 10.92s\n",
      "Epoch  400 | Train Loss: 0.393976 | LR: 0.1000 | Patience:  3/50 | Time: 11.56s\n",
      "Epoch  420 | Train Loss: 0.393976 | LR: 0.1000 | Patience: 23/50 | Time: 12.15s\n",
      "Epoch  440 | Train Loss: 0.393975 | LR: 0.1000 | Patience: 43/50 | Time: 12.75s\n",
      "\n",
      "Early stopping triggered at epoch 447\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393975\n",
      "Total training time: 12.95s\n",
      "Average time per epoch: 0.0290s\n",
      "Weighted final loss:   0.393975\n",
      "Weighted final epoch:   447\n"
     ]
    }
   ],
   "source": [
    "weighted_model = WeightedLogisticRegression()\n",
    "configs = [\n",
    "    {\"learning_rate\": 0.01},\n",
    "    {\"learning_rate\": 0.05},\n",
    "    {\"learning_rate\": 0.1},\n",
    "]\n",
    "for config in configs:\n",
    "    weighted_optimizer = SGDOptimizer(\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        early_stop_threshold=EARLY_STOP_THRESHOLD,\n",
    "        log_interval=20,\n",
    "        verbose=True,\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "    )\n",
    "    weighted_result = weighted_optimizer.optimize(\n",
    "        weighted_model, X_train_scaled, y_train, weights_train=weights_train\n",
    "    )\n",
    "\n",
    "    print(f\"Weighted final loss:   {weighted_result['final_loss']:.6f}\")\n",
    "    print(f\"Weighted final epoch:   {weighted_result['final_epoch']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3163d533",
   "metadata": {},
   "source": [
    "## SGD backtracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85d12c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SGD with Backtracking Line Search\n",
      "Initial step size: 0.1, c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch   50 | Train Loss: 0.411206, Val Loss: 0.410006 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 2.15s\n",
      "Epoch  100 | Train Loss: 0.400001, Val Loss: 0.399086 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 4.34s\n",
      "Epoch  150 | Train Loss: 0.396421, Val Loss: 0.395591 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 6.59s\n",
      "Epoch  200 | Train Loss: 0.395093, Val Loss: 0.394292 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 8.67s\n",
      "Epoch  250 | Train Loss: 0.394557, Val Loss: 0.393768 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 11.11s\n",
      "Epoch  300 | Train Loss: 0.394324, Val Loss: 0.393542 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 13.32s\n",
      "Epoch  350 | Train Loss: 0.394215, Val Loss: 0.393438 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 15.49s\n",
      "Epoch  400 | Train Loss: 0.394159, Val Loss: 0.393386 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 17.87s\n",
      "Epoch  450 | Train Loss: 0.394127, Val Loss: 0.393357 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 20.00s\n",
      "Epoch  500 | Train Loss: 0.394105, Val Loss: 0.393339 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 22.17s\n",
      "Epoch  550 | Train Loss: 0.394090, Val Loss: 0.393326 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 24.34s\n",
      "Epoch  600 | Train Loss: 0.394077, Val Loss: 0.393316 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 26.43s\n",
      "Epoch  650 | Train Loss: 0.394066, Val Loss: 0.393308 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 28.42s\n",
      "Epoch  700 | Train Loss: 0.394057, Val Loss: 0.393301 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 30.47s\n",
      "Epoch  750 | Train Loss: 0.394049, Val Loss: 0.393294 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 32.46s\n",
      "Epoch  800 | Train Loss: 0.394041, Val Loss: 0.393288 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 34.57s\n",
      "Epoch  850 | Train Loss: 0.394035, Val Loss: 0.393283 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 36.61s\n",
      "Epoch  900 | Train Loss: 0.394029, Val Loss: 0.393279 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 38.76s\n",
      "Epoch  950 | Train Loss: 0.394023, Val Loss: 0.393274 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 40.74s\n",
      "Epoch 1000 | Train Loss: 0.394018, Val Loss: 0.393270 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 42.82s\n",
      "Epoch 1050 | Train Loss: 0.394014, Val Loss: 0.393266 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 44.98s\n",
      "Epoch 1100 | Train Loss: 0.394010, Val Loss: 0.393263 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 47.08s\n",
      "Epoch 1150 | Train Loss: 0.394006, Val Loss: 0.393260 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 49.13s\n",
      "Epoch 1200 | Train Loss: 0.394002, Val Loss: 0.393257 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 51.25s\n",
      "Epoch 1250 | Train Loss: 0.393999, Val Loss: 0.393254 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 53.44s\n",
      "Epoch 1300 | Train Loss: 0.393997, Val Loss: 0.393252 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 55.52s\n",
      "Epoch 1350 | Train Loss: 0.393994, Val Loss: 0.393250 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 14/50 | Time: 57.62s\n",
      "Epoch 1400 | Train Loss: 0.393992, Val Loss: 0.393247 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 59.71s\n",
      "Epoch 1450 | Train Loss: 0.393989, Val Loss: 0.393245 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 61.76s\n",
      "Epoch 1500 | Train Loss: 0.393987, Val Loss: 0.393244 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 63.77s\n",
      "Epoch 1550 | Train Loss: 0.393985, Val Loss: 0.393242 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 26/50 | Time: 65.86s\n",
      "Epoch 1600 | Train Loss: 0.393984, Val Loss: 0.393240 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 18/50 | Time: 67.88s\n",
      "Epoch 1650 | Train Loss: 0.393982, Val Loss: 0.393239 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 70.35s\n",
      "Epoch 1700 | Train Loss: 0.393981, Val Loss: 0.393237 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 19/50 | Time: 72.75s\n",
      "Epoch 1750 | Train Loss: 0.393979, Val Loss: 0.393236 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 32/50 | Time: 74.94s\n",
      "Epoch 1800 | Train Loss: 0.393978, Val Loss: 0.393234 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 76.92s\n",
      "Epoch 1850 | Train Loss: 0.393977, Val Loss: 0.393233 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 79.00s\n",
      "Epoch 1900 | Train Loss: 0.393976, Val Loss: 0.393232 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 81.38s\n",
      "\n",
      "Early stopping triggered at epoch 1942\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393975\n",
      "Total training time: 83.31s\n",
      "Average time per epoch: 0.0429s\n",
      "Average step size: 0.100000\n",
      "Final step size: 0.100000\n",
      "Average backtrack iterations: 0.00\n",
      "Starting SGD with Backtracking Line Search\n",
      "Initial step size: 1.0, c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch   50 | Train Loss: 0.393969, Val Loss: 0.393223 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 2.34s\n",
      "Epoch  100 | Train Loss: 0.393966, Val Loss: 0.393218 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 17/50 | Time: 4.39s\n",
      "Epoch  150 | Train Loss: 0.393964, Val Loss: 0.393215 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 6.45s\n",
      "\n",
      "Early stopping triggered at epoch 191\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393964\n",
      "Total training time: 8.24s\n",
      "Average time per epoch: 0.0431s\n",
      "Average step size: 1.000000\n",
      "Final step size: 1.000000\n",
      "Average backtrack iterations: 0.00\n",
      "Starting SGD with Backtracking Line Search\n",
      "Initial step size: 5.0, c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch   50 | Train Loss: 0.393963, Val Loss: 0.393209 | Step Size: 5.000000 | Avg Step: 5.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 49/50 | Time: 2.15s\n",
      "\n",
      "Early stopping triggered at epoch 51\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393963\n",
      "Total training time: 2.19s\n",
      "Average time per epoch: 0.0429s\n",
      "Average step size: 5.000000\n",
      "Final step size: 5.000000\n",
      "Average backtrack iterations: 0.00\n"
     ]
    }
   ],
   "source": [
    "sgd_bt_model = WeightedLogisticRegression()\n",
    "configs = [\n",
    "    {\"init_learning_rate\": 0.1},\n",
    "    {\"init_learning_rate\": 1.0},\n",
    "    {\"init_learning_rate\": 5.0},\n",
    "]\n",
    "for config in configs:\n",
    "    sgd_bt_opt = SGDBacktrackingOptimizer(\n",
    "        initial_learning_rate=config[\"init_learning_rate\"],\n",
    "        log_interval=50,\n",
    "        verbose=True,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        early_stop_threshold=EARLY_STOP_THRESHOLD,\n",
    "    )\n",
    "    weighted_result = sgd_bt_opt.optimize(\n",
    "        sgd_bt_model,\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        X_val_scaled,\n",
    "        y_val,\n",
    "        weights_train,\n",
    "        weights_val,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044f518",
   "metadata": {},
   "source": [
    "## AGD fixed learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c5d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "agd_weighted_model = WeightedLogisticRegression()\n",
    "configs = [\n",
    "    {\"learning_rate\": 0.001, \"momentum\": 0.9},\n",
    "    {\"learning_rate\": 0.005, \"momentum\": 0.9},\n",
    "    {\"learning_rate\": 0.01, \"momentum\": 0.9},\n",
    "    {\"learning_rate\": 0.1, \"momentum\": 0.9},\n",
    "]\n",
    "for config in configs:\n",
    "    agd_weighted_optimizer = AGDOptimizer(\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        early_stop_threshold=EARLY_STOP_THRESHOLD,\n",
    "        log_interval=20,\n",
    "        verbose=True,\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        momentum=config[\"momentum\"],\n",
    "    )\n",
    "    agd_weighted_result = agd_weighted_optimizer.optimize(\n",
    "        agd_weighted_model,\n",
    "        X_train=X_train_scaled,\n",
    "        y_train=y_train,\n",
    "        weights_train=weights_train,\n",
    "    )\n",
    "    print(f\"AGD Weighted final loss:   {agd_weighted_result['final_loss']:.6f}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06963d5b",
   "metadata": {},
   "source": [
    "## AGD backtracking line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e473df4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AGD with Backtracking Line Search\n",
      "Initial step size: 0.1, momentum: 0.9\n",
      "c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.465012 | Step Size: 0.000000 | Avg Step: 0.040000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 12.0 | Patience: 11/50 | Time: 4.78s\n",
      "Epoch   40 | Train Loss: 0.400968 | Step Size: 0.000000 | Avg Step: 0.075000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 5.0 | Patience:  3/50 | Time: 7.30s\n",
      "Epoch   60 | Train Loss: 0.395129 | Step Size: 0.000000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 10.0 | Patience:  2/50 | Time: 11.26s\n",
      "Epoch   80 | Train Loss: 0.395985 | Step Size: 0.100000 | Avg Step: 0.045000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 11.0 | Patience: 22/50 | Time: 15.57s\n",
      "Epoch  100 | Train Loss: 0.396662 | Step Size: 0.100000 | Avg Step: 0.040000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 12.0 | Patience: 16/50 | Time: 19.86s\n",
      "Epoch  120 | Train Loss: 0.395924 | Step Size: 0.100000 | Avg Step: 0.035000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 13.0 | Patience: 13/50 | Time: 24.40s\n",
      "Epoch  140 | Train Loss: 0.394595 | Step Size: 0.000000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 10.0 | Patience:  9/50 | Time: 28.01s\n",
      "Epoch  160 | Train Loss: 0.394146 | Step Size: 0.000000 | Avg Step: 0.055000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 9.0 | Patience:  5/50 | Time: 31.31s\n",
      "Epoch  180 | Train Loss: 0.394015 | Step Size: 0.000000 | Avg Step: 0.055000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 9.0 | Patience:  1/50 | Time: 34.72s\n",
      "Epoch  200 | Train Loss: 0.394021 | Step Size: 0.100000 | Avg Step: 0.045000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 11.0 | Patience: 21/50 | Time: 38.58s\n",
      "Epoch  220 | Train Loss: 0.394034 | Step Size: 0.100000 | Avg Step: 0.040000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 12.0 | Patience: 17/50 | Time: 42.71s\n",
      "Epoch  240 | Train Loss: 0.394018 | Step Size: 0.100000 | Avg Step: 0.045000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 11.0 | Patience: 14/50 | Time: 46.55s\n",
      "Epoch  260 | Train Loss: 0.393996 | Step Size: 0.000000 | Avg Step: 0.045000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 11.0 | Patience: 12/50 | Time: 50.63s\n",
      "Epoch  280 | Train Loss: 0.393982 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 51.47s\n",
      "Epoch  300 | Train Loss: 0.393976 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 52.29s\n",
      "Epoch  320 | Train Loss: 0.393972 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 53.16s\n",
      "Epoch  340 | Train Loss: 0.393970 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 54.10s\n",
      "Epoch  360 | Train Loss: 0.393968 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 54.94s\n",
      "Epoch  380 | Train Loss: 0.393967 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 16/50 | Time: 55.73s\n",
      "Epoch  400 | Train Loss: 0.393966 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 18/50 | Time: 56.51s\n",
      "Epoch  420 | Train Loss: 0.393965 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 12/50 | Time: 57.32s\n",
      "Epoch  440 | Train Loss: 0.393965 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 32/50 | Time: 58.26s\n",
      "Epoch  460 | Train Loss: 0.393965 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  7/50 | Time: 59.08s\n",
      "Epoch  480 | Train Loss: 0.393964 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 27/50 | Time: 59.92s\n",
      "Epoch  500 | Train Loss: 0.393964 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 47/50 | Time: 60.74s\n",
      "\n",
      "Early stopping triggered at epoch 503\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393964\n",
      "Total training time: 60.86s\n",
      "Average time per epoch: 0.1210s\n",
      "Average step size: 0.072962\n",
      "Final step size: 0.100000\n",
      "Average backtrack iterations: 5.41\n",
      "Starting AGD with Backtracking Line Search\n",
      "Initial step size: 1.0, momentum: 0.9\n",
      "c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.393964 | Step Size: 1.000000 | Avg Step: 1.000000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 19/50 | Time: 0.80s\n",
      "Epoch   40 | Train Loss: 0.393964 | Step Size: 1.000000 | Avg Step: 1.000000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 39/50 | Time: 1.66s\n",
      "\n",
      "Early stopping triggered at epoch 51\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393964\n",
      "Total training time: 2.10s\n",
      "Average time per epoch: 0.0412s\n",
      "Average step size: 1.000000\n",
      "Final step size: 1.000000\n",
      "Average backtrack iterations: 0.00\n",
      "Starting AGD with Backtracking Line Search\n",
      "Initial step size: 5.0, momentum: 0.9\n",
      "c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.393963 | Step Size: 5.000000 | Avg Step: 5.000000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 19/50 | Time: 0.80s\n",
      "Epoch   40 | Train Loss: 0.393963 | Step Size: 2.500000 | Avg Step: 4.500000 | Momentum: 0.900 | Backtrack:  1 | Avg BT: 0.2 | Patience: 39/50 | Time: 1.64s\n",
      "\n",
      "Early stopping triggered at epoch 51\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393963\n",
      "Total training time: 2.11s\n",
      "Average time per epoch: 0.0414s\n",
      "Average step size: 4.705882\n",
      "Final step size: 5.000000\n",
      "Average backtrack iterations: 0.12\n"
     ]
    }
   ],
   "source": [
    "agd_bt_weighted_model = WeightedLogisticRegression()\n",
    "configs = [\n",
    "    {\"init_learning_rate\": 0.1, \"momentum\": 0.9},\n",
    "    {\"init_learning_rate\": 1.0, \"momentum\": 0.9},\n",
    "    {\"init_learning_rate\": 5.0, \"momentum\": 0.9},\n",
    "]\n",
    "for config in configs:\n",
    "    agd_bt_weighted_optimizer = AGDBacktrackingOptimizer(\n",
    "        initial_learning_rate=config[\"init_learning_rate\"],\n",
    "        momentum=config[\"momentum\"],\n",
    "        log_interval=20,\n",
    "        verbose=True,\n",
    "    )\n",
    "    agd_bt_weighted_result = agd_bt_weighted_optimizer.optimize(\n",
    "        agd_bt_weighted_model,\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        weights_train=weights_train,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54467f22",
   "metadata": {},
   "source": [
    "## SGD iterations (mini-batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbcdad72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing SGD Iterations with LR: 0.01, Batch Size: 32\n",
      "================================================================================\n",
      "Starting SGD Iterations training\n",
      "Learning rate: 0.01, Batch size: 32\n",
      "Max iterations: 50000\n",
      "Early stopping: patience=200, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "Initial full dataset loss: 0.689627\n",
      "----------------------------------------------------------------------\n",
      "Iter    20 | Train Loss: 0.594208, Val Loss: 0.594168 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 86.4 iter/s | Time: 0.23s\n",
      "Iter    40 | Train Loss: 0.538080, Val Loss: 0.537966 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 91.6 iter/s | Time: 0.44s\n",
      "Iter    60 | Train Loss: 0.506395, Val Loss: 0.506175 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 83.9 iter/s | Time: 0.72s\n",
      "Iter    80 | Train Loss: 0.485677, Val Loss: 0.485346 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 82.3 iter/s | Time: 0.97s\n",
      "Iter   100 | Train Loss: 0.471340, Val Loss: 0.470923 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 86.0 iter/s | Time: 1.16s\n",
      "Iter   120 | Train Loss: 0.459665, Val Loss: 0.459187 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 88.5 iter/s | Time: 1.36s\n",
      "Iter   140 | Train Loss: 0.450472, Val Loss: 0.449933 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 90.3 iter/s | Time: 1.55s\n",
      "Iter   160 | Train Loss: 0.443811, Val Loss: 0.443246 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 91.2 iter/s | Time: 1.75s\n",
      "Iter   180 | Train Loss: 0.439327, Val Loss: 0.438716 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.0 iter/s | Time: 1.96s\n",
      "Iter   200 | Train Loss: 0.435482, Val Loss: 0.434858 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.8 iter/s | Time: 2.16s\n",
      "Iter   220 | Train Loss: 0.431164, Val Loss: 0.430499 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.5 iter/s | Time: 2.35s\n",
      "Iter   240 | Train Loss: 0.428327, Val Loss: 0.427654 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.6 iter/s | Time: 2.56s\n",
      "Iter   260 | Train Loss: 0.426867, Val Loss: 0.426204 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.7 iter/s | Time: 2.77s\n",
      "Iter   280 | Train Loss: 0.424385, Val Loss: 0.423694 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.1 iter/s | Time: 3.04s\n",
      "Iter   300 | Train Loss: 0.423161, Val Loss: 0.422485 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.7 iter/s | Time: 3.24s\n",
      "Iter   320 | Train Loss: 0.421546, Val Loss: 0.420855 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.1 iter/s | Time: 3.44s\n",
      "Iter   340 | Train Loss: 0.419731, Val Loss: 0.419037 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.4 iter/s | Time: 3.64s\n",
      "Iter   360 | Train Loss: 0.417880, Val Loss: 0.417170 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.8 iter/s | Time: 3.84s\n",
      "Iter   380 | Train Loss: 0.416239, Val Loss: 0.415524 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 94.0 iter/s | Time: 4.04s\n",
      "Iter   400 | Train Loss: 0.415254, Val Loss: 0.414569 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 94.4 iter/s | Time: 4.24s\n",
      "Iter   420 | Train Loss: 0.413966, Val Loss: 0.413279 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 94.6 iter/s | Time: 4.44s\n",
      "Iter   440 | Train Loss: 0.412790, Val Loss: 0.412071 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 94.9 iter/s | Time: 4.64s\n",
      "Iter   460 | Train Loss: 0.411830, Val Loss: 0.411087 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 95.1 iter/s | Time: 4.84s\n",
      "Iter   480 | Train Loss: 0.411057, Val Loss: 0.410322 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 95.2 iter/s | Time: 5.04s\n",
      "Iter   500 | Train Loss: 0.410273, Val Loss: 0.409528 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 94.0 iter/s | Time: 5.32s\n",
      "Iter   520 | Train Loss: 0.409427, Val Loss: 0.408685 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 94.1 iter/s | Time: 5.53s\n",
      "Iter   540 | Train Loss: 0.408862, Val Loss: 0.408123 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.0 iter/s | Time: 5.81s\n",
      "Iter   560 | Train Loss: 0.408354, Val Loss: 0.407607 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.2 iter/s | Time: 6.08s\n",
      "Iter   580 | Train Loss: 0.407687, Val Loss: 0.406951 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.3 iter/s | Time: 6.28s\n",
      "Iter   600 | Train Loss: 0.406918, Val Loss: 0.406182 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.0 iter/s | Time: 6.52s\n",
      "Iter   620 | Train Loss: 0.406193, Val Loss: 0.405445 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 91.9 iter/s | Time: 6.74s\n",
      "Iter   640 | Train Loss: 0.405538, Val Loss: 0.404775 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.0 iter/s | Time: 6.96s\n",
      "Iter   660 | Train Loss: 0.404985, Val Loss: 0.404211 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.1 iter/s | Time: 7.17s\n",
      "Iter   680 | Train Loss: 0.404442, Val Loss: 0.403647 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.2 iter/s | Time: 7.38s\n",
      "Iter   700 | Train Loss: 0.404001, Val Loss: 0.403186 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.3 iter/s | Time: 7.59s\n",
      "Iter   720 | Train Loss: 0.403613, Val Loss: 0.402789 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 92.3 iter/s | Time: 7.80s\n",
      "Iter   740 | Train Loss: 0.403238, Val Loss: 0.402397 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.4 iter/s | Time: 8.01s\n",
      "Iter   760 | Train Loss: 0.402605, Val Loss: 0.401785 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 91.9 iter/s | Time: 8.27s\n",
      "Iter   780 | Train Loss: 0.402408, Val Loss: 0.401601 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.1 iter/s | Time: 8.47s\n",
      "Iter   800 | Train Loss: 0.401854, Val Loss: 0.401050 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 91.9 iter/s | Time: 8.70s\n",
      "Iter   820 | Train Loss: 0.401535, Val Loss: 0.400723 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 91.9 iter/s | Time: 8.93s\n",
      "Iter   840 | Train Loss: 0.401255, Val Loss: 0.400435 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 91.6 iter/s | Time: 9.17s\n",
      "Iter   860 | Train Loss: 0.400985, Val Loss: 0.400178 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 91.5 iter/s | Time: 9.40s\n",
      "Iter   880 | Train Loss: 0.400744, Val Loss: 0.399935 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 91.7 iter/s | Time: 9.59s\n",
      "Iter   900 | Train Loss: 0.400525, Val Loss: 0.399717 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 91.9 iter/s | Time: 9.79s\n",
      "Iter   920 | Train Loss: 0.400297, Val Loss: 0.399500 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.0 iter/s | Time: 9.99s\n",
      "Iter   940 | Train Loss: 0.400134, Val Loss: 0.399336 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.2 iter/s | Time: 10.20s\n",
      "Iter   960 | Train Loss: 0.399759, Val Loss: 0.398951 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.4 iter/s | Time: 10.39s\n",
      "Iter   980 | Train Loss: 0.399602, Val Loss: 0.398775 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.5 iter/s | Time: 10.59s\n",
      "Iter  1000 | Train Loss: 0.399362, Val Loss: 0.398545 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.6 iter/s | Time: 10.80s\n",
      "Iter  1020 | Train Loss: 0.399264, Val Loss: 0.398450 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.8 iter/s | Time: 10.99s\n",
      "Iter  1040 | Train Loss: 0.398988, Val Loss: 0.398169 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.9 iter/s | Time: 11.20s\n",
      "Iter  1060 | Train Loss: 0.398724, Val Loss: 0.397917 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.0 iter/s | Time: 11.39s\n",
      "Iter  1080 | Train Loss: 0.398550, Val Loss: 0.397737 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.2 iter/s | Time: 11.59s\n",
      "Iter  1100 | Train Loss: 0.398258, Val Loss: 0.397434 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.3 iter/s | Time: 11.80s\n",
      "Iter  1120 | Train Loss: 0.398209, Val Loss: 0.397370 | LR: 0.0100 | Batch:  32 | Patience:   4/200 | Rate: 93.4 iter/s | Time: 12.00s\n",
      "Iter  1140 | Train Loss: 0.398038, Val Loss: 0.397192 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.5 iter/s | Time: 12.20s\n",
      "Iter  1160 | Train Loss: 0.397776, Val Loss: 0.396936 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.6 iter/s | Time: 12.39s\n",
      "Iter  1180 | Train Loss: 0.397587, Val Loss: 0.396734 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.7 iter/s | Time: 12.60s\n",
      "Iter  1200 | Train Loss: 0.397494, Val Loss: 0.396634 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.8 iter/s | Time: 12.79s\n",
      "Iter  1220 | Train Loss: 0.397228, Val Loss: 0.396382 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.9 iter/s | Time: 12.99s\n",
      "Iter  1240 | Train Loss: 0.397162, Val Loss: 0.396317 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 94.0 iter/s | Time: 13.20s\n",
      "Iter  1260 | Train Loss: 0.397048, Val Loss: 0.396226 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 94.1 iter/s | Time: 13.39s\n",
      "Iter  1280 | Train Loss: 0.396964, Val Loss: 0.396120 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 94.2 iter/s | Time: 13.59s\n",
      "Iter  1300 | Train Loss: 0.396855, Val Loss: 0.395992 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 94.2 iter/s | Time: 13.81s\n",
      "Iter  1320 | Train Loss: 0.396752, Val Loss: 0.395910 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 94.1 iter/s | Time: 14.02s\n",
      "Iter  1340 | Train Loss: 0.396605, Val Loss: 0.395767 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 94.0 iter/s | Time: 14.25s\n",
      "Iter  1360 | Train Loss: 0.396576, Val Loss: 0.395716 | LR: 0.0100 | Batch:  32 | Patience:   3/200 | Rate: 94.0 iter/s | Time: 14.47s\n",
      "Iter  1380 | Train Loss: 0.396429, Val Loss: 0.395558 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.8 iter/s | Time: 14.71s\n",
      "Iter  1400 | Train Loss: 0.396393, Val Loss: 0.395520 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.7 iter/s | Time: 14.94s\n",
      "Iter  1420 | Train Loss: 0.396277, Val Loss: 0.395424 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 93.6 iter/s | Time: 15.17s\n",
      "Iter  1440 | Train Loss: 0.396256, Val Loss: 0.395379 | LR: 0.0100 | Batch:  32 | Patience:   2/200 | Rate: 93.5 iter/s | Time: 15.41s\n",
      "Iter  1460 | Train Loss: 0.396157, Val Loss: 0.395274 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 93.3 iter/s | Time: 15.64s\n",
      "Iter  1480 | Train Loss: 0.396058, Val Loss: 0.395205 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.2 iter/s | Time: 15.88s\n",
      "Iter  1500 | Train Loss: 0.395967, Val Loss: 0.395127 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.1 iter/s | Time: 16.11s\n",
      "Iter  1520 | Train Loss: 0.395993, Val Loss: 0.395171 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 93.0 iter/s | Time: 16.35s\n",
      "Iter  1540 | Train Loss: 0.395871, Val Loss: 0.395040 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.9 iter/s | Time: 16.58s\n",
      "Iter  1560 | Train Loss: 0.395788, Val Loss: 0.394971 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.8 iter/s | Time: 16.82s\n",
      "Iter  1580 | Train Loss: 0.395677, Val Loss: 0.394840 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.7 iter/s | Time: 17.04s\n",
      "Iter  1600 | Train Loss: 0.395717, Val Loss: 0.394861 | LR: 0.0100 | Batch:  32 | Patience:   2/200 | Rate: 92.8 iter/s | Time: 17.24s\n",
      "Iter  1620 | Train Loss: 0.395747, Val Loss: 0.394886 | LR: 0.0100 | Batch:  32 | Patience:   6/200 | Rate: 92.7 iter/s | Time: 17.47s\n",
      "Iter  1640 | Train Loss: 0.395626, Val Loss: 0.394792 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.6 iter/s | Time: 17.70s\n",
      "Iter  1660 | Train Loss: 0.395534, Val Loss: 0.394705 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.6 iter/s | Time: 17.93s\n",
      "Iter  1680 | Train Loss: 0.395515, Val Loss: 0.394676 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 92.5 iter/s | Time: 18.16s\n",
      "Iter  1700 | Train Loss: 0.395459, Val Loss: 0.394610 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.6 iter/s | Time: 18.35s\n",
      "Iter  1720 | Train Loss: 0.395368, Val Loss: 0.394526 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.6 iter/s | Time: 18.56s\n",
      "Iter  1740 | Train Loss: 0.395335, Val Loss: 0.394499 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.7 iter/s | Time: 18.76s\n",
      "Iter  1760 | Train Loss: 0.395264, Val Loss: 0.394428 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.8 iter/s | Time: 18.97s\n",
      "Iter  1780 | Train Loss: 0.395282, Val Loss: 0.394475 | LR: 0.0100 | Batch:  32 | Patience:   3/200 | Rate: 92.8 iter/s | Time: 19.17s\n",
      "Iter  1800 | Train Loss: 0.395243, Val Loss: 0.394407 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 92.8 iter/s | Time: 19.40s\n",
      "Iter  1820 | Train Loss: 0.395223, Val Loss: 0.394345 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.9 iter/s | Time: 19.60s\n",
      "Iter  1840 | Train Loss: 0.395168, Val Loss: 0.394312 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.9 iter/s | Time: 19.80s\n",
      "Iter  1860 | Train Loss: 0.395119, Val Loss: 0.394257 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.0 iter/s | Time: 20.00s\n",
      "Iter  1880 | Train Loss: 0.395078, Val Loss: 0.394217 | LR: 0.0100 | Batch:  32 | Patience:   4/200 | Rate: 93.1 iter/s | Time: 20.20s\n",
      "Iter  1900 | Train Loss: 0.395046, Val Loss: 0.394166 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.2 iter/s | Time: 20.39s\n",
      "Iter  1920 | Train Loss: 0.394986, Val Loss: 0.394110 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.2 iter/s | Time: 20.59s\n",
      "Iter  1940 | Train Loss: 0.394932, Val Loss: 0.394072 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.3 iter/s | Time: 20.79s\n",
      "Iter  1960 | Train Loss: 0.394895, Val Loss: 0.394040 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.3 iter/s | Time: 21.00s\n",
      "Iter  1980 | Train Loss: 0.394942, Val Loss: 0.394077 | LR: 0.0100 | Batch:  32 | Patience:   3/200 | Rate: 93.4 iter/s | Time: 21.20s\n",
      "Iter  2000 | Train Loss: 0.394881, Val Loss: 0.394029 | LR: 0.0100 | Batch:  32 | Patience:   7/200 | Rate: 93.4 iter/s | Time: 21.40s\n",
      "Iter  2020 | Train Loss: 0.394872, Val Loss: 0.394046 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 93.4 iter/s | Time: 21.62s\n",
      "Iter  2040 | Train Loss: 0.394847, Val Loss: 0.394024 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 93.5 iter/s | Time: 21.82s\n",
      "Iter  2060 | Train Loss: 0.394826, Val Loss: 0.394014 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 93.4 iter/s | Time: 22.05s\n",
      "Iter  2080 | Train Loss: 0.394810, Val Loss: 0.393999 | LR: 0.0100 | Batch:  32 | Patience:   3/200 | Rate: 93.4 iter/s | Time: 22.27s\n",
      "Iter  2100 | Train Loss: 0.394800, Val Loss: 0.393990 | LR: 0.0100 | Batch:  32 | Patience:   2/200 | Rate: 93.4 iter/s | Time: 22.48s\n",
      "Iter  2120 | Train Loss: 0.394733, Val Loss: 0.393918 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.3 iter/s | Time: 22.71s\n",
      "Iter  2140 | Train Loss: 0.394735, Val Loss: 0.393939 | LR: 0.0100 | Batch:  32 | Patience:   3/200 | Rate: 93.4 iter/s | Time: 22.92s\n",
      "Iter  2160 | Train Loss: 0.394729, Val Loss: 0.393922 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.4 iter/s | Time: 23.12s\n",
      "Iter  2180 | Train Loss: 0.394730, Val Loss: 0.393942 | LR: 0.0100 | Batch:  32 | Patience:   4/200 | Rate: 93.0 iter/s | Time: 23.43s\n",
      "Iter  2200 | Train Loss: 0.394688, Val Loss: 0.393868 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.8 iter/s | Time: 23.71s\n",
      "Iter  2220 | Train Loss: 0.394716, Val Loss: 0.393885 | LR: 0.0100 | Batch:  32 | Patience:   2/200 | Rate: 92.7 iter/s | Time: 23.94s\n",
      "Iter  2240 | Train Loss: 0.394684, Val Loss: 0.393854 | LR: 0.0100 | Batch:  32 | Patience:   2/200 | Rate: 92.6 iter/s | Time: 24.18s\n",
      "Iter  2260 | Train Loss: 0.394697, Val Loss: 0.393847 | LR: 0.0100 | Batch:  32 | Patience:   2/200 | Rate: 92.5 iter/s | Time: 24.44s\n",
      "Iter  2280 | Train Loss: 0.394736, Val Loss: 0.393887 | LR: 0.0100 | Batch:  32 | Patience:   6/200 | Rate: 92.3 iter/s | Time: 24.69s\n",
      "Iter  2300 | Train Loss: 0.394672, Val Loss: 0.393843 | LR: 0.0100 | Batch:  32 | Patience:  10/200 | Rate: 92.4 iter/s | Time: 24.89s\n",
      "Iter  2320 | Train Loss: 0.394658, Val Loss: 0.393821 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 92.4 iter/s | Time: 25.11s\n",
      "Iter  2340 | Train Loss: 0.394645, Val Loss: 0.393808 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.3 iter/s | Time: 25.34s\n",
      "Iter  2360 | Train Loss: 0.394664, Val Loss: 0.393850 | LR: 0.0100 | Batch:  32 | Patience:   4/200 | Rate: 92.4 iter/s | Time: 25.54s\n",
      "Iter  2380 | Train Loss: 0.394696, Val Loss: 0.393900 | LR: 0.0100 | Batch:  32 | Patience:   8/200 | Rate: 92.4 iter/s | Time: 25.76s\n",
      "Iter  2400 | Train Loss: 0.394679, Val Loss: 0.393871 | LR: 0.0100 | Batch:  32 | Patience:  12/200 | Rate: 92.4 iter/s | Time: 25.96s\n",
      "Iter  2420 | Train Loss: 0.394668, Val Loss: 0.393840 | LR: 0.0100 | Batch:  32 | Patience:  16/200 | Rate: 92.5 iter/s | Time: 26.17s\n",
      "Iter  2440 | Train Loss: 0.394666, Val Loss: 0.393833 | LR: 0.0100 | Batch:  32 | Patience:  20/200 | Rate: 92.5 iter/s | Time: 26.39s\n",
      "Iter  2460 | Train Loss: 0.394668, Val Loss: 0.393827 | LR: 0.0100 | Batch:  32 | Patience:  24/200 | Rate: 92.5 iter/s | Time: 26.60s\n",
      "Iter  2480 | Train Loss: 0.394658, Val Loss: 0.393825 | LR: 0.0100 | Batch:  32 | Patience:  28/200 | Rate: 92.5 iter/s | Time: 26.81s\n",
      "Iter  2500 | Train Loss: 0.394644, Val Loss: 0.393814 | LR: 0.0100 | Batch:  32 | Patience:   2/200 | Rate: 92.5 iter/s | Time: 27.03s\n",
      "Iter  2520 | Train Loss: 0.394619, Val Loss: 0.393801 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 92.5 iter/s | Time: 27.24s\n",
      "Iter  2540 | Train Loss: 0.394613, Val Loss: 0.393801 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 92.5 iter/s | Time: 27.46s\n",
      "Iter  2560 | Train Loss: 0.394595, Val Loss: 0.393792 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 92.5 iter/s | Time: 27.67s\n",
      "Iter  2580 | Train Loss: 0.394562, Val Loss: 0.393764 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.6 iter/s | Time: 27.87s\n",
      "Iter  2600 | Train Loss: 0.394559, Val Loss: 0.393747 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.4 iter/s | Time: 28.14s\n",
      "Iter  2620 | Train Loss: 0.394575, Val Loss: 0.393757 | LR: 0.0100 | Batch:  32 | Patience:   4/200 | Rate: 92.4 iter/s | Time: 28.34s\n",
      "Iter  2640 | Train Loss: 0.394606, Val Loss: 0.393778 | LR: 0.0100 | Batch:  32 | Patience:   8/200 | Rate: 92.5 iter/s | Time: 28.55s\n",
      "Iter  2660 | Train Loss: 0.394595, Val Loss: 0.393754 | LR: 0.0100 | Batch:  32 | Patience:  12/200 | Rate: 92.5 iter/s | Time: 28.75s\n",
      "Iter  2680 | Train Loss: 0.394610, Val Loss: 0.393747 | LR: 0.0100 | Batch:  32 | Patience:  16/200 | Rate: 92.6 iter/s | Time: 28.95s\n",
      "Iter  2700 | Train Loss: 0.394576, Val Loss: 0.393727 | LR: 0.0100 | Batch:  32 | Patience:  20/200 | Rate: 92.6 iter/s | Time: 29.17s\n",
      "Iter  2720 | Train Loss: 0.394553, Val Loss: 0.393698 | LR: 0.0100 | Batch:  32 | Patience:  24/200 | Rate: 92.6 iter/s | Time: 29.37s\n",
      "Iter  2740 | Train Loss: 0.394545, Val Loss: 0.393708 | LR: 0.0100 | Batch:  32 | Patience:   2/200 | Rate: 92.7 iter/s | Time: 29.57s\n",
      "Iter  2760 | Train Loss: 0.394503, Val Loss: 0.393660 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.7 iter/s | Time: 29.77s\n",
      "Iter  2780 | Train Loss: 0.394487, Val Loss: 0.393654 | LR: 0.0100 | Batch:  32 | Patience:   4/200 | Rate: 92.8 iter/s | Time: 29.96s\n",
      "Iter  2800 | Train Loss: 0.394464, Val Loss: 0.393631 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.8 iter/s | Time: 30.16s\n",
      "Iter  2820 | Train Loss: 0.394449, Val Loss: 0.393632 | LR: 0.0100 | Batch:  32 | Patience:   4/200 | Rate: 92.8 iter/s | Time: 30.38s\n",
      "Iter  2840 | Train Loss: 0.394426, Val Loss: 0.393619 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.7 iter/s | Time: 30.65s\n",
      "Iter  2860 | Train Loss: 0.394441, Val Loss: 0.393643 | LR: 0.0100 | Batch:  32 | Patience:   4/200 | Rate: 92.6 iter/s | Time: 30.88s\n",
      "Iter  2880 | Train Loss: 0.394409, Val Loss: 0.393605 | LR: 0.0100 | Batch:  32 | Patience:   8/200 | Rate: 92.7 iter/s | Time: 31.08s\n",
      "Iter  2900 | Train Loss: 0.394385, Val Loss: 0.393547 | LR: 0.0100 | Batch:  32 | Patience:   2/200 | Rate: 92.7 iter/s | Time: 31.29s\n",
      "Iter  2920 | Train Loss: 0.394374, Val Loss: 0.393561 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.8 iter/s | Time: 31.48s\n",
      "Iter  2940 | Train Loss: 0.394354, Val Loss: 0.393552 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 92.8 iter/s | Time: 31.68s\n",
      "Iter  2960 | Train Loss: 0.394383, Val Loss: 0.393581 | LR: 0.0100 | Batch:  32 | Patience:   4/200 | Rate: 92.9 iter/s | Time: 31.88s\n",
      "Iter  2980 | Train Loss: 0.394384, Val Loss: 0.393611 | LR: 0.0100 | Batch:  32 | Patience:   8/200 | Rate: 92.8 iter/s | Time: 32.10s\n",
      "Iter  3000 | Train Loss: 0.394422, Val Loss: 0.393650 | LR: 0.0100 | Batch:  32 | Patience:  12/200 | Rate: 92.9 iter/s | Time: 32.30s\n",
      "Iter  3020 | Train Loss: 0.394428, Val Loss: 0.393657 | LR: 0.0100 | Batch:  32 | Patience:  16/200 | Rate: 92.9 iter/s | Time: 32.50s\n",
      "Iter  3040 | Train Loss: 0.394435, Val Loss: 0.393661 | LR: 0.0100 | Batch:  32 | Patience:  20/200 | Rate: 92.9 iter/s | Time: 32.71s\n",
      "Iter  3060 | Train Loss: 0.394491, Val Loss: 0.393732 | LR: 0.0100 | Batch:  32 | Patience:  24/200 | Rate: 93.0 iter/s | Time: 32.90s\n",
      "Iter  3080 | Train Loss: 0.394432, Val Loss: 0.393668 | LR: 0.0100 | Batch:  32 | Patience:  28/200 | Rate: 93.0 iter/s | Time: 33.11s\n",
      "Iter  3100 | Train Loss: 0.394433, Val Loss: 0.393683 | LR: 0.0100 | Batch:  32 | Patience:  32/200 | Rate: 93.1 iter/s | Time: 33.31s\n",
      "Iter  3120 | Train Loss: 0.394451, Val Loss: 0.393700 | LR: 0.0100 | Batch:  32 | Patience:  36/200 | Rate: 93.1 iter/s | Time: 33.51s\n",
      "Iter  3140 | Train Loss: 0.394389, Val Loss: 0.393624 | LR: 0.0100 | Batch:  32 | Patience:  40/200 | Rate: 93.1 iter/s | Time: 33.71s\n",
      "Iter  3160 | Train Loss: 0.394437, Val Loss: 0.393698 | LR: 0.0100 | Batch:  32 | Patience:  44/200 | Rate: 93.2 iter/s | Time: 33.91s\n",
      "Iter  3180 | Train Loss: 0.394380, Val Loss: 0.393626 | LR: 0.0100 | Batch:  32 | Patience:  48/200 | Rate: 93.2 iter/s | Time: 34.11s\n",
      "Iter  3200 | Train Loss: 0.394375, Val Loss: 0.393625 | LR: 0.0100 | Batch:  32 | Patience:  52/200 | Rate: 93.3 iter/s | Time: 34.31s\n",
      "Iter  3220 | Train Loss: 0.394318, Val Loss: 0.393564 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.3 iter/s | Time: 34.51s\n",
      "Iter  3240 | Train Loss: 0.394285, Val Loss: 0.393520 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 93.4 iter/s | Time: 34.71s\n",
      "Iter  3260 | Train Loss: 0.394372, Val Loss: 0.393610 | LR: 0.0100 | Batch:  32 | Patience:   5/200 | Rate: 93.4 iter/s | Time: 34.91s\n",
      "Iter  3280 | Train Loss: 0.394330, Val Loss: 0.393570 | LR: 0.0100 | Batch:  32 | Patience:   9/200 | Rate: 93.4 iter/s | Time: 35.11s\n",
      "Iter  3300 | Train Loss: 0.394332, Val Loss: 0.393578 | LR: 0.0100 | Batch:  32 | Patience:  13/200 | Rate: 93.4 iter/s | Time: 35.32s\n",
      "Iter  3320 | Train Loss: 0.394362, Val Loss: 0.393618 | LR: 0.0100 | Batch:  32 | Patience:  17/200 | Rate: 93.2 iter/s | Time: 35.62s\n",
      "Iter  3340 | Train Loss: 0.394341, Val Loss: 0.393568 | LR: 0.0100 | Batch:  32 | Patience:  21/200 | Rate: 93.2 iter/s | Time: 35.83s\n",
      "Iter  3360 | Train Loss: 0.394342, Val Loss: 0.393584 | LR: 0.0100 | Batch:  32 | Patience:  25/200 | Rate: 93.2 iter/s | Time: 36.03s\n",
      "Iter  3380 | Train Loss: 0.394318, Val Loss: 0.393557 | LR: 0.0100 | Batch:  32 | Patience:  29/200 | Rate: 93.3 iter/s | Time: 36.24s\n",
      "Iter  3400 | Train Loss: 0.394339, Val Loss: 0.393609 | LR: 0.0100 | Batch:  32 | Patience:  33/200 | Rate: 93.3 iter/s | Time: 36.44s\n",
      "Iter  3420 | Train Loss: 0.394335, Val Loss: 0.393602 | LR: 0.0100 | Batch:  32 | Patience:  37/200 | Rate: 93.4 iter/s | Time: 36.63s\n",
      "Iter  3440 | Train Loss: 0.394331, Val Loss: 0.393619 | LR: 0.0100 | Batch:  32 | Patience:  41/200 | Rate: 93.4 iter/s | Time: 36.83s\n",
      "Iter  3460 | Train Loss: 0.394313, Val Loss: 0.393604 | LR: 0.0100 | Batch:  32 | Patience:  45/200 | Rate: 93.4 iter/s | Time: 37.03s\n",
      "Iter  3480 | Train Loss: 0.394352, Val Loss: 0.393678 | LR: 0.0100 | Batch:  32 | Patience:  49/200 | Rate: 93.5 iter/s | Time: 37.22s\n",
      "Iter  3500 | Train Loss: 0.394359, Val Loss: 0.393700 | LR: 0.0100 | Batch:  32 | Patience:  53/200 | Rate: 93.4 iter/s | Time: 37.48s\n",
      "Iter  3520 | Train Loss: 0.394387, Val Loss: 0.393745 | LR: 0.0100 | Batch:  32 | Patience:  57/200 | Rate: 93.3 iter/s | Time: 37.72s\n",
      "Iter  3540 | Train Loss: 0.394398, Val Loss: 0.393764 | LR: 0.0100 | Batch:  32 | Patience:  61/200 | Rate: 93.3 iter/s | Time: 37.94s\n",
      "Iter  3560 | Train Loss: 0.394378, Val Loss: 0.393730 | LR: 0.0100 | Batch:  32 | Patience:  65/200 | Rate: 93.3 iter/s | Time: 38.15s\n",
      "Iter  3580 | Train Loss: 0.394386, Val Loss: 0.393734 | LR: 0.0100 | Batch:  32 | Patience:  69/200 | Rate: 93.3 iter/s | Time: 38.36s\n",
      "Iter  3600 | Train Loss: 0.394391, Val Loss: 0.393748 | LR: 0.0100 | Batch:  32 | Patience:  73/200 | Rate: 93.2 iter/s | Time: 38.63s\n",
      "Iter  3620 | Train Loss: 0.394360, Val Loss: 0.393716 | LR: 0.0100 | Batch:  32 | Patience:  77/200 | Rate: 93.2 iter/s | Time: 38.85s\n",
      "Iter  3640 | Train Loss: 0.394365, Val Loss: 0.393719 | LR: 0.0100 | Batch:  32 | Patience:  81/200 | Rate: 93.2 iter/s | Time: 39.07s\n",
      "Iter  3660 | Train Loss: 0.394267, Val Loss: 0.393582 | LR: 0.0100 | Batch:  32 | Patience:  85/200 | Rate: 93.2 iter/s | Time: 39.28s\n",
      "Iter  3680 | Train Loss: 0.394265, Val Loss: 0.393576 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.1 iter/s | Time: 39.52s\n",
      "Iter  3700 | Train Loss: 0.394252, Val Loss: 0.393538 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.1 iter/s | Time: 39.75s\n",
      "Iter  3720 | Train Loss: 0.394266, Val Loss: 0.393534 | LR: 0.0100 | Batch:  32 | Patience:   2/200 | Rate: 93.1 iter/s | Time: 39.96s\n",
      "Iter  3740 | Train Loss: 0.394282, Val Loss: 0.393551 | LR: 0.0100 | Batch:  32 | Patience:   6/200 | Rate: 93.1 iter/s | Time: 40.17s\n",
      "Iter  3760 | Train Loss: 0.394339, Val Loss: 0.393590 | LR: 0.0100 | Batch:  32 | Patience:  10/200 | Rate: 93.1 iter/s | Time: 40.38s\n",
      "Iter  3780 | Train Loss: 0.394338, Val Loss: 0.393600 | LR: 0.0100 | Batch:  32 | Patience:  14/200 | Rate: 93.1 iter/s | Time: 40.59s\n",
      "Iter  3800 | Train Loss: 0.394360, Val Loss: 0.393634 | LR: 0.0100 | Batch:  32 | Patience:  18/200 | Rate: 93.1 iter/s | Time: 40.80s\n",
      "Iter  3820 | Train Loss: 0.394348, Val Loss: 0.393634 | LR: 0.0100 | Batch:  32 | Patience:  22/200 | Rate: 93.2 iter/s | Time: 41.01s\n",
      "Iter  3840 | Train Loss: 0.394366, Val Loss: 0.393658 | LR: 0.0100 | Batch:  32 | Patience:  26/200 | Rate: 93.2 iter/s | Time: 41.22s\n",
      "Iter  3860 | Train Loss: 0.394368, Val Loss: 0.393646 | LR: 0.0100 | Batch:  32 | Patience:  30/200 | Rate: 93.2 iter/s | Time: 41.43s\n",
      "Iter  3880 | Train Loss: 0.394401, Val Loss: 0.393672 | LR: 0.0100 | Batch:  32 | Patience:  34/200 | Rate: 93.2 iter/s | Time: 41.63s\n",
      "Iter  3900 | Train Loss: 0.394388, Val Loss: 0.393698 | LR: 0.0100 | Batch:  32 | Patience:  38/200 | Rate: 93.2 iter/s | Time: 41.85s\n",
      "Iter  3920 | Train Loss: 0.394349, Val Loss: 0.393666 | LR: 0.0100 | Batch:  32 | Patience:  42/200 | Rate: 93.2 iter/s | Time: 42.08s\n",
      "Iter  3940 | Train Loss: 0.394331, Val Loss: 0.393650 | LR: 0.0100 | Batch:  32 | Patience:  46/200 | Rate: 93.2 iter/s | Time: 42.29s\n",
      "Iter  3960 | Train Loss: 0.394350, Val Loss: 0.393675 | LR: 0.0100 | Batch:  32 | Patience:  50/200 | Rate: 93.2 iter/s | Time: 42.51s\n",
      "Iter  3980 | Train Loss: 0.394343, Val Loss: 0.393678 | LR: 0.0100 | Batch:  32 | Patience:  54/200 | Rate: 93.2 iter/s | Time: 42.72s\n",
      "Iter  4000 | Train Loss: 0.394357, Val Loss: 0.393671 | LR: 0.0100 | Batch:  32 | Patience:  58/200 | Rate: 93.2 iter/s | Time: 42.93s\n",
      "Iter  4020 | Train Loss: 0.394362, Val Loss: 0.393677 | LR: 0.0100 | Batch:  32 | Patience:  62/200 | Rate: 93.2 iter/s | Time: 43.14s\n",
      "Iter  4040 | Train Loss: 0.394358, Val Loss: 0.393684 | LR: 0.0100 | Batch:  32 | Patience:  66/200 | Rate: 93.2 iter/s | Time: 43.34s\n",
      "Iter  4060 | Train Loss: 0.394373, Val Loss: 0.393705 | LR: 0.0100 | Batch:  32 | Patience:  70/200 | Rate: 93.2 iter/s | Time: 43.55s\n",
      "Iter  4080 | Train Loss: 0.394338, Val Loss: 0.393642 | LR: 0.0100 | Batch:  32 | Patience:  74/200 | Rate: 93.3 iter/s | Time: 43.75s\n",
      "Iter  4100 | Train Loss: 0.394334, Val Loss: 0.393609 | LR: 0.0100 | Batch:  32 | Patience:  78/200 | Rate: 93.3 iter/s | Time: 43.96s\n",
      "Iter  4120 | Train Loss: 0.394343, Val Loss: 0.393608 | LR: 0.0100 | Batch:  32 | Patience:  82/200 | Rate: 93.3 iter/s | Time: 44.17s\n",
      "Iter  4140 | Train Loss: 0.394338, Val Loss: 0.393590 | LR: 0.0100 | Batch:  32 | Patience:  86/200 | Rate: 93.3 iter/s | Time: 44.38s\n",
      "Iter  4160 | Train Loss: 0.394330, Val Loss: 0.393598 | LR: 0.0100 | Batch:  32 | Patience:  90/200 | Rate: 93.3 iter/s | Time: 44.58s\n",
      "Iter  4180 | Train Loss: 0.394318, Val Loss: 0.393599 | LR: 0.0100 | Batch:  32 | Patience:  94/200 | Rate: 93.3 iter/s | Time: 44.78s\n",
      "Iter  4200 | Train Loss: 0.394310, Val Loss: 0.393572 | LR: 0.0100 | Batch:  32 | Patience:  98/200 | Rate: 93.4 iter/s | Time: 44.98s\n",
      "Iter  4220 | Train Loss: 0.394322, Val Loss: 0.393595 | LR: 0.0100 | Batch:  32 | Patience: 102/200 | Rate: 93.3 iter/s | Time: 45.24s\n",
      "Iter  4240 | Train Loss: 0.394318, Val Loss: 0.393587 | LR: 0.0100 | Batch:  32 | Patience: 106/200 | Rate: 93.2 iter/s | Time: 45.48s\n",
      "Iter  4260 | Train Loss: 0.394348, Val Loss: 0.393620 | LR: 0.0100 | Batch:  32 | Patience: 110/200 | Rate: 93.2 iter/s | Time: 45.70s\n",
      "Iter  4280 | Train Loss: 0.394320, Val Loss: 0.393584 | LR: 0.0100 | Batch:  32 | Patience: 114/200 | Rate: 93.2 iter/s | Time: 45.91s\n",
      "Iter  4300 | Train Loss: 0.394306, Val Loss: 0.393550 | LR: 0.0100 | Batch:  32 | Patience: 118/200 | Rate: 93.2 iter/s | Time: 46.15s\n",
      "Iter  4320 | Train Loss: 0.394310, Val Loss: 0.393536 | LR: 0.0100 | Batch:  32 | Patience: 122/200 | Rate: 93.2 iter/s | Time: 46.35s\n",
      "Iter  4340 | Train Loss: 0.394314, Val Loss: 0.393548 | LR: 0.0100 | Batch:  32 | Patience: 126/200 | Rate: 93.2 iter/s | Time: 46.56s\n",
      "Iter  4360 | Train Loss: 0.394282, Val Loss: 0.393497 | LR: 0.0100 | Batch:  32 | Patience: 130/200 | Rate: 93.2 iter/s | Time: 46.77s\n",
      "Iter  4380 | Train Loss: 0.394230, Val Loss: 0.393461 | LR: 0.0100 | Batch:  32 | Patience: 134/200 | Rate: 93.2 iter/s | Time: 46.98s\n",
      "Iter  4400 | Train Loss: 0.394226, Val Loss: 0.393473 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 93.2 iter/s | Time: 47.19s\n",
      "Iter  4420 | Train Loss: 0.394215, Val Loss: 0.393471 | LR: 0.0100 | Batch:  32 | Patience:   2/200 | Rate: 93.2 iter/s | Time: 47.40s\n",
      "Iter  4440 | Train Loss: 0.394227, Val Loss: 0.393483 | LR: 0.0100 | Batch:  32 | Patience:   3/200 | Rate: 93.3 iter/s | Time: 47.60s\n",
      "Iter  4460 | Train Loss: 0.394227, Val Loss: 0.393474 | LR: 0.0100 | Batch:  32 | Patience:   7/200 | Rate: 93.3 iter/s | Time: 47.80s\n",
      "Iter  4480 | Train Loss: 0.394251, Val Loss: 0.393510 | LR: 0.0100 | Batch:  32 | Patience:  11/200 | Rate: 93.3 iter/s | Time: 48.00s\n",
      "Iter  4500 | Train Loss: 0.394265, Val Loss: 0.393520 | LR: 0.0100 | Batch:  32 | Patience:  15/200 | Rate: 93.4 iter/s | Time: 48.20s\n",
      "Iter  4520 | Train Loss: 0.394228, Val Loss: 0.393484 | LR: 0.0100 | Batch:  32 | Patience:  19/200 | Rate: 93.4 iter/s | Time: 48.40s\n",
      "Iter  4540 | Train Loss: 0.394270, Val Loss: 0.393537 | LR: 0.0100 | Batch:  32 | Patience:  23/200 | Rate: 93.4 iter/s | Time: 48.59s\n",
      "Iter  4560 | Train Loss: 0.394260, Val Loss: 0.393541 | LR: 0.0100 | Batch:  32 | Patience:  27/200 | Rate: 93.5 iter/s | Time: 48.79s\n",
      "Iter  4580 | Train Loss: 0.394272, Val Loss: 0.393582 | LR: 0.0100 | Batch:  32 | Patience:  31/200 | Rate: 93.5 iter/s | Time: 48.99s\n",
      "Iter  4600 | Train Loss: 0.394266, Val Loss: 0.393554 | LR: 0.0100 | Batch:  32 | Patience:  35/200 | Rate: 93.5 iter/s | Time: 49.18s\n",
      "Iter  4620 | Train Loss: 0.394291, Val Loss: 0.393580 | LR: 0.0100 | Batch:  32 | Patience:  39/200 | Rate: 93.6 iter/s | Time: 49.38s\n",
      "Iter  4640 | Train Loss: 0.394263, Val Loss: 0.393543 | LR: 0.0100 | Batch:  32 | Patience:  43/200 | Rate: 93.6 iter/s | Time: 49.57s\n",
      "Iter  4660 | Train Loss: 0.394254, Val Loss: 0.393539 | LR: 0.0100 | Batch:  32 | Patience:  47/200 | Rate: 93.6 iter/s | Time: 49.81s\n",
      "Iter  4680 | Train Loss: 0.394248, Val Loss: 0.393511 | LR: 0.0100 | Batch:  32 | Patience:  51/200 | Rate: 93.6 iter/s | Time: 50.01s\n",
      "Iter  4700 | Train Loss: 0.394254, Val Loss: 0.393546 | LR: 0.0100 | Batch:  32 | Patience:  55/200 | Rate: 93.6 iter/s | Time: 50.21s\n",
      "Iter  4720 | Train Loss: 0.394247, Val Loss: 0.393510 | LR: 0.0100 | Batch:  32 | Patience:  59/200 | Rate: 93.6 iter/s | Time: 50.41s\n",
      "Iter  4740 | Train Loss: 0.394276, Val Loss: 0.393515 | LR: 0.0100 | Batch:  32 | Patience:  63/200 | Rate: 93.6 iter/s | Time: 50.63s\n",
      "Iter  4760 | Train Loss: 0.394251, Val Loss: 0.393499 | LR: 0.0100 | Batch:  32 | Patience:  67/200 | Rate: 93.7 iter/s | Time: 50.83s\n",
      "Iter  4780 | Train Loss: 0.394273, Val Loss: 0.393499 | LR: 0.0100 | Batch:  32 | Patience:  71/200 | Rate: 93.7 iter/s | Time: 51.03s\n",
      "Iter  4800 | Train Loss: 0.394229, Val Loss: 0.393491 | LR: 0.0100 | Batch:  32 | Patience:  75/200 | Rate: 93.7 iter/s | Time: 51.23s\n",
      "Iter  4820 | Train Loss: 0.394197, Val Loss: 0.393470 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.7 iter/s | Time: 51.43s\n",
      "Iter  4840 | Train Loss: 0.394179, Val Loss: 0.393458 | LR: 0.0100 | Batch:  32 | Patience:   0/200 | Rate: 93.7 iter/s | Time: 51.63s\n",
      "Iter  4860 | Train Loss: 0.394209, Val Loss: 0.393507 | LR: 0.0100 | Batch:  32 | Patience:   3/200 | Rate: 93.8 iter/s | Time: 51.83s\n",
      "Iter  4880 | Train Loss: 0.394217, Val Loss: 0.393499 | LR: 0.0100 | Batch:  32 | Patience:   7/200 | Rate: 93.8 iter/s | Time: 52.02s\n",
      "Iter  4900 | Train Loss: 0.394211, Val Loss: 0.393480 | LR: 0.0100 | Batch:  32 | Patience:  11/200 | Rate: 93.8 iter/s | Time: 52.23s\n",
      "Iter  4920 | Train Loss: 0.394198, Val Loss: 0.393460 | LR: 0.0100 | Batch:  32 | Patience:  15/200 | Rate: 93.8 iter/s | Time: 52.43s\n",
      "Iter  4940 | Train Loss: 0.394196, Val Loss: 0.393459 | LR: 0.0100 | Batch:  32 | Patience:  19/200 | Rate: 93.9 iter/s | Time: 52.64s\n",
      "Iter  4960 | Train Loss: 0.394184, Val Loss: 0.393439 | LR: 0.0100 | Batch:  32 | Patience:  23/200 | Rate: 93.9 iter/s | Time: 52.83s\n",
      "Iter  4980 | Train Loss: 0.394196, Val Loss: 0.393472 | LR: 0.0100 | Batch:  32 | Patience:  27/200 | Rate: 93.9 iter/s | Time: 53.05s\n",
      "Iter  5000 | Train Loss: 0.394191, Val Loss: 0.393451 | LR: 0.0100 | Batch:  32 | Patience:  31/200 | Rate: 93.9 iter/s | Time: 53.25s\n",
      "Iter  5020 | Train Loss: 0.394189, Val Loss: 0.393459 | LR: 0.0100 | Batch:  32 | Patience:  35/200 | Rate: 93.8 iter/s | Time: 53.52s\n",
      "Iter  5040 | Train Loss: 0.394184, Val Loss: 0.393448 | LR: 0.0100 | Batch:  32 | Patience:  39/200 | Rate: 93.8 iter/s | Time: 53.75s\n",
      "Iter  5060 | Train Loss: 0.394189, Val Loss: 0.393483 | LR: 0.0100 | Batch:  32 | Patience:  43/200 | Rate: 93.8 iter/s | Time: 53.96s\n",
      "Iter  5080 | Train Loss: 0.394223, Val Loss: 0.393540 | LR: 0.0100 | Batch:  32 | Patience:  47/200 | Rate: 93.8 iter/s | Time: 54.18s\n",
      "Iter  5100 | Train Loss: 0.394217, Val Loss: 0.393526 | LR: 0.0100 | Batch:  32 | Patience:  51/200 | Rate: 93.7 iter/s | Time: 54.42s\n",
      "Iter  5120 | Train Loss: 0.394218, Val Loss: 0.393520 | LR: 0.0100 | Batch:  32 | Patience:  55/200 | Rate: 93.7 iter/s | Time: 54.62s\n",
      "Iter  5140 | Train Loss: 0.394235, Val Loss: 0.393553 | LR: 0.0100 | Batch:  32 | Patience:  59/200 | Rate: 93.6 iter/s | Time: 54.93s\n",
      "Iter  5160 | Train Loss: 0.394253, Val Loss: 0.393587 | LR: 0.0100 | Batch:  32 | Patience:  63/200 | Rate: 93.6 iter/s | Time: 55.12s\n",
      "Iter  5180 | Train Loss: 0.394247, Val Loss: 0.393582 | LR: 0.0100 | Batch:  32 | Patience:  67/200 | Rate: 93.6 iter/s | Time: 55.32s\n",
      "Iter  5200 | Train Loss: 0.394260, Val Loss: 0.393611 | LR: 0.0100 | Batch:  32 | Patience:  71/200 | Rate: 93.6 iter/s | Time: 55.53s\n",
      "Iter  5220 | Train Loss: 0.394240, Val Loss: 0.393588 | LR: 0.0100 | Batch:  32 | Patience:  75/200 | Rate: 93.7 iter/s | Time: 55.73s\n",
      "Iter  5240 | Train Loss: 0.394229, Val Loss: 0.393555 | LR: 0.0100 | Batch:  32 | Patience:  79/200 | Rate: 93.7 iter/s | Time: 55.94s\n",
      "Iter  5260 | Train Loss: 0.394248, Val Loss: 0.393580 | LR: 0.0100 | Batch:  32 | Patience:  83/200 | Rate: 93.7 iter/s | Time: 56.14s\n",
      "Iter  5280 | Train Loss: 0.394237, Val Loss: 0.393573 | LR: 0.0100 | Batch:  32 | Patience:  87/200 | Rate: 93.6 iter/s | Time: 56.41s\n",
      "Iter  5300 | Train Loss: 0.394243, Val Loss: 0.393589 | LR: 0.0100 | Batch:  32 | Patience:  91/200 | Rate: 93.6 iter/s | Time: 56.64s\n",
      "Iter  5320 | Train Loss: 0.394253, Val Loss: 0.393613 | LR: 0.0100 | Batch:  32 | Patience:  95/200 | Rate: 93.6 iter/s | Time: 56.86s\n",
      "Iter  5340 | Train Loss: 0.394261, Val Loss: 0.393613 | LR: 0.0100 | Batch:  32 | Patience:  99/200 | Rate: 93.6 iter/s | Time: 57.06s\n",
      "Iter  5360 | Train Loss: 0.394279, Val Loss: 0.393637 | LR: 0.0100 | Batch:  32 | Patience: 103/200 | Rate: 93.6 iter/s | Time: 57.28s\n",
      "Iter  5380 | Train Loss: 0.394245, Val Loss: 0.393577 | LR: 0.0100 | Batch:  32 | Patience: 107/200 | Rate: 93.6 iter/s | Time: 57.48s\n",
      "Iter  5400 | Train Loss: 0.394250, Val Loss: 0.393586 | LR: 0.0100 | Batch:  32 | Patience: 111/200 | Rate: 93.6 iter/s | Time: 57.68s\n",
      "Iter  5420 | Train Loss: 0.394257, Val Loss: 0.393588 | LR: 0.0100 | Batch:  32 | Patience: 115/200 | Rate: 93.6 iter/s | Time: 57.90s\n",
      "Iter  5440 | Train Loss: 0.394277, Val Loss: 0.393620 | LR: 0.0100 | Batch:  32 | Patience: 119/200 | Rate: 93.6 iter/s | Time: 58.10s\n",
      "Iter  5460 | Train Loss: 0.394248, Val Loss: 0.393577 | LR: 0.0100 | Batch:  32 | Patience: 123/200 | Rate: 93.6 iter/s | Time: 58.35s\n",
      "Iter  5480 | Train Loss: 0.394252, Val Loss: 0.393576 | LR: 0.0100 | Batch:  32 | Patience: 127/200 | Rate: 93.6 iter/s | Time: 58.56s\n",
      "Iter  5500 | Train Loss: 0.394270, Val Loss: 0.393612 | LR: 0.0100 | Batch:  32 | Patience: 131/200 | Rate: 93.6 iter/s | Time: 58.78s\n",
      "Iter  5520 | Train Loss: 0.394217, Val Loss: 0.393545 | LR: 0.0100 | Batch:  32 | Patience: 135/200 | Rate: 93.6 iter/s | Time: 59.00s\n",
      "Iter  5540 | Train Loss: 0.394196, Val Loss: 0.393510 | LR: 0.0100 | Batch:  32 | Patience: 139/200 | Rate: 93.6 iter/s | Time: 59.22s\n",
      "Iter  5560 | Train Loss: 0.394187, Val Loss: 0.393495 | LR: 0.0100 | Batch:  32 | Patience: 143/200 | Rate: 93.6 iter/s | Time: 59.42s\n",
      "Iter  5580 | Train Loss: 0.394182, Val Loss: 0.393488 | LR: 0.0100 | Batch:  32 | Patience:   1/200 | Rate: 93.6 iter/s | Time: 59.63s\n",
      "Iter  5600 | Train Loss: 0.394227, Val Loss: 0.393545 | LR: 0.0100 | Batch:  32 | Patience:   5/200 | Rate: 93.6 iter/s | Time: 59.83s\n",
      "Iter  5620 | Train Loss: 0.394201, Val Loss: 0.393496 | LR: 0.0100 | Batch:  32 | Patience:   9/200 | Rate: 93.6 iter/s | Time: 60.04s\n",
      "Iter  5640 | Train Loss: 0.394220, Val Loss: 0.393526 | LR: 0.0100 | Batch:  32 | Patience:  13/200 | Rate: 93.6 iter/s | Time: 60.26s\n",
      "Iter  5660 | Train Loss: 0.394206, Val Loss: 0.393518 | LR: 0.0100 | Batch:  32 | Patience:  17/200 | Rate: 93.6 iter/s | Time: 60.48s\n",
      "Iter  5680 | Train Loss: 0.394201, Val Loss: 0.393505 | LR: 0.0100 | Batch:  32 | Patience:  21/200 | Rate: 93.6 iter/s | Time: 60.71s\n",
      "Iter  5700 | Train Loss: 0.394199, Val Loss: 0.393513 | LR: 0.0100 | Batch:  32 | Patience:  25/200 | Rate: 93.6 iter/s | Time: 60.92s\n",
      "Iter  5720 | Train Loss: 0.394193, Val Loss: 0.393480 | LR: 0.0100 | Batch:  32 | Patience:  29/200 | Rate: 93.6 iter/s | Time: 61.12s\n",
      "Iter  5740 | Train Loss: 0.394211, Val Loss: 0.393472 | LR: 0.0100 | Batch:  32 | Patience:  33/200 | Rate: 93.6 iter/s | Time: 61.32s\n",
      "Iter  5760 | Train Loss: 0.394211, Val Loss: 0.393464 | LR: 0.0100 | Batch:  32 | Patience:  37/200 | Rate: 93.5 iter/s | Time: 61.61s\n",
      "Iter  5780 | Train Loss: 0.394225, Val Loss: 0.393489 | LR: 0.0100 | Batch:  32 | Patience:  41/200 | Rate: 93.5 iter/s | Time: 61.82s\n",
      "Iter  5800 | Train Loss: 0.394226, Val Loss: 0.393488 | LR: 0.0100 | Batch:  32 | Patience:  45/200 | Rate: 93.5 iter/s | Time: 62.02s\n",
      "Iter  5820 | Train Loss: 0.394215, Val Loss: 0.393482 | LR: 0.0100 | Batch:  32 | Patience:  49/200 | Rate: 93.5 iter/s | Time: 62.25s\n",
      "Iter  5840 | Train Loss: 0.394209, Val Loss: 0.393455 | LR: 0.0100 | Batch:  32 | Patience:  53/200 | Rate: 93.5 iter/s | Time: 62.47s\n",
      "Iter  5860 | Train Loss: 0.394213, Val Loss: 0.393460 | LR: 0.0100 | Batch:  32 | Patience:  57/200 | Rate: 93.5 iter/s | Time: 62.67s\n",
      "Iter  5880 | Train Loss: 0.394208, Val Loss: 0.393446 | LR: 0.0100 | Batch:  32 | Patience:  61/200 | Rate: 93.5 iter/s | Time: 62.88s\n",
      "Iter  5900 | Train Loss: 0.394218, Val Loss: 0.393448 | LR: 0.0100 | Batch:  32 | Patience:  65/200 | Rate: 93.5 iter/s | Time: 63.09s\n",
      "Iter  5920 | Train Loss: 0.394249, Val Loss: 0.393498 | LR: 0.0100 | Batch:  32 | Patience:  69/200 | Rate: 93.5 iter/s | Time: 63.32s\n",
      "Iter  5940 | Train Loss: 0.394287, Val Loss: 0.393531 | LR: 0.0100 | Batch:  32 | Patience:  73/200 | Rate: 93.5 iter/s | Time: 63.54s\n",
      "Iter  5960 | Train Loss: 0.394359, Val Loss: 0.393617 | LR: 0.0100 | Batch:  32 | Patience:  77/200 | Rate: 93.5 iter/s | Time: 63.75s\n",
      "Iter  5980 | Train Loss: 0.394317, Val Loss: 0.393561 | LR: 0.0100 | Batch:  32 | Patience:  81/200 | Rate: 93.5 iter/s | Time: 63.97s\n",
      "Iter  6000 | Train Loss: 0.394285, Val Loss: 0.393512 | LR: 0.0100 | Batch:  32 | Patience:  85/200 | Rate: 93.5 iter/s | Time: 64.18s\n",
      "Iter  6020 | Train Loss: 0.394264, Val Loss: 0.393508 | LR: 0.0100 | Batch:  32 | Patience:  89/200 | Rate: 93.5 iter/s | Time: 64.40s\n",
      "Iter  6040 | Train Loss: 0.394276, Val Loss: 0.393521 | LR: 0.0100 | Batch:  32 | Patience:  93/200 | Rate: 93.5 iter/s | Time: 64.61s\n",
      "Iter  6060 | Train Loss: 0.394258, Val Loss: 0.393498 | LR: 0.0100 | Batch:  32 | Patience:  97/200 | Rate: 93.5 iter/s | Time: 64.80s\n",
      "Iter  6080 | Train Loss: 0.394252, Val Loss: 0.393486 | LR: 0.0100 | Batch:  32 | Patience: 101/200 | Rate: 93.5 iter/s | Time: 65.06s\n",
      "Iter  6100 | Train Loss: 0.394213, Val Loss: 0.393446 | LR: 0.0100 | Batch:  32 | Patience: 105/200 | Rate: 93.5 iter/s | Time: 65.27s\n",
      "Iter  6120 | Train Loss: 0.394212, Val Loss: 0.393437 | LR: 0.0100 | Batch:  32 | Patience: 109/200 | Rate: 93.5 iter/s | Time: 65.47s\n",
      "Iter  6140 | Train Loss: 0.394235, Val Loss: 0.393490 | LR: 0.0100 | Batch:  32 | Patience: 113/200 | Rate: 93.4 iter/s | Time: 65.74s\n",
      "Iter  6160 | Train Loss: 0.394242, Val Loss: 0.393495 | LR: 0.0100 | Batch:  32 | Patience: 117/200 | Rate: 93.2 iter/s | Time: 66.09s\n",
      "Iter  6180 | Train Loss: 0.394251, Val Loss: 0.393507 | LR: 0.0100 | Batch:  32 | Patience: 121/200 | Rate: 93.1 iter/s | Time: 66.36s\n",
      "Iter  6200 | Train Loss: 0.394251, Val Loss: 0.393480 | LR: 0.0100 | Batch:  32 | Patience: 125/200 | Rate: 93.1 iter/s | Time: 66.57s\n",
      "Iter  6220 | Train Loss: 0.394261, Val Loss: 0.393503 | LR: 0.0100 | Batch:  32 | Patience: 129/200 | Rate: 93.2 iter/s | Time: 66.77s\n",
      "Iter  6240 | Train Loss: 0.394300, Val Loss: 0.393537 | LR: 0.0100 | Batch:  32 | Patience: 133/200 | Rate: 93.1 iter/s | Time: 67.01s\n",
      "Iter  6260 | Train Loss: 0.394298, Val Loss: 0.393520 | LR: 0.0100 | Batch:  32 | Patience: 137/200 | Rate: 93.1 iter/s | Time: 67.23s\n",
      "Iter  6280 | Train Loss: 0.394344, Val Loss: 0.393566 | LR: 0.0100 | Batch:  32 | Patience: 141/200 | Rate: 93.1 iter/s | Time: 67.49s\n",
      "Iter  6300 | Train Loss: 0.394346, Val Loss: 0.393556 | LR: 0.0100 | Batch:  32 | Patience: 145/200 | Rate: 93.1 iter/s | Time: 67.69s\n",
      "Iter  6320 | Train Loss: 0.394308, Val Loss: 0.393533 | LR: 0.0100 | Batch:  32 | Patience: 149/200 | Rate: 93.0 iter/s | Time: 67.96s\n",
      "Iter  6340 | Train Loss: 0.394358, Val Loss: 0.393592 | LR: 0.0100 | Batch:  32 | Patience: 153/200 | Rate: 92.9 iter/s | Time: 68.24s\n",
      "Iter  6360 | Train Loss: 0.394310, Val Loss: 0.393549 | LR: 0.0100 | Batch:  32 | Patience: 157/200 | Rate: 92.9 iter/s | Time: 68.44s\n",
      "Iter  6380 | Train Loss: 0.394351, Val Loss: 0.393612 | LR: 0.0100 | Batch:  32 | Patience: 161/200 | Rate: 92.9 iter/s | Time: 68.64s\n",
      "Iter  6400 | Train Loss: 0.394312, Val Loss: 0.393576 | LR: 0.0100 | Batch:  32 | Patience: 165/200 | Rate: 92.9 iter/s | Time: 68.87s\n",
      "Iter  6420 | Train Loss: 0.394256, Val Loss: 0.393476 | LR: 0.0100 | Batch:  32 | Patience: 169/200 | Rate: 93.0 iter/s | Time: 69.07s\n",
      "Iter  6440 | Train Loss: 0.394235, Val Loss: 0.393453 | LR: 0.0100 | Batch:  32 | Patience: 173/200 | Rate: 93.0 iter/s | Time: 69.26s\n",
      "Iter  6460 | Train Loss: 0.394235, Val Loss: 0.393460 | LR: 0.0100 | Batch:  32 | Patience: 177/200 | Rate: 93.0 iter/s | Time: 69.46s\n",
      "Iter  6480 | Train Loss: 0.394202, Val Loss: 0.393412 | LR: 0.0100 | Batch:  32 | Patience: 181/200 | Rate: 93.0 iter/s | Time: 69.65s\n",
      "Iter  6500 | Train Loss: 0.394197, Val Loss: 0.393381 | LR: 0.0100 | Batch:  32 | Patience: 185/200 | Rate: 93.1 iter/s | Time: 69.85s\n",
      "Iter  6520 | Train Loss: 0.394201, Val Loss: 0.393392 | LR: 0.0100 | Batch:  32 | Patience: 189/200 | Rate: 93.1 iter/s | Time: 70.05s\n",
      "Iter  6540 | Train Loss: 0.394227, Val Loss: 0.393410 | LR: 0.0100 | Batch:  32 | Patience: 193/200 | Rate: 93.1 iter/s | Time: 70.25s\n",
      "Iter  6560 | Train Loss: 0.394195, Val Loss: 0.393391 | LR: 0.0100 | Batch:  32 | Patience: 197/200 | Rate: 93.1 iter/s | Time: 70.46s\n",
      "\n",
      "Early stopping triggered at iteration 6571\n",
      "No improvement in loss for 200 checks (threshold: 1e-06)\n",
      "Final loss: 0.394199\n",
      "Total training time: 70.57s\n",
      "Average time per iteration: 0.0107s\n",
      "Total iterations: 6571\n",
      "Approximate epochs: 0.48\n",
      "Final loss: 0.394199\n",
      "Total iterations: 6571\n",
      "Approximate epochs: 0.48\n",
      "================================================================================\n",
      "\n",
      "Testing SGD Iterations with LR: 0.05, Batch Size: 32\n",
      "================================================================================\n",
      "Starting SGD Iterations training\n",
      "Learning rate: 0.05, Batch size: 32\n",
      "Max iterations: 50000\n",
      "Early stopping: patience=200, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "Initial full dataset loss: 0.394199\n",
      "----------------------------------------------------------------------\n",
      "Iter    20 | Train Loss: 0.394236, Val Loss: 0.393412 | LR: 0.0500 | Batch:  32 | Patience:   2/200 | Rate: 101.1 iter/s | Time: 0.20s\n",
      "Iter    40 | Train Loss: 0.394771, Val Loss: 0.394052 | LR: 0.0500 | Batch:  32 | Patience:   6/200 | Rate: 101.2 iter/s | Time: 0.40s\n",
      "Iter    60 | Train Loss: 0.394668, Val Loss: 0.393856 | LR: 0.0500 | Batch:  32 | Patience:  10/200 | Rate: 99.4 iter/s | Time: 0.60s\n",
      "Iter    80 | Train Loss: 0.394642, Val Loss: 0.393892 | LR: 0.0500 | Batch:  32 | Patience:  14/200 | Rate: 97.1 iter/s | Time: 0.82s\n",
      "Iter   100 | Train Loss: 0.394922, Val Loss: 0.394220 | LR: 0.0500 | Batch:  32 | Patience:  18/200 | Rate: 96.8 iter/s | Time: 1.03s\n",
      "Iter   120 | Train Loss: 0.394654, Val Loss: 0.393893 | LR: 0.0500 | Batch:  32 | Patience:  22/200 | Rate: 94.0 iter/s | Time: 1.28s\n",
      "Iter   140 | Train Loss: 0.394469, Val Loss: 0.393668 | LR: 0.0500 | Batch:  32 | Patience:  26/200 | Rate: 94.8 iter/s | Time: 1.48s\n",
      "Iter   160 | Train Loss: 0.394462, Val Loss: 0.393644 | LR: 0.0500 | Batch:  32 | Patience:  30/200 | Rate: 94.8 iter/s | Time: 1.69s\n",
      "Iter   180 | Train Loss: 0.394788, Val Loss: 0.393838 | LR: 0.0500 | Batch:  32 | Patience:  34/200 | Rate: 93.4 iter/s | Time: 1.93s\n",
      "Iter   200 | Train Loss: 0.394988, Val Loss: 0.394131 | LR: 0.0500 | Batch:  32 | Patience:  38/200 | Rate: 94.0 iter/s | Time: 2.13s\n",
      "Iter   220 | Train Loss: 0.394728, Val Loss: 0.393890 | LR: 0.0500 | Batch:  32 | Patience:  42/200 | Rate: 94.4 iter/s | Time: 2.33s\n",
      "Iter   240 | Train Loss: 0.394789, Val Loss: 0.393848 | LR: 0.0500 | Batch:  32 | Patience:  46/200 | Rate: 94.8 iter/s | Time: 2.53s\n",
      "Iter   260 | Train Loss: 0.394584, Val Loss: 0.393685 | LR: 0.0500 | Batch:  32 | Patience:  50/200 | Rate: 95.0 iter/s | Time: 2.74s\n",
      "Iter   280 | Train Loss: 0.394607, Val Loss: 0.393689 | LR: 0.0500 | Batch:  32 | Patience:  54/200 | Rate: 95.3 iter/s | Time: 2.94s\n",
      "Iter   300 | Train Loss: 0.395056, Val Loss: 0.394118 | LR: 0.0500 | Batch:  32 | Patience:  58/200 | Rate: 95.7 iter/s | Time: 3.13s\n",
      "Iter   320 | Train Loss: 0.395109, Val Loss: 0.394274 | LR: 0.0500 | Batch:  32 | Patience:  62/200 | Rate: 95.7 iter/s | Time: 3.34s\n",
      "Iter   340 | Train Loss: 0.394996, Val Loss: 0.394088 | LR: 0.0500 | Batch:  32 | Patience:  66/200 | Rate: 96.0 iter/s | Time: 3.54s\n",
      "Iter   360 | Train Loss: 0.394706, Val Loss: 0.393837 | LR: 0.0500 | Batch:  32 | Patience:  70/200 | Rate: 95.8 iter/s | Time: 3.76s\n",
      "Iter   380 | Train Loss: 0.394825, Val Loss: 0.393886 | LR: 0.0500 | Batch:  32 | Patience:  74/200 | Rate: 96.1 iter/s | Time: 3.96s\n",
      "Iter   400 | Train Loss: 0.394812, Val Loss: 0.393882 | LR: 0.0500 | Batch:  32 | Patience:  78/200 | Rate: 96.3 iter/s | Time: 4.15s\n",
      "Iter   420 | Train Loss: 0.394735, Val Loss: 0.393859 | LR: 0.0500 | Batch:  32 | Patience:  82/200 | Rate: 96.6 iter/s | Time: 4.35s\n",
      "Iter   440 | Train Loss: 0.394585, Val Loss: 0.393826 | LR: 0.0500 | Batch:  32 | Patience:  86/200 | Rate: 96.8 iter/s | Time: 4.55s\n",
      "Iter   460 | Train Loss: 0.394609, Val Loss: 0.393842 | LR: 0.0500 | Batch:  32 | Patience:  90/200 | Rate: 97.0 iter/s | Time: 4.74s\n",
      "Iter   480 | Train Loss: 0.394493, Val Loss: 0.393650 | LR: 0.0500 | Batch:  32 | Patience:  94/200 | Rate: 97.2 iter/s | Time: 4.94s\n",
      "Iter   500 | Train Loss: 0.394415, Val Loss: 0.393549 | LR: 0.0500 | Batch:  32 | Patience:  98/200 | Rate: 97.3 iter/s | Time: 5.14s\n",
      "Iter   520 | Train Loss: 0.394621, Val Loss: 0.393641 | LR: 0.0500 | Batch:  32 | Patience: 102/200 | Rate: 97.5 iter/s | Time: 5.33s\n",
      "Iter   540 | Train Loss: 0.394642, Val Loss: 0.393775 | LR: 0.0500 | Batch:  32 | Patience: 106/200 | Rate: 97.6 iter/s | Time: 5.53s\n",
      "Iter   560 | Train Loss: 0.394738, Val Loss: 0.393823 | LR: 0.0500 | Batch:  32 | Patience: 110/200 | Rate: 97.4 iter/s | Time: 5.75s\n",
      "Iter   580 | Train Loss: 0.394798, Val Loss: 0.393936 | LR: 0.0500 | Batch:  32 | Patience: 114/200 | Rate: 97.5 iter/s | Time: 5.95s\n",
      "Iter   600 | Train Loss: 0.394477, Val Loss: 0.393776 | LR: 0.0500 | Batch:  32 | Patience: 118/200 | Rate: 97.6 iter/s | Time: 6.15s\n",
      "Iter   620 | Train Loss: 0.394993, Val Loss: 0.394446 | LR: 0.0500 | Batch:  32 | Patience: 122/200 | Rate: 97.6 iter/s | Time: 6.35s\n",
      "Iter   640 | Train Loss: 0.394881, Val Loss: 0.394300 | LR: 0.0500 | Batch:  32 | Patience: 126/200 | Rate: 97.8 iter/s | Time: 6.55s\n",
      "Iter   660 | Train Loss: 0.394707, Val Loss: 0.394086 | LR: 0.0500 | Batch:  32 | Patience: 130/200 | Rate: 97.9 iter/s | Time: 6.74s\n",
      "Iter   680 | Train Loss: 0.394792, Val Loss: 0.394211 | LR: 0.0500 | Batch:  32 | Patience: 134/200 | Rate: 97.6 iter/s | Time: 6.97s\n",
      "Iter   700 | Train Loss: 0.394887, Val Loss: 0.394344 | LR: 0.0500 | Batch:  32 | Patience: 138/200 | Rate: 97.8 iter/s | Time: 7.16s\n",
      "Iter   720 | Train Loss: 0.394909, Val Loss: 0.394201 | LR: 0.0500 | Batch:  32 | Patience: 142/200 | Rate: 97.9 iter/s | Time: 7.35s\n",
      "Iter   740 | Train Loss: 0.394907, Val Loss: 0.394196 | LR: 0.0500 | Batch:  32 | Patience: 146/200 | Rate: 98.1 iter/s | Time: 7.55s\n",
      "Iter   760 | Train Loss: 0.395336, Val Loss: 0.394662 | LR: 0.0500 | Batch:  32 | Patience: 150/200 | Rate: 97.9 iter/s | Time: 7.76s\n",
      "Iter   780 | Train Loss: 0.394951, Val Loss: 0.394208 | LR: 0.0500 | Batch:  32 | Patience: 154/200 | Rate: 98.0 iter/s | Time: 7.96s\n",
      "Iter   800 | Train Loss: 0.394544, Val Loss: 0.393731 | LR: 0.0500 | Batch:  32 | Patience: 158/200 | Rate: 98.1 iter/s | Time: 8.16s\n",
      "Iter   820 | Train Loss: 0.394534, Val Loss: 0.393672 | LR: 0.0500 | Batch:  32 | Patience: 162/200 | Rate: 98.1 iter/s | Time: 8.36s\n",
      "Iter   840 | Train Loss: 0.394674, Val Loss: 0.393741 | LR: 0.0500 | Batch:  32 | Patience: 166/200 | Rate: 98.2 iter/s | Time: 8.55s\n",
      "Iter   860 | Train Loss: 0.394931, Val Loss: 0.393985 | LR: 0.0500 | Batch:  32 | Patience: 170/200 | Rate: 98.2 iter/s | Time: 8.75s\n",
      "Iter   880 | Train Loss: 0.394849, Val Loss: 0.393957 | LR: 0.0500 | Batch:  32 | Patience: 174/200 | Rate: 98.3 iter/s | Time: 8.96s\n",
      "Iter   900 | Train Loss: 0.394630, Val Loss: 0.393738 | LR: 0.0500 | Batch:  32 | Patience: 178/200 | Rate: 98.4 iter/s | Time: 9.15s\n",
      "Iter   920 | Train Loss: 0.394658, Val Loss: 0.393820 | LR: 0.0500 | Batch:  32 | Patience: 182/200 | Rate: 98.5 iter/s | Time: 9.34s\n",
      "Iter   940 | Train Loss: 0.394931, Val Loss: 0.394056 | LR: 0.0500 | Batch:  32 | Patience: 186/200 | Rate: 98.6 iter/s | Time: 9.54s\n",
      "Iter   960 | Train Loss: 0.395047, Val Loss: 0.394185 | LR: 0.0500 | Batch:  32 | Patience: 190/200 | Rate: 98.6 iter/s | Time: 9.73s\n",
      "Iter   980 | Train Loss: 0.394879, Val Loss: 0.394131 | LR: 0.0500 | Batch:  32 | Patience: 194/200 | Rate: 98.7 iter/s | Time: 9.93s\n",
      "Iter  1000 | Train Loss: 0.394928, Val Loss: 0.394260 | LR: 0.0500 | Batch:  32 | Patience: 198/200 | Rate: 98.7 iter/s | Time: 10.13s\n",
      "\n",
      "Early stopping triggered at iteration 1006\n",
      "No improvement in loss for 200 checks (threshold: 1e-06)\n",
      "Final loss: 0.395070\n",
      "Total training time: 10.20s\n",
      "Average time per iteration: 0.0101s\n",
      "Total iterations: 1006\n",
      "Approximate epochs: 0.07\n",
      "Final loss: 0.395070\n",
      "Total iterations: 1006\n",
      "Approximate epochs: 0.07\n",
      "================================================================================\n",
      "\n",
      "Testing SGD Iterations with LR: 0.01, Batch Size: 128\n",
      "================================================================================\n",
      "Starting SGD Iterations training\n",
      "Learning rate: 0.01, Batch size: 128\n",
      "Max iterations: 50000\n",
      "Early stopping: patience=200, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "Initial full dataset loss: 0.395070\n",
      "----------------------------------------------------------------------\n",
      "Iter    20 | Train Loss: 0.394981, Val Loss: 0.394270 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 91.7 iter/s | Time: 0.22s\n",
      "Iter    40 | Train Loss: 0.394920, Val Loss: 0.394207 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 96.2 iter/s | Time: 0.42s\n",
      "Iter    60 | Train Loss: 0.394868, Val Loss: 0.394155 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 97.3 iter/s | Time: 0.62s\n",
      "Iter    80 | Train Loss: 0.394826, Val Loss: 0.394110 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 98.0 iter/s | Time: 0.82s\n",
      "Iter   100 | Train Loss: 0.394782, Val Loss: 0.394056 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 98.6 iter/s | Time: 1.01s\n",
      "Iter   120 | Train Loss: 0.394738, Val Loss: 0.394024 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 99.1 iter/s | Time: 1.21s\n",
      "Iter   140 | Train Loss: 0.394722, Val Loss: 0.394006 | LR: 0.0100 | Batch: 128 | Patience:   2/200 | Rate: 99.7 iter/s | Time: 1.40s\n",
      "Iter   160 | Train Loss: 0.394700, Val Loss: 0.393982 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 99.1 iter/s | Time: 1.61s\n",
      "Iter   180 | Train Loss: 0.394682, Val Loss: 0.393964 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 99.2 iter/s | Time: 1.81s\n",
      "Iter   200 | Train Loss: 0.394634, Val Loss: 0.393919 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 94.3 iter/s | Time: 2.12s\n",
      "Iter   220 | Train Loss: 0.394637, Val Loss: 0.393912 | LR: 0.0100 | Batch: 128 | Patience:   3/200 | Rate: 94.7 iter/s | Time: 2.32s\n",
      "Iter   240 | Train Loss: 0.394609, Val Loss: 0.393878 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 93.9 iter/s | Time: 2.55s\n",
      "Iter   260 | Train Loss: 0.394592, Val Loss: 0.393868 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 90.4 iter/s | Time: 2.88s\n",
      "Iter   280 | Train Loss: 0.394573, Val Loss: 0.393852 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 89.6 iter/s | Time: 3.13s\n",
      "Iter   300 | Train Loss: 0.394559, Val Loss: 0.393836 | LR: 0.0100 | Batch: 128 | Patience:   5/200 | Rate: 89.7 iter/s | Time: 3.34s\n",
      "Iter   320 | Train Loss: 0.394534, Val Loss: 0.393816 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 89.5 iter/s | Time: 3.58s\n",
      "Iter   340 | Train Loss: 0.394501, Val Loss: 0.393788 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 87.2 iter/s | Time: 3.90s\n",
      "Iter   360 | Train Loss: 0.394466, Val Loss: 0.393754 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 87.8 iter/s | Time: 4.10s\n",
      "Iter   380 | Train Loss: 0.394439, Val Loss: 0.393731 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 86.9 iter/s | Time: 4.37s\n",
      "Iter   400 | Train Loss: 0.394418, Val Loss: 0.393705 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 87.5 iter/s | Time: 4.57s\n",
      "Iter   420 | Train Loss: 0.394411, Val Loss: 0.393706 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 87.8 iter/s | Time: 4.79s\n",
      "Iter   440 | Train Loss: 0.394391, Val Loss: 0.393682 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 88.3 iter/s | Time: 4.98s\n",
      "Iter   460 | Train Loss: 0.394390, Val Loss: 0.393677 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 88.7 iter/s | Time: 5.19s\n",
      "Iter   480 | Train Loss: 0.394370, Val Loss: 0.393650 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 89.0 iter/s | Time: 5.39s\n",
      "Iter   500 | Train Loss: 0.394355, Val Loss: 0.393642 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 89.4 iter/s | Time: 5.59s\n",
      "Iter   520 | Train Loss: 0.394339, Val Loss: 0.393631 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 89.5 iter/s | Time: 5.81s\n",
      "Iter   540 | Train Loss: 0.394321, Val Loss: 0.393605 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 89.9 iter/s | Time: 6.01s\n",
      "Iter   560 | Train Loss: 0.394314, Val Loss: 0.393592 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 90.2 iter/s | Time: 6.21s\n",
      "Iter   580 | Train Loss: 0.394293, Val Loss: 0.393576 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 90.4 iter/s | Time: 6.41s\n",
      "Iter   600 | Train Loss: 0.394289, Val Loss: 0.393575 | LR: 0.0100 | Batch: 128 | Patience:   3/200 | Rate: 90.6 iter/s | Time: 6.62s\n",
      "Iter   620 | Train Loss: 0.394272, Val Loss: 0.393576 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 90.8 iter/s | Time: 6.83s\n",
      "Iter   640 | Train Loss: 0.394279, Val Loss: 0.393593 | LR: 0.0100 | Batch: 128 | Patience:   2/200 | Rate: 91.0 iter/s | Time: 7.03s\n",
      "Iter   660 | Train Loss: 0.394260, Val Loss: 0.393561 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 91.2 iter/s | Time: 7.24s\n",
      "Iter   680 | Train Loss: 0.394256, Val Loss: 0.393561 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 91.3 iter/s | Time: 7.44s\n",
      "Iter   700 | Train Loss: 0.394239, Val Loss: 0.393546 | LR: 0.0100 | Batch: 128 | Patience:   4/200 | Rate: 90.7 iter/s | Time: 7.72s\n",
      "Iter   720 | Train Loss: 0.394221, Val Loss: 0.393513 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 90.5 iter/s | Time: 7.95s\n",
      "Iter   740 | Train Loss: 0.394211, Val Loss: 0.393519 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 90.4 iter/s | Time: 8.18s\n",
      "Iter   760 | Train Loss: 0.394207, Val Loss: 0.393521 | LR: 0.0100 | Batch: 128 | Patience:   5/200 | Rate: 90.2 iter/s | Time: 8.42s\n",
      "Iter   780 | Train Loss: 0.394185, Val Loss: 0.393487 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 90.2 iter/s | Time: 8.65s\n",
      "Iter   800 | Train Loss: 0.394201, Val Loss: 0.393512 | LR: 0.0100 | Batch: 128 | Patience:   3/200 | Rate: 90.1 iter/s | Time: 8.88s\n",
      "Iter   820 | Train Loss: 0.394183, Val Loss: 0.393484 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 90.0 iter/s | Time: 9.11s\n",
      "Iter   840 | Train Loss: 0.394177, Val Loss: 0.393483 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 90.2 iter/s | Time: 9.32s\n",
      "Iter   860 | Train Loss: 0.394174, Val Loss: 0.393474 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 90.4 iter/s | Time: 9.52s\n",
      "Iter   880 | Train Loss: 0.394155, Val Loss: 0.393461 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 90.6 iter/s | Time: 9.71s\n",
      "Iter   900 | Train Loss: 0.394161, Val Loss: 0.393472 | LR: 0.0100 | Batch: 128 | Patience:   4/200 | Rate: 90.7 iter/s | Time: 9.92s\n",
      "Iter   920 | Train Loss: 0.394149, Val Loss: 0.393467 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 90.9 iter/s | Time: 10.12s\n",
      "Iter   940 | Train Loss: 0.394150, Val Loss: 0.393460 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 91.0 iter/s | Time: 10.32s\n",
      "Iter   960 | Train Loss: 0.394133, Val Loss: 0.393435 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 91.2 iter/s | Time: 10.52s\n",
      "Iter   980 | Train Loss: 0.394130, Val Loss: 0.393430 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 91.4 iter/s | Time: 10.72s\n",
      "Iter  1000 | Train Loss: 0.394121, Val Loss: 0.393414 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 91.6 iter/s | Time: 10.91s\n",
      "Iter  1020 | Train Loss: 0.394108, Val Loss: 0.393394 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 91.8 iter/s | Time: 11.11s\n",
      "Iter  1040 | Train Loss: 0.394106, Val Loss: 0.393394 | LR: 0.0100 | Batch: 128 | Patience:   4/200 | Rate: 91.9 iter/s | Time: 11.31s\n",
      "Iter  1060 | Train Loss: 0.394109, Val Loss: 0.393396 | LR: 0.0100 | Batch: 128 | Patience:   2/200 | Rate: 92.1 iter/s | Time: 11.51s\n",
      "Iter  1080 | Train Loss: 0.394114, Val Loss: 0.393407 | LR: 0.0100 | Batch: 128 | Patience:   6/200 | Rate: 92.2 iter/s | Time: 11.71s\n",
      "Iter  1100 | Train Loss: 0.394111, Val Loss: 0.393405 | LR: 0.0100 | Batch: 128 | Patience:  10/200 | Rate: 92.3 iter/s | Time: 11.92s\n",
      "Iter  1120 | Train Loss: 0.394107, Val Loss: 0.393403 | LR: 0.0100 | Batch: 128 | Patience:  14/200 | Rate: 92.3 iter/s | Time: 12.13s\n",
      "Iter  1140 | Train Loss: 0.394108, Val Loss: 0.393403 | LR: 0.0100 | Batch: 128 | Patience:  18/200 | Rate: 92.1 iter/s | Time: 12.37s\n",
      "Iter  1160 | Train Loss: 0.394101, Val Loss: 0.393398 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 92.3 iter/s | Time: 12.57s\n",
      "Iter  1180 | Train Loss: 0.394099, Val Loss: 0.393393 | LR: 0.0100 | Batch: 128 | Patience:   2/200 | Rate: 92.4 iter/s | Time: 12.77s\n",
      "Iter  1200 | Train Loss: 0.394095, Val Loss: 0.393381 | LR: 0.0100 | Batch: 128 | Patience:   6/200 | Rate: 92.5 iter/s | Time: 12.98s\n",
      "Iter  1220 | Train Loss: 0.394095, Val Loss: 0.393377 | LR: 0.0100 | Batch: 128 | Patience:   2/200 | Rate: 92.6 iter/s | Time: 13.18s\n",
      "Iter  1240 | Train Loss: 0.394097, Val Loss: 0.393379 | LR: 0.0100 | Batch: 128 | Patience:   6/200 | Rate: 92.7 iter/s | Time: 13.38s\n",
      "Iter  1260 | Train Loss: 0.394103, Val Loss: 0.393391 | LR: 0.0100 | Batch: 128 | Patience:  10/200 | Rate: 92.8 iter/s | Time: 13.57s\n",
      "Iter  1280 | Train Loss: 0.394125, Val Loss: 0.393407 | LR: 0.0100 | Batch: 128 | Patience:  14/200 | Rate: 92.9 iter/s | Time: 13.78s\n",
      "Iter  1300 | Train Loss: 0.394108, Val Loss: 0.393385 | LR: 0.0100 | Batch: 128 | Patience:  18/200 | Rate: 93.0 iter/s | Time: 13.98s\n",
      "Iter  1320 | Train Loss: 0.394100, Val Loss: 0.393374 | LR: 0.0100 | Batch: 128 | Patience:  22/200 | Rate: 93.1 iter/s | Time: 14.18s\n",
      "Iter  1340 | Train Loss: 0.394104, Val Loss: 0.393368 | LR: 0.0100 | Batch: 128 | Patience:  26/200 | Rate: 93.1 iter/s | Time: 14.40s\n",
      "Iter  1360 | Train Loss: 0.394103, Val Loss: 0.393369 | LR: 0.0100 | Batch: 128 | Patience:  30/200 | Rate: 92.9 iter/s | Time: 14.63s\n",
      "Iter  1380 | Train Loss: 0.394110, Val Loss: 0.393366 | LR: 0.0100 | Batch: 128 | Patience:  34/200 | Rate: 92.3 iter/s | Time: 14.95s\n",
      "Iter  1400 | Train Loss: 0.394099, Val Loss: 0.393360 | LR: 0.0100 | Batch: 128 | Patience:  38/200 | Rate: 92.2 iter/s | Time: 15.18s\n",
      "Iter  1420 | Train Loss: 0.394094, Val Loss: 0.393366 | LR: 0.0100 | Batch: 128 | Patience:  42/200 | Rate: 92.4 iter/s | Time: 15.38s\n",
      "Iter  1440 | Train Loss: 0.394079, Val Loss: 0.393359 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 92.5 iter/s | Time: 15.57s\n",
      "Iter  1460 | Train Loss: 0.394076, Val Loss: 0.393354 | LR: 0.0100 | Batch: 128 | Patience:   2/200 | Rate: 92.6 iter/s | Time: 15.78s\n",
      "Iter  1480 | Train Loss: 0.394070, Val Loss: 0.393356 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 92.6 iter/s | Time: 15.98s\n",
      "Iter  1500 | Train Loss: 0.394066, Val Loss: 0.393349 | LR: 0.0100 | Batch: 128 | Patience:   2/200 | Rate: 92.7 iter/s | Time: 16.18s\n",
      "Iter  1520 | Train Loss: 0.394063, Val Loss: 0.393346 | LR: 0.0100 | Batch: 128 | Patience:   6/200 | Rate: 92.5 iter/s | Time: 16.42s\n",
      "Iter  1540 | Train Loss: 0.394059, Val Loss: 0.393339 | LR: 0.0100 | Batch: 128 | Patience:   2/200 | Rate: 92.6 iter/s | Time: 16.62s\n",
      "Iter  1560 | Train Loss: 0.394062, Val Loss: 0.393345 | LR: 0.0100 | Batch: 128 | Patience:   3/200 | Rate: 92.6 iter/s | Time: 16.84s\n",
      "Iter  1580 | Train Loss: 0.394066, Val Loss: 0.393350 | LR: 0.0100 | Batch: 128 | Patience:   7/200 | Rate: 92.7 iter/s | Time: 17.04s\n",
      "Iter  1600 | Train Loss: 0.394057, Val Loss: 0.393326 | LR: 0.0100 | Batch: 128 | Patience:  11/200 | Rate: 92.0 iter/s | Time: 17.39s\n",
      "Iter  1620 | Train Loss: 0.394054, Val Loss: 0.393331 | LR: 0.0100 | Batch: 128 | Patience:   3/200 | Rate: 91.7 iter/s | Time: 17.67s\n",
      "Iter  1640 | Train Loss: 0.394057, Val Loss: 0.393337 | LR: 0.0100 | Batch: 128 | Patience:   7/200 | Rate: 91.7 iter/s | Time: 17.89s\n",
      "Iter  1660 | Train Loss: 0.394057, Val Loss: 0.393336 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 91.7 iter/s | Time: 18.09s\n",
      "Iter  1680 | Train Loss: 0.394053, Val Loss: 0.393322 | LR: 0.0100 | Batch: 128 | Patience:   5/200 | Rate: 91.7 iter/s | Time: 18.31s\n",
      "Iter  1700 | Train Loss: 0.394051, Val Loss: 0.393320 | LR: 0.0100 | Batch: 128 | Patience:   9/200 | Rate: 91.7 iter/s | Time: 18.53s\n",
      "Iter  1720 | Train Loss: 0.394053, Val Loss: 0.393331 | LR: 0.0100 | Batch: 128 | Patience:   3/200 | Rate: 91.8 iter/s | Time: 18.74s\n",
      "Iter  1740 | Train Loss: 0.394056, Val Loss: 0.393335 | LR: 0.0100 | Batch: 128 | Patience:   7/200 | Rate: 91.7 iter/s | Time: 18.98s\n",
      "Iter  1760 | Train Loss: 0.394054, Val Loss: 0.393328 | LR: 0.0100 | Batch: 128 | Patience:  11/200 | Rate: 91.5 iter/s | Time: 19.23s\n",
      "Iter  1780 | Train Loss: 0.394046, Val Loss: 0.393313 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 91.6 iter/s | Time: 19.44s\n",
      "Iter  1800 | Train Loss: 0.394045, Val Loss: 0.393305 | LR: 0.0100 | Batch: 128 | Patience:   4/200 | Rate: 91.7 iter/s | Time: 19.64s\n",
      "Iter  1820 | Train Loss: 0.394046, Val Loss: 0.393305 | LR: 0.0100 | Batch: 128 | Patience:   2/200 | Rate: 91.7 iter/s | Time: 19.85s\n",
      "Iter  1840 | Train Loss: 0.394048, Val Loss: 0.393325 | LR: 0.0100 | Batch: 128 | Patience:   6/200 | Rate: 91.7 iter/s | Time: 20.07s\n",
      "Iter  1860 | Train Loss: 0.394054, Val Loss: 0.393341 | LR: 0.0100 | Batch: 128 | Patience:  10/200 | Rate: 91.8 iter/s | Time: 20.27s\n",
      "Iter  1880 | Train Loss: 0.394049, Val Loss: 0.393334 | LR: 0.0100 | Batch: 128 | Patience:  14/200 | Rate: 91.6 iter/s | Time: 20.52s\n",
      "Iter  1900 | Train Loss: 0.394059, Val Loss: 0.393342 | LR: 0.0100 | Batch: 128 | Patience:  18/200 | Rate: 91.3 iter/s | Time: 20.82s\n",
      "Iter  1920 | Train Loss: 0.394067, Val Loss: 0.393350 | LR: 0.0100 | Batch: 128 | Patience:  22/200 | Rate: 91.2 iter/s | Time: 21.04s\n",
      "Iter  1940 | Train Loss: 0.394069, Val Loss: 0.393365 | LR: 0.0100 | Batch: 128 | Patience:  26/200 | Rate: 91.1 iter/s | Time: 21.29s\n",
      "Iter  1960 | Train Loss: 0.394071, Val Loss: 0.393361 | LR: 0.0100 | Batch: 128 | Patience:  30/200 | Rate: 91.0 iter/s | Time: 21.53s\n",
      "Iter  1980 | Train Loss: 0.394061, Val Loss: 0.393336 | LR: 0.0100 | Batch: 128 | Patience:  34/200 | Rate: 91.1 iter/s | Time: 21.73s\n",
      "Iter  2000 | Train Loss: 0.394059, Val Loss: 0.393331 | LR: 0.0100 | Batch: 128 | Patience:  38/200 | Rate: 91.1 iter/s | Time: 21.96s\n",
      "Iter  2020 | Train Loss: 0.394058, Val Loss: 0.393328 | LR: 0.0100 | Batch: 128 | Patience:  42/200 | Rate: 91.0 iter/s | Time: 22.19s\n",
      "Iter  2040 | Train Loss: 0.394064, Val Loss: 0.393331 | LR: 0.0100 | Batch: 128 | Patience:  46/200 | Rate: 91.1 iter/s | Time: 22.39s\n",
      "Iter  2060 | Train Loss: 0.394068, Val Loss: 0.393341 | LR: 0.0100 | Batch: 128 | Patience:  50/200 | Rate: 91.1 iter/s | Time: 22.61s\n",
      "Iter  2080 | Train Loss: 0.394090, Val Loss: 0.393357 | LR: 0.0100 | Batch: 128 | Patience:  54/200 | Rate: 91.2 iter/s | Time: 22.82s\n",
      "Iter  2100 | Train Loss: 0.394084, Val Loss: 0.393353 | LR: 0.0100 | Batch: 128 | Patience:  58/200 | Rate: 91.2 iter/s | Time: 23.03s\n",
      "Iter  2120 | Train Loss: 0.394077, Val Loss: 0.393347 | LR: 0.0100 | Batch: 128 | Patience:  62/200 | Rate: 91.2 iter/s | Time: 23.23s\n",
      "Iter  2140 | Train Loss: 0.394077, Val Loss: 0.393345 | LR: 0.0100 | Batch: 128 | Patience:  66/200 | Rate: 91.2 iter/s | Time: 23.46s\n",
      "Iter  2160 | Train Loss: 0.394075, Val Loss: 0.393337 | LR: 0.0100 | Batch: 128 | Patience:  70/200 | Rate: 91.1 iter/s | Time: 23.71s\n",
      "Iter  2180 | Train Loss: 0.394060, Val Loss: 0.393322 | LR: 0.0100 | Batch: 128 | Patience:  74/200 | Rate: 91.0 iter/s | Time: 23.96s\n",
      "Iter  2200 | Train Loss: 0.394056, Val Loss: 0.393324 | LR: 0.0100 | Batch: 128 | Patience:  78/200 | Rate: 90.7 iter/s | Time: 24.26s\n",
      "Iter  2220 | Train Loss: 0.394057, Val Loss: 0.393316 | LR: 0.0100 | Batch: 128 | Patience:  82/200 | Rate: 90.7 iter/s | Time: 24.47s\n",
      "Iter  2240 | Train Loss: 0.394059, Val Loss: 0.393313 | LR: 0.0100 | Batch: 128 | Patience:  86/200 | Rate: 90.7 iter/s | Time: 24.71s\n",
      "Iter  2260 | Train Loss: 0.394060, Val Loss: 0.393321 | LR: 0.0100 | Batch: 128 | Patience:  90/200 | Rate: 90.7 iter/s | Time: 24.92s\n",
      "Iter  2280 | Train Loss: 0.394058, Val Loss: 0.393311 | LR: 0.0100 | Batch: 128 | Patience:  94/200 | Rate: 90.7 iter/s | Time: 25.13s\n",
      "Iter  2300 | Train Loss: 0.394053, Val Loss: 0.393314 | LR: 0.0100 | Batch: 128 | Patience:  98/200 | Rate: 90.7 iter/s | Time: 25.36s\n",
      "Iter  2320 | Train Loss: 0.394044, Val Loss: 0.393293 | LR: 0.0100 | Batch: 128 | Patience: 102/200 | Rate: 90.8 iter/s | Time: 25.56s\n",
      "Iter  2340 | Train Loss: 0.394034, Val Loss: 0.393275 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 90.8 iter/s | Time: 25.76s\n",
      "Iter  2360 | Train Loss: 0.394029, Val Loss: 0.393285 | LR: 0.0100 | Batch: 128 | Patience:   2/200 | Rate: 90.9 iter/s | Time: 25.96s\n",
      "Iter  2380 | Train Loss: 0.394027, Val Loss: 0.393297 | LR: 0.0100 | Batch: 128 | Patience:   2/200 | Rate: 90.9 iter/s | Time: 26.19s\n",
      "Iter  2400 | Train Loss: 0.394027, Val Loss: 0.393287 | LR: 0.0100 | Batch: 128 | Patience:   6/200 | Rate: 90.9 iter/s | Time: 26.40s\n",
      "Iter  2420 | Train Loss: 0.394036, Val Loss: 0.393289 | LR: 0.0100 | Batch: 128 | Patience:  10/200 | Rate: 91.0 iter/s | Time: 26.61s\n",
      "Iter  2440 | Train Loss: 0.394030, Val Loss: 0.393289 | LR: 0.0100 | Batch: 128 | Patience:  14/200 | Rate: 91.0 iter/s | Time: 26.81s\n",
      "Iter  2460 | Train Loss: 0.394039, Val Loss: 0.393287 | LR: 0.0100 | Batch: 128 | Patience:  18/200 | Rate: 91.0 iter/s | Time: 27.04s\n",
      "Iter  2480 | Train Loss: 0.394041, Val Loss: 0.393302 | LR: 0.0100 | Batch: 128 | Patience:  22/200 | Rate: 91.0 iter/s | Time: 27.24s\n",
      "Iter  2500 | Train Loss: 0.394034, Val Loss: 0.393303 | LR: 0.0100 | Batch: 128 | Patience:  26/200 | Rate: 91.1 iter/s | Time: 27.44s\n",
      "Iter  2520 | Train Loss: 0.394032, Val Loss: 0.393307 | LR: 0.0100 | Batch: 128 | Patience:  30/200 | Rate: 91.2 iter/s | Time: 27.64s\n",
      "Iter  2540 | Train Loss: 0.394036, Val Loss: 0.393307 | LR: 0.0100 | Batch: 128 | Patience:  34/200 | Rate: 91.2 iter/s | Time: 27.84s\n",
      "Iter  2560 | Train Loss: 0.394032, Val Loss: 0.393308 | LR: 0.0100 | Batch: 128 | Patience:  38/200 | Rate: 91.3 iter/s | Time: 28.05s\n",
      "Iter  2580 | Train Loss: 0.394026, Val Loss: 0.393320 | LR: 0.0100 | Batch: 128 | Patience:  42/200 | Rate: 91.3 iter/s | Time: 28.26s\n",
      "Iter  2600 | Train Loss: 0.394030, Val Loss: 0.393310 | LR: 0.0100 | Batch: 128 | Patience:  46/200 | Rate: 91.3 iter/s | Time: 28.47s\n",
      "Iter  2620 | Train Loss: 0.394035, Val Loss: 0.393319 | LR: 0.0100 | Batch: 128 | Patience:  50/200 | Rate: 91.3 iter/s | Time: 28.71s\n",
      "Iter  2640 | Train Loss: 0.394036, Val Loss: 0.393316 | LR: 0.0100 | Batch: 128 | Patience:  54/200 | Rate: 91.3 iter/s | Time: 28.92s\n",
      "Iter  2660 | Train Loss: 0.394030, Val Loss: 0.393319 | LR: 0.0100 | Batch: 128 | Patience:  58/200 | Rate: 91.3 iter/s | Time: 29.12s\n",
      "Iter  2680 | Train Loss: 0.394033, Val Loss: 0.393316 | LR: 0.0100 | Batch: 128 | Patience:  62/200 | Rate: 91.4 iter/s | Time: 29.34s\n",
      "Iter  2700 | Train Loss: 0.394038, Val Loss: 0.393317 | LR: 0.0100 | Batch: 128 | Patience:  66/200 | Rate: 91.3 iter/s | Time: 29.59s\n",
      "Iter  2720 | Train Loss: 0.394037, Val Loss: 0.393317 | LR: 0.0100 | Batch: 128 | Patience:  70/200 | Rate: 91.2 iter/s | Time: 29.82s\n",
      "Iter  2740 | Train Loss: 0.394041, Val Loss: 0.393318 | LR: 0.0100 | Batch: 128 | Patience:  74/200 | Rate: 91.1 iter/s | Time: 30.06s\n",
      "Iter  2760 | Train Loss: 0.394047, Val Loss: 0.393318 | LR: 0.0100 | Batch: 128 | Patience:  78/200 | Rate: 91.1 iter/s | Time: 30.31s\n",
      "Iter  2780 | Train Loss: 0.394040, Val Loss: 0.393313 | LR: 0.0100 | Batch: 128 | Patience:  82/200 | Rate: 90.9 iter/s | Time: 30.57s\n",
      "Iter  2800 | Train Loss: 0.394031, Val Loss: 0.393311 | LR: 0.0100 | Batch: 128 | Patience:  86/200 | Rate: 90.8 iter/s | Time: 30.84s\n",
      "Iter  2820 | Train Loss: 0.394031, Val Loss: 0.393305 | LR: 0.0100 | Batch: 128 | Patience:  90/200 | Rate: 90.7 iter/s | Time: 31.09s\n",
      "Iter  2840 | Train Loss: 0.394039, Val Loss: 0.393308 | LR: 0.0100 | Batch: 128 | Patience:  94/200 | Rate: 90.6 iter/s | Time: 31.33s\n",
      "Iter  2860 | Train Loss: 0.394034, Val Loss: 0.393300 | LR: 0.0100 | Batch: 128 | Patience:  98/200 | Rate: 90.7 iter/s | Time: 31.54s\n",
      "Iter  2880 | Train Loss: 0.394032, Val Loss: 0.393299 | LR: 0.0100 | Batch: 128 | Patience: 102/200 | Rate: 90.7 iter/s | Time: 31.75s\n",
      "Iter  2900 | Train Loss: 0.394044, Val Loss: 0.393305 | LR: 0.0100 | Batch: 128 | Patience: 106/200 | Rate: 90.6 iter/s | Time: 31.99s\n",
      "Iter  2920 | Train Loss: 0.394045, Val Loss: 0.393299 | LR: 0.0100 | Batch: 128 | Patience: 110/200 | Rate: 90.6 iter/s | Time: 32.24s\n",
      "Iter  2940 | Train Loss: 0.394046, Val Loss: 0.393300 | LR: 0.0100 | Batch: 128 | Patience: 114/200 | Rate: 90.5 iter/s | Time: 32.47s\n",
      "Iter  2960 | Train Loss: 0.394047, Val Loss: 0.393315 | LR: 0.0100 | Batch: 128 | Patience: 118/200 | Rate: 90.5 iter/s | Time: 32.70s\n",
      "Iter  2980 | Train Loss: 0.394046, Val Loss: 0.393313 | LR: 0.0100 | Batch: 128 | Patience: 122/200 | Rate: 90.6 iter/s | Time: 32.90s\n",
      "Iter  3000 | Train Loss: 0.394039, Val Loss: 0.393321 | LR: 0.0100 | Batch: 128 | Patience: 126/200 | Rate: 90.4 iter/s | Time: 33.18s\n",
      "Iter  3020 | Train Loss: 0.394037, Val Loss: 0.393316 | LR: 0.0100 | Batch: 128 | Patience: 130/200 | Rate: 90.3 iter/s | Time: 33.43s\n",
      "Iter  3040 | Train Loss: 0.394036, Val Loss: 0.393302 | LR: 0.0100 | Batch: 128 | Patience: 134/200 | Rate: 90.3 iter/s | Time: 33.66s\n",
      "Iter  3060 | Train Loss: 0.394030, Val Loss: 0.393309 | LR: 0.0100 | Batch: 128 | Patience: 138/200 | Rate: 90.0 iter/s | Time: 34.00s\n",
      "Iter  3080 | Train Loss: 0.394042, Val Loss: 0.393307 | LR: 0.0100 | Batch: 128 | Patience: 142/200 | Rate: 89.9 iter/s | Time: 34.27s\n",
      "Iter  3100 | Train Loss: 0.394042, Val Loss: 0.393301 | LR: 0.0100 | Batch: 128 | Patience: 146/200 | Rate: 89.7 iter/s | Time: 34.54s\n",
      "Iter  3120 | Train Loss: 0.394047, Val Loss: 0.393296 | LR: 0.0100 | Batch: 128 | Patience: 150/200 | Rate: 89.5 iter/s | Time: 34.84s\n",
      "Iter  3140 | Train Loss: 0.394048, Val Loss: 0.393296 | LR: 0.0100 | Batch: 128 | Patience: 154/200 | Rate: 89.5 iter/s | Time: 35.08s\n",
      "Iter  3160 | Train Loss: 0.394053, Val Loss: 0.393307 | LR: 0.0100 | Batch: 128 | Patience: 158/200 | Rate: 89.5 iter/s | Time: 35.31s\n",
      "Iter  3180 | Train Loss: 0.394051, Val Loss: 0.393305 | LR: 0.0100 | Batch: 128 | Patience: 162/200 | Rate: 89.5 iter/s | Time: 35.52s\n",
      "Iter  3200 | Train Loss: 0.394052, Val Loss: 0.393304 | LR: 0.0100 | Batch: 128 | Patience: 166/200 | Rate: 89.6 iter/s | Time: 35.73s\n",
      "Iter  3220 | Train Loss: 0.394035, Val Loss: 0.393291 | LR: 0.0100 | Batch: 128 | Patience: 170/200 | Rate: 89.6 iter/s | Time: 35.95s\n",
      "Iter  3240 | Train Loss: 0.394024, Val Loss: 0.393297 | LR: 0.0100 | Batch: 128 | Patience: 174/200 | Rate: 89.4 iter/s | Time: 36.24s\n",
      "Iter  3260 | Train Loss: 0.394024, Val Loss: 0.393299 | LR: 0.0100 | Batch: 128 | Patience:   3/200 | Rate: 89.4 iter/s | Time: 36.48s\n",
      "Iter  3280 | Train Loss: 0.394022, Val Loss: 0.393300 | LR: 0.0100 | Batch: 128 | Patience:   7/200 | Rate: 89.4 iter/s | Time: 36.71s\n",
      "Iter  3300 | Train Loss: 0.394021, Val Loss: 0.393307 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 89.4 iter/s | Time: 36.93s\n",
      "Iter  3320 | Train Loss: 0.394014, Val Loss: 0.393296 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 89.3 iter/s | Time: 37.16s\n",
      "Iter  3340 | Train Loss: 0.394016, Val Loss: 0.393287 | LR: 0.0100 | Batch: 128 | Patience:   5/200 | Rate: 89.4 iter/s | Time: 37.36s\n",
      "Iter  3360 | Train Loss: 0.394014, Val Loss: 0.393288 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 89.4 iter/s | Time: 37.58s\n",
      "Iter  3380 | Train Loss: 0.394018, Val Loss: 0.393278 | LR: 0.0100 | Batch: 128 | Patience:   5/200 | Rate: 89.4 iter/s | Time: 37.80s\n",
      "Iter  3400 | Train Loss: 0.394017, Val Loss: 0.393284 | LR: 0.0100 | Batch: 128 | Patience:   9/200 | Rate: 89.4 iter/s | Time: 38.02s\n",
      "Iter  3420 | Train Loss: 0.394016, Val Loss: 0.393285 | LR: 0.0100 | Batch: 128 | Patience:  13/200 | Rate: 89.5 iter/s | Time: 38.23s\n",
      "Iter  3440 | Train Loss: 0.394019, Val Loss: 0.393286 | LR: 0.0100 | Batch: 128 | Patience:  17/200 | Rate: 89.5 iter/s | Time: 38.43s\n",
      "Iter  3460 | Train Loss: 0.394037, Val Loss: 0.393287 | LR: 0.0100 | Batch: 128 | Patience:  21/200 | Rate: 89.6 iter/s | Time: 38.63s\n",
      "Iter  3480 | Train Loss: 0.394028, Val Loss: 0.393281 | LR: 0.0100 | Batch: 128 | Patience:  25/200 | Rate: 89.6 iter/s | Time: 38.86s\n",
      "Iter  3500 | Train Loss: 0.394031, Val Loss: 0.393281 | LR: 0.0100 | Batch: 128 | Patience:  29/200 | Rate: 89.5 iter/s | Time: 39.09s\n",
      "Iter  3520 | Train Loss: 0.394031, Val Loss: 0.393291 | LR: 0.0100 | Batch: 128 | Patience:  33/200 | Rate: 89.5 iter/s | Time: 39.32s\n",
      "Iter  3540 | Train Loss: 0.394032, Val Loss: 0.393284 | LR: 0.0100 | Batch: 128 | Patience:  37/200 | Rate: 89.5 iter/s | Time: 39.55s\n",
      "Iter  3560 | Train Loss: 0.394023, Val Loss: 0.393283 | LR: 0.0100 | Batch: 128 | Patience:  41/200 | Rate: 89.5 iter/s | Time: 39.78s\n",
      "Iter  3580 | Train Loss: 0.394029, Val Loss: 0.393286 | LR: 0.0100 | Batch: 128 | Patience:  45/200 | Rate: 89.5 iter/s | Time: 40.00s\n",
      "Iter  3600 | Train Loss: 0.394040, Val Loss: 0.393287 | LR: 0.0100 | Batch: 128 | Patience:  49/200 | Rate: 89.5 iter/s | Time: 40.24s\n",
      "Iter  3620 | Train Loss: 0.394036, Val Loss: 0.393285 | LR: 0.0100 | Batch: 128 | Patience:  53/200 | Rate: 89.4 iter/s | Time: 40.47s\n",
      "Iter  3640 | Train Loss: 0.394030, Val Loss: 0.393279 | LR: 0.0100 | Batch: 128 | Patience:  57/200 | Rate: 89.5 iter/s | Time: 40.69s\n",
      "Iter  3660 | Train Loss: 0.394034, Val Loss: 0.393281 | LR: 0.0100 | Batch: 128 | Patience:  61/200 | Rate: 89.5 iter/s | Time: 40.91s\n",
      "Iter  3680 | Train Loss: 0.394037, Val Loss: 0.393286 | LR: 0.0100 | Batch: 128 | Patience:  65/200 | Rate: 89.5 iter/s | Time: 41.14s\n",
      "Iter  3700 | Train Loss: 0.394034, Val Loss: 0.393293 | LR: 0.0100 | Batch: 128 | Patience:  69/200 | Rate: 89.4 iter/s | Time: 41.37s\n",
      "Iter  3720 | Train Loss: 0.394018, Val Loss: 0.393282 | LR: 0.0100 | Batch: 128 | Patience:  73/200 | Rate: 89.3 iter/s | Time: 41.67s\n",
      "Iter  3740 | Train Loss: 0.394022, Val Loss: 0.393282 | LR: 0.0100 | Batch: 128 | Patience:  77/200 | Rate: 89.2 iter/s | Time: 41.92s\n",
      "Iter  3760 | Train Loss: 0.394030, Val Loss: 0.393287 | LR: 0.0100 | Batch: 128 | Patience:  81/200 | Rate: 89.2 iter/s | Time: 42.15s\n",
      "Iter  3780 | Train Loss: 0.394040, Val Loss: 0.393285 | LR: 0.0100 | Batch: 128 | Patience:  85/200 | Rate: 89.0 iter/s | Time: 42.47s\n",
      "Iter  3800 | Train Loss: 0.394036, Val Loss: 0.393294 | LR: 0.0100 | Batch: 128 | Patience:  89/200 | Rate: 88.9 iter/s | Time: 42.75s\n",
      "Iter  3820 | Train Loss: 0.394038, Val Loss: 0.393295 | LR: 0.0100 | Batch: 128 | Patience:  93/200 | Rate: 88.7 iter/s | Time: 43.06s\n",
      "Iter  3840 | Train Loss: 0.394049, Val Loss: 0.393307 | LR: 0.0100 | Batch: 128 | Patience:  97/200 | Rate: 88.7 iter/s | Time: 43.27s\n",
      "Iter  3860 | Train Loss: 0.394064, Val Loss: 0.393317 | LR: 0.0100 | Batch: 128 | Patience: 101/200 | Rate: 88.7 iter/s | Time: 43.50s\n",
      "Iter  3880 | Train Loss: 0.394057, Val Loss: 0.393306 | LR: 0.0100 | Batch: 128 | Patience: 105/200 | Rate: 88.8 iter/s | Time: 43.71s\n",
      "Iter  3900 | Train Loss: 0.394064, Val Loss: 0.393299 | LR: 0.0100 | Batch: 128 | Patience: 109/200 | Rate: 88.8 iter/s | Time: 43.93s\n",
      "Iter  3920 | Train Loss: 0.394050, Val Loss: 0.393296 | LR: 0.0100 | Batch: 128 | Patience: 113/200 | Rate: 88.8 iter/s | Time: 44.13s\n",
      "Iter  3940 | Train Loss: 0.394051, Val Loss: 0.393303 | LR: 0.0100 | Batch: 128 | Patience: 117/200 | Rate: 88.9 iter/s | Time: 44.34s\n",
      "Iter  3960 | Train Loss: 0.394045, Val Loss: 0.393290 | LR: 0.0100 | Batch: 128 | Patience: 121/200 | Rate: 88.9 iter/s | Time: 44.54s\n",
      "Iter  3980 | Train Loss: 0.394041, Val Loss: 0.393277 | LR: 0.0100 | Batch: 128 | Patience: 125/200 | Rate: 88.9 iter/s | Time: 44.74s\n",
      "Iter  4000 | Train Loss: 0.394067, Val Loss: 0.393290 | LR: 0.0100 | Batch: 128 | Patience: 129/200 | Rate: 88.9 iter/s | Time: 44.97s\n",
      "Iter  4020 | Train Loss: 0.394060, Val Loss: 0.393282 | LR: 0.0100 | Batch: 128 | Patience: 133/200 | Rate: 88.8 iter/s | Time: 45.25s\n",
      "Iter  4040 | Train Loss: 0.394081, Val Loss: 0.393303 | LR: 0.0100 | Batch: 128 | Patience: 137/200 | Rate: 88.8 iter/s | Time: 45.49s\n",
      "Iter  4060 | Train Loss: 0.394059, Val Loss: 0.393284 | LR: 0.0100 | Batch: 128 | Patience: 141/200 | Rate: 88.8 iter/s | Time: 45.73s\n",
      "Iter  4080 | Train Loss: 0.394046, Val Loss: 0.393280 | LR: 0.0100 | Batch: 128 | Patience: 145/200 | Rate: 88.8 iter/s | Time: 45.94s\n",
      "Iter  4100 | Train Loss: 0.394050, Val Loss: 0.393279 | LR: 0.0100 | Batch: 128 | Patience: 149/200 | Rate: 88.8 iter/s | Time: 46.18s\n",
      "Iter  4120 | Train Loss: 0.394069, Val Loss: 0.393294 | LR: 0.0100 | Batch: 128 | Patience: 153/200 | Rate: 88.8 iter/s | Time: 46.40s\n",
      "Iter  4140 | Train Loss: 0.394070, Val Loss: 0.393290 | LR: 0.0100 | Batch: 128 | Patience: 157/200 | Rate: 88.8 iter/s | Time: 46.63s\n",
      "Iter  4160 | Train Loss: 0.394062, Val Loss: 0.393278 | LR: 0.0100 | Batch: 128 | Patience: 161/200 | Rate: 88.8 iter/s | Time: 46.86s\n",
      "Iter  4180 | Train Loss: 0.394033, Val Loss: 0.393267 | LR: 0.0100 | Batch: 128 | Patience: 165/200 | Rate: 88.8 iter/s | Time: 47.07s\n",
      "Iter  4200 | Train Loss: 0.394022, Val Loss: 0.393270 | LR: 0.0100 | Batch: 128 | Patience: 169/200 | Rate: 88.8 iter/s | Time: 47.28s\n",
      "Iter  4220 | Train Loss: 0.394011, Val Loss: 0.393271 | LR: 0.0100 | Batch: 128 | Patience: 173/200 | Rate: 88.8 iter/s | Time: 47.50s\n",
      "Iter  4240 | Train Loss: 0.394016, Val Loss: 0.393269 | LR: 0.0100 | Batch: 128 | Patience:   1/200 | Rate: 88.9 iter/s | Time: 47.72s\n",
      "Iter  4260 | Train Loss: 0.394025, Val Loss: 0.393275 | LR: 0.0100 | Batch: 128 | Patience:   5/200 | Rate: 88.8 iter/s | Time: 47.98s\n",
      "Iter  4280 | Train Loss: 0.394008, Val Loss: 0.393269 | LR: 0.0100 | Batch: 128 | Patience:   9/200 | Rate: 88.7 iter/s | Time: 48.23s\n",
      "Iter  4300 | Train Loss: 0.394002, Val Loss: 0.393262 | LR: 0.0100 | Batch: 128 | Patience:  13/200 | Rate: 88.8 iter/s | Time: 48.42s\n",
      "Iter  4320 | Train Loss: 0.394004, Val Loss: 0.393256 | LR: 0.0100 | Batch: 128 | Patience:   3/200 | Rate: 88.8 iter/s | Time: 48.64s\n",
      "Iter  4340 | Train Loss: 0.394006, Val Loss: 0.393255 | LR: 0.0100 | Batch: 128 | Patience:   7/200 | Rate: 88.8 iter/s | Time: 48.87s\n",
      "Iter  4360 | Train Loss: 0.394000, Val Loss: 0.393254 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 88.8 iter/s | Time: 49.08s\n",
      "Iter  4380 | Train Loss: 0.394008, Val Loss: 0.393251 | LR: 0.0100 | Batch: 128 | Patience:   3/200 | Rate: 88.9 iter/s | Time: 49.29s\n",
      "Iter  4400 | Train Loss: 0.394008, Val Loss: 0.393257 | LR: 0.0100 | Batch: 128 | Patience:   7/200 | Rate: 88.9 iter/s | Time: 49.51s\n",
      "Iter  4420 | Train Loss: 0.394008, Val Loss: 0.393250 | LR: 0.0100 | Batch: 128 | Patience:  11/200 | Rate: 88.9 iter/s | Time: 49.72s\n",
      "Iter  4440 | Train Loss: 0.394019, Val Loss: 0.393246 | LR: 0.0100 | Batch: 128 | Patience:  15/200 | Rate: 88.9 iter/s | Time: 49.94s\n",
      "Iter  4460 | Train Loss: 0.394020, Val Loss: 0.393252 | LR: 0.0100 | Batch: 128 | Patience:  19/200 | Rate: 88.9 iter/s | Time: 50.16s\n",
      "Iter  4480 | Train Loss: 0.394001, Val Loss: 0.393246 | LR: 0.0100 | Batch: 128 | Patience:  23/200 | Rate: 88.9 iter/s | Time: 50.39s\n",
      "Iter  4500 | Train Loss: 0.394001, Val Loss: 0.393240 | LR: 0.0100 | Batch: 128 | Patience:  27/200 | Rate: 88.9 iter/s | Time: 50.63s\n",
      "Iter  4520 | Train Loss: 0.394007, Val Loss: 0.393244 | LR: 0.0100 | Batch: 128 | Patience:  31/200 | Rate: 88.9 iter/s | Time: 50.82s\n",
      "Iter  4540 | Train Loss: 0.394002, Val Loss: 0.393233 | LR: 0.0100 | Batch: 128 | Patience:  35/200 | Rate: 89.0 iter/s | Time: 51.03s\n",
      "Iter  4560 | Train Loss: 0.393996, Val Loss: 0.393243 | LR: 0.0100 | Batch: 128 | Patience:   0/200 | Rate: 89.0 iter/s | Time: 51.23s\n",
      "Iter  4580 | Train Loss: 0.394001, Val Loss: 0.393237 | LR: 0.0100 | Batch: 128 | Patience:   3/200 | Rate: 89.0 iter/s | Time: 51.45s\n",
      "Iter  4600 | Train Loss: 0.394002, Val Loss: 0.393232 | LR: 0.0100 | Batch: 128 | Patience:   7/200 | Rate: 89.0 iter/s | Time: 51.66s\n",
      "Iter  4620 | Train Loss: 0.394003, Val Loss: 0.393229 | LR: 0.0100 | Batch: 128 | Patience:  11/200 | Rate: 89.1 iter/s | Time: 51.87s\n",
      "Iter  4640 | Train Loss: 0.394013, Val Loss: 0.393236 | LR: 0.0100 | Batch: 128 | Patience:  15/200 | Rate: 89.1 iter/s | Time: 52.07s\n",
      "Iter  4660 | Train Loss: 0.394001, Val Loss: 0.393241 | LR: 0.0100 | Batch: 128 | Patience:  19/200 | Rate: 89.1 iter/s | Time: 52.28s\n",
      "Iter  4680 | Train Loss: 0.394017, Val Loss: 0.393242 | LR: 0.0100 | Batch: 128 | Patience:  23/200 | Rate: 89.2 iter/s | Time: 52.48s\n",
      "Iter  4700 | Train Loss: 0.394047, Val Loss: 0.393251 | LR: 0.0100 | Batch: 128 | Patience:  27/200 | Rate: 89.2 iter/s | Time: 52.69s\n",
      "Iter  4720 | Train Loss: 0.394047, Val Loss: 0.393257 | LR: 0.0100 | Batch: 128 | Patience:  31/200 | Rate: 89.2 iter/s | Time: 52.92s\n",
      "Iter  4740 | Train Loss: 0.394045, Val Loss: 0.393257 | LR: 0.0100 | Batch: 128 | Patience:  35/200 | Rate: 89.2 iter/s | Time: 53.13s\n",
      "Iter  4760 | Train Loss: 0.394025, Val Loss: 0.393245 | LR: 0.0100 | Batch: 128 | Patience:  39/200 | Rate: 89.2 iter/s | Time: 53.35s\n",
      "Iter  4780 | Train Loss: 0.394013, Val Loss: 0.393240 | LR: 0.0100 | Batch: 128 | Patience:  43/200 | Rate: 89.2 iter/s | Time: 53.59s\n",
      "Iter  4800 | Train Loss: 0.394010, Val Loss: 0.393235 | LR: 0.0100 | Batch: 128 | Patience:  47/200 | Rate: 89.2 iter/s | Time: 53.79s\n",
      "Iter  4820 | Train Loss: 0.394011, Val Loss: 0.393230 | LR: 0.0100 | Batch: 128 | Patience:  51/200 | Rate: 89.2 iter/s | Time: 54.02s\n",
      "Iter  4840 | Train Loss: 0.394022, Val Loss: 0.393234 | LR: 0.0100 | Batch: 128 | Patience:  55/200 | Rate: 89.3 iter/s | Time: 54.23s\n",
      "Iter  4860 | Train Loss: 0.394025, Val Loss: 0.393243 | LR: 0.0100 | Batch: 128 | Patience:  59/200 | Rate: 89.2 iter/s | Time: 54.47s\n",
      "Iter  4880 | Train Loss: 0.394032, Val Loss: 0.393246 | LR: 0.0100 | Batch: 128 | Patience:  63/200 | Rate: 89.3 iter/s | Time: 54.67s\n",
      "Iter  4900 | Train Loss: 0.394017, Val Loss: 0.393239 | LR: 0.0100 | Batch: 128 | Patience:  67/200 | Rate: 89.3 iter/s | Time: 54.88s\n",
      "Iter  4920 | Train Loss: 0.394022, Val Loss: 0.393254 | LR: 0.0100 | Batch: 128 | Patience:  71/200 | Rate: 89.3 iter/s | Time: 55.10s\n",
      "Iter  4940 | Train Loss: 0.394011, Val Loss: 0.393246 | LR: 0.0100 | Batch: 128 | Patience:  75/200 | Rate: 89.3 iter/s | Time: 55.30s\n",
      "Iter  4960 | Train Loss: 0.394012, Val Loss: 0.393238 | LR: 0.0100 | Batch: 128 | Patience:  79/200 | Rate: 89.4 iter/s | Time: 55.51s\n",
      "Iter  4980 | Train Loss: 0.394013, Val Loss: 0.393235 | LR: 0.0100 | Batch: 128 | Patience:  83/200 | Rate: 89.4 iter/s | Time: 55.71s\n",
      "Iter  5000 | Train Loss: 0.394013, Val Loss: 0.393240 | LR: 0.0100 | Batch: 128 | Patience:  87/200 | Rate: 89.4 iter/s | Time: 55.95s\n",
      "Iter  5020 | Train Loss: 0.394017, Val Loss: 0.393244 | LR: 0.0100 | Batch: 128 | Patience:  91/200 | Rate: 89.3 iter/s | Time: 56.19s\n",
      "Iter  5040 | Train Loss: 0.394018, Val Loss: 0.393241 | LR: 0.0100 | Batch: 128 | Patience:  95/200 | Rate: 89.4 iter/s | Time: 56.40s\n",
      "Iter  5060 | Train Loss: 0.394024, Val Loss: 0.393249 | LR: 0.0100 | Batch: 128 | Patience:  99/200 | Rate: 89.3 iter/s | Time: 56.64s\n",
      "Iter  5080 | Train Loss: 0.394039, Val Loss: 0.393262 | LR: 0.0100 | Batch: 128 | Patience: 103/200 | Rate: 89.3 iter/s | Time: 56.87s\n",
      "Iter  5100 | Train Loss: 0.394041, Val Loss: 0.393270 | LR: 0.0100 | Batch: 128 | Patience: 107/200 | Rate: 89.3 iter/s | Time: 57.10s\n",
      "Iter  5120 | Train Loss: 0.394032, Val Loss: 0.393275 | LR: 0.0100 | Batch: 128 | Patience: 111/200 | Rate: 89.3 iter/s | Time: 57.31s\n",
      "Iter  5140 | Train Loss: 0.394037, Val Loss: 0.393274 | LR: 0.0100 | Batch: 128 | Patience: 115/200 | Rate: 89.4 iter/s | Time: 57.52s\n",
      "Iter  5160 | Train Loss: 0.394014, Val Loss: 0.393268 | LR: 0.0100 | Batch: 128 | Patience: 119/200 | Rate: 89.4 iter/s | Time: 57.73s\n",
      "Iter  5180 | Train Loss: 0.394014, Val Loss: 0.393276 | LR: 0.0100 | Batch: 128 | Patience: 123/200 | Rate: 89.4 iter/s | Time: 57.94s\n",
      "Iter  5200 | Train Loss: 0.394015, Val Loss: 0.393281 | LR: 0.0100 | Batch: 128 | Patience: 127/200 | Rate: 89.4 iter/s | Time: 58.15s\n",
      "Iter  5220 | Train Loss: 0.394017, Val Loss: 0.393282 | LR: 0.0100 | Batch: 128 | Patience: 131/200 | Rate: 89.4 iter/s | Time: 58.37s\n",
      "Iter  5240 | Train Loss: 0.394010, Val Loss: 0.393281 | LR: 0.0100 | Batch: 128 | Patience: 135/200 | Rate: 89.4 iter/s | Time: 58.59s\n",
      "Iter  5260 | Train Loss: 0.394006, Val Loss: 0.393269 | LR: 0.0100 | Batch: 128 | Patience: 139/200 | Rate: 89.5 iter/s | Time: 58.80s\n",
      "Iter  5280 | Train Loss: 0.394005, Val Loss: 0.393255 | LR: 0.0100 | Batch: 128 | Patience: 143/200 | Rate: 89.5 iter/s | Time: 59.01s\n",
      "Iter  5300 | Train Loss: 0.394005, Val Loss: 0.393256 | LR: 0.0100 | Batch: 128 | Patience: 147/200 | Rate: 89.5 iter/s | Time: 59.22s\n",
      "Iter  5320 | Train Loss: 0.394004, Val Loss: 0.393253 | LR: 0.0100 | Batch: 128 | Patience: 151/200 | Rate: 89.5 iter/s | Time: 59.43s\n",
      "Iter  5340 | Train Loss: 0.394009, Val Loss: 0.393262 | LR: 0.0100 | Batch: 128 | Patience: 155/200 | Rate: 89.5 iter/s | Time: 59.67s\n",
      "Iter  5360 | Train Loss: 0.394009, Val Loss: 0.393257 | LR: 0.0100 | Batch: 128 | Patience: 159/200 | Rate: 89.5 iter/s | Time: 59.87s\n",
      "Iter  5380 | Train Loss: 0.394012, Val Loss: 0.393262 | LR: 0.0100 | Batch: 128 | Patience: 163/200 | Rate: 89.5 iter/s | Time: 60.10s\n",
      "Iter  5400 | Train Loss: 0.394022, Val Loss: 0.393265 | LR: 0.0100 | Batch: 128 | Patience: 167/200 | Rate: 89.5 iter/s | Time: 60.31s\n",
      "Iter  5420 | Train Loss: 0.394025, Val Loss: 0.393265 | LR: 0.0100 | Batch: 128 | Patience: 171/200 | Rate: 89.6 iter/s | Time: 60.51s\n",
      "Iter  5440 | Train Loss: 0.394021, Val Loss: 0.393257 | LR: 0.0100 | Batch: 128 | Patience: 175/200 | Rate: 89.6 iter/s | Time: 60.72s\n",
      "Iter  5460 | Train Loss: 0.394024, Val Loss: 0.393266 | LR: 0.0100 | Batch: 128 | Patience: 179/200 | Rate: 89.6 iter/s | Time: 60.94s\n",
      "Iter  5480 | Train Loss: 0.394014, Val Loss: 0.393267 | LR: 0.0100 | Batch: 128 | Patience: 183/200 | Rate: 89.6 iter/s | Time: 61.14s\n",
      "Iter  5500 | Train Loss: 0.394014, Val Loss: 0.393261 | LR: 0.0100 | Batch: 128 | Patience: 187/200 | Rate: 89.6 iter/s | Time: 61.35s\n",
      "Iter  5520 | Train Loss: 0.394007, Val Loss: 0.393245 | LR: 0.0100 | Batch: 128 | Patience: 191/200 | Rate: 89.7 iter/s | Time: 61.55s\n",
      "Iter  5540 | Train Loss: 0.394009, Val Loss: 0.393251 | LR: 0.0100 | Batch: 128 | Patience: 195/200 | Rate: 89.7 iter/s | Time: 61.75s\n",
      "Iter  5560 | Train Loss: 0.394011, Val Loss: 0.393256 | LR: 0.0100 | Batch: 128 | Patience: 199/200 | Rate: 89.7 iter/s | Time: 61.95s\n",
      "\n",
      "Early stopping triggered at iteration 5561\n",
      "No improvement in loss for 200 checks (threshold: 1e-06)\n",
      "Final loss: 0.394010\n",
      "Total training time: 61.97s\n",
      "Average time per iteration: 0.0111s\n",
      "Total iterations: 5561\n",
      "Approximate epochs: 1.62\n",
      "Final loss: 0.394010\n",
      "Total iterations: 5561\n",
      "Approximate epochs: 1.62\n",
      "================================================================================\n",
      "\n",
      "Testing SGD Iterations with LR: 0.05, Batch Size: 128\n",
      "================================================================================\n",
      "Starting SGD Iterations training\n",
      "Learning rate: 0.05, Batch size: 128\n",
      "Max iterations: 50000\n",
      "Early stopping: patience=200, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "Initial full dataset loss: 0.394010\n",
      "----------------------------------------------------------------------\n",
      "Iter    20 | Train Loss: 0.394071, Val Loss: 0.393309 | LR: 0.0500 | Batch: 128 | Patience:   3/200 | Rate: 101.6 iter/s | Time: 0.20s\n",
      "Iter    40 | Train Loss: 0.394073, Val Loss: 0.393299 | LR: 0.0500 | Batch: 128 | Patience:   7/200 | Rate: 101.7 iter/s | Time: 0.39s\n",
      "Iter    60 | Train Loss: 0.394151, Val Loss: 0.393386 | LR: 0.0500 | Batch: 128 | Patience:  11/200 | Rate: 90.2 iter/s | Time: 0.67s\n",
      "Iter    80 | Train Loss: 0.394251, Val Loss: 0.393513 | LR: 0.0500 | Batch: 128 | Patience:  15/200 | Rate: 93.1 iter/s | Time: 0.86s\n",
      "Iter   100 | Train Loss: 0.394179, Val Loss: 0.393435 | LR: 0.0500 | Batch: 128 | Patience:  19/200 | Rate: 94.9 iter/s | Time: 1.05s\n",
      "Iter   120 | Train Loss: 0.394102, Val Loss: 0.393316 | LR: 0.0500 | Batch: 128 | Patience:  23/200 | Rate: 95.9 iter/s | Time: 1.25s\n",
      "Iter   140 | Train Loss: 0.394133, Val Loss: 0.393289 | LR: 0.0500 | Batch: 128 | Patience:  27/200 | Rate: 95.8 iter/s | Time: 1.46s\n",
      "Iter   160 | Train Loss: 0.394153, Val Loss: 0.393309 | LR: 0.0500 | Batch: 128 | Patience:  31/200 | Rate: 95.7 iter/s | Time: 1.67s\n",
      "Iter   180 | Train Loss: 0.394189, Val Loss: 0.393353 | LR: 0.0500 | Batch: 128 | Patience:  35/200 | Rate: 95.8 iter/s | Time: 1.88s\n",
      "Iter   200 | Train Loss: 0.394271, Val Loss: 0.393437 | LR: 0.0500 | Batch: 128 | Patience:  39/200 | Rate: 94.2 iter/s | Time: 2.12s\n",
      "Iter   220 | Train Loss: 0.394295, Val Loss: 0.393523 | LR: 0.0500 | Batch: 128 | Patience:  43/200 | Rate: 91.8 iter/s | Time: 2.40s\n",
      "Iter   240 | Train Loss: 0.394398, Val Loss: 0.393704 | LR: 0.0500 | Batch: 128 | Patience:  47/200 | Rate: 92.2 iter/s | Time: 2.60s\n",
      "Iter   260 | Train Loss: 0.394406, Val Loss: 0.393694 | LR: 0.0500 | Batch: 128 | Patience:  51/200 | Rate: 91.6 iter/s | Time: 2.84s\n",
      "Iter   280 | Train Loss: 0.394297, Val Loss: 0.393578 | LR: 0.0500 | Batch: 128 | Patience:  55/200 | Rate: 92.1 iter/s | Time: 3.04s\n",
      "Iter   300 | Train Loss: 0.394299, Val Loss: 0.393629 | LR: 0.0500 | Batch: 128 | Patience:  59/200 | Rate: 91.6 iter/s | Time: 3.27s\n",
      "Iter   320 | Train Loss: 0.394201, Val Loss: 0.393456 | LR: 0.0500 | Batch: 128 | Patience:  63/200 | Rate: 91.0 iter/s | Time: 3.52s\n",
      "Iter   340 | Train Loss: 0.394211, Val Loss: 0.393478 | LR: 0.0500 | Batch: 128 | Patience:  67/200 | Rate: 90.6 iter/s | Time: 3.75s\n",
      "Iter   360 | Train Loss: 0.394207, Val Loss: 0.393462 | LR: 0.0500 | Batch: 128 | Patience:  71/200 | Rate: 89.8 iter/s | Time: 4.01s\n",
      "Iter   380 | Train Loss: 0.394295, Val Loss: 0.393579 | LR: 0.0500 | Batch: 128 | Patience:  75/200 | Rate: 90.3 iter/s | Time: 4.21s\n",
      "Iter   400 | Train Loss: 0.394393, Val Loss: 0.393599 | LR: 0.0500 | Batch: 128 | Patience:  79/200 | Rate: 90.5 iter/s | Time: 4.42s\n",
      "Iter   420 | Train Loss: 0.394306, Val Loss: 0.393511 | LR: 0.0500 | Batch: 128 | Patience:  83/200 | Rate: 90.8 iter/s | Time: 4.62s\n",
      "Iter   440 | Train Loss: 0.394393, Val Loss: 0.393567 | LR: 0.0500 | Batch: 128 | Patience:  87/200 | Rate: 91.1 iter/s | Time: 4.83s\n",
      "Iter   460 | Train Loss: 0.394359, Val Loss: 0.393520 | LR: 0.0500 | Batch: 128 | Patience:  91/200 | Rate: 91.0 iter/s | Time: 5.06s\n",
      "Iter   480 | Train Loss: 0.394480, Val Loss: 0.393593 | LR: 0.0500 | Batch: 128 | Patience:  95/200 | Rate: 90.3 iter/s | Time: 5.32s\n",
      "Iter   500 | Train Loss: 0.394283, Val Loss: 0.393406 | LR: 0.0500 | Batch: 128 | Patience:  99/200 | Rate: 90.4 iter/s | Time: 5.53s\n",
      "Iter   520 | Train Loss: 0.394258, Val Loss: 0.393393 | LR: 0.0500 | Batch: 128 | Patience: 103/200 | Rate: 90.5 iter/s | Time: 5.74s\n",
      "Iter   540 | Train Loss: 0.394446, Val Loss: 0.393510 | LR: 0.0500 | Batch: 128 | Patience: 107/200 | Rate: 90.2 iter/s | Time: 5.98s\n",
      "Iter   560 | Train Loss: 0.394316, Val Loss: 0.393368 | LR: 0.0500 | Batch: 128 | Patience: 111/200 | Rate: 90.3 iter/s | Time: 6.20s\n",
      "Iter   580 | Train Loss: 0.394239, Val Loss: 0.393315 | LR: 0.0500 | Batch: 128 | Patience: 115/200 | Rate: 90.5 iter/s | Time: 6.41s\n",
      "Iter   600 | Train Loss: 0.394235, Val Loss: 0.393321 | LR: 0.0500 | Batch: 128 | Patience: 119/200 | Rate: 89.6 iter/s | Time: 6.70s\n",
      "Iter   620 | Train Loss: 0.394218, Val Loss: 0.393321 | LR: 0.0500 | Batch: 128 | Patience: 123/200 | Rate: 89.4 iter/s | Time: 6.93s\n",
      "Iter   640 | Train Loss: 0.394218, Val Loss: 0.393318 | LR: 0.0500 | Batch: 128 | Patience: 127/200 | Rate: 89.5 iter/s | Time: 7.15s\n",
      "Iter   660 | Train Loss: 0.394168, Val Loss: 0.393356 | LR: 0.0500 | Batch: 128 | Patience: 131/200 | Rate: 89.7 iter/s | Time: 7.35s\n",
      "Iter   680 | Train Loss: 0.394198, Val Loss: 0.393374 | LR: 0.0500 | Batch: 128 | Patience: 135/200 | Rate: 89.8 iter/s | Time: 7.58s\n",
      "Iter   700 | Train Loss: 0.394226, Val Loss: 0.393414 | LR: 0.0500 | Batch: 128 | Patience: 139/200 | Rate: 89.3 iter/s | Time: 7.84s\n",
      "Iter   720 | Train Loss: 0.394069, Val Loss: 0.393278 | LR: 0.0500 | Batch: 128 | Patience: 143/200 | Rate: 89.6 iter/s | Time: 8.04s\n",
      "Iter   740 | Train Loss: 0.394058, Val Loss: 0.393265 | LR: 0.0500 | Batch: 128 | Patience: 147/200 | Rate: 89.8 iter/s | Time: 8.24s\n",
      "Iter   760 | Train Loss: 0.394069, Val Loss: 0.393267 | LR: 0.0500 | Batch: 128 | Patience: 151/200 | Rate: 90.1 iter/s | Time: 8.44s\n",
      "Iter   780 | Train Loss: 0.394176, Val Loss: 0.393354 | LR: 0.0500 | Batch: 128 | Patience: 155/200 | Rate: 90.3 iter/s | Time: 8.64s\n",
      "Iter   800 | Train Loss: 0.394095, Val Loss: 0.393286 | LR: 0.0500 | Batch: 128 | Patience: 159/200 | Rate: 90.2 iter/s | Time: 8.87s\n",
      "Iter   820 | Train Loss: 0.394138, Val Loss: 0.393385 | LR: 0.0500 | Batch: 128 | Patience: 163/200 | Rate: 90.4 iter/s | Time: 9.07s\n",
      "Iter   840 | Train Loss: 0.394160, Val Loss: 0.393410 | LR: 0.0500 | Batch: 128 | Patience: 167/200 | Rate: 90.4 iter/s | Time: 9.29s\n",
      "Iter   860 | Train Loss: 0.394162, Val Loss: 0.393372 | LR: 0.0500 | Batch: 128 | Patience: 171/200 | Rate: 90.6 iter/s | Time: 9.50s\n",
      "Iter   880 | Train Loss: 0.394102, Val Loss: 0.393348 | LR: 0.0500 | Batch: 128 | Patience: 175/200 | Rate: 90.8 iter/s | Time: 9.69s\n",
      "Iter   900 | Train Loss: 0.394149, Val Loss: 0.393360 | LR: 0.0500 | Batch: 128 | Patience: 179/200 | Rate: 91.0 iter/s | Time: 9.89s\n",
      "Iter   920 | Train Loss: 0.394112, Val Loss: 0.393426 | LR: 0.0500 | Batch: 128 | Patience: 183/200 | Rate: 91.2 iter/s | Time: 10.09s\n",
      "Iter   940 | Train Loss: 0.394200, Val Loss: 0.393464 | LR: 0.0500 | Batch: 128 | Patience: 187/200 | Rate: 91.2 iter/s | Time: 10.31s\n",
      "Iter   960 | Train Loss: 0.394318, Val Loss: 0.393593 | LR: 0.0500 | Batch: 128 | Patience: 191/200 | Rate: 91.2 iter/s | Time: 10.53s\n",
      "Iter   980 | Train Loss: 0.394313, Val Loss: 0.393582 | LR: 0.0500 | Batch: 128 | Patience: 195/200 | Rate: 91.3 iter/s | Time: 10.73s\n",
      "Iter  1000 | Train Loss: 0.394215, Val Loss: 0.393486 | LR: 0.0500 | Batch: 128 | Patience: 199/200 | Rate: 91.4 iter/s | Time: 10.94s\n",
      "\n",
      "Early stopping triggered at iteration 1001\n",
      "No improvement in loss for 200 checks (threshold: 1e-06)\n",
      "Final loss: 0.394240\n",
      "Total training time: 10.96s\n",
      "Average time per iteration: 0.0109s\n",
      "Total iterations: 1001\n",
      "Approximate epochs: 0.29\n",
      "Final loss: 0.394240\n",
      "Total iterations: 1001\n",
      "Approximate epochs: 0.29\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "sgd_iter_weighted_model = WeightedLogisticRegression()\n",
    "configs = [\n",
    "    {\"learning_rate\": 0.01, \"batch_size\": 32},\n",
    "    {\"learning_rate\": 0.05, \"batch_size\": 32},\n",
    "    {\"learning_rate\": 0.01, \"batch_size\": 128},\n",
    "    {\"learning_rate\": 0.05, \"batch_size\": 128},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\nTesting SGD Iterations with LR: {config['learning_rate']}, Batch Size: {config['batch_size']}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    sgd_iter_weighted_optimizer = SGDIterationsOptimizer(\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        max_iterations=50000,  # Equivalent to roughly 50-100 epochs depending on batch size\n",
    "        early_stop_patience=200,\n",
    "        early_stop_threshold=EARLY_STOP_THRESHOLD,\n",
    "        log_interval=20,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    sgd_iter_weighted_result = sgd_iter_weighted_optimizer.optimize(\n",
    "        sgd_iter_weighted_model,\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        X_val_scaled,\n",
    "        y_val,\n",
    "        weights_train=weights_train,\n",
    "        weights_val=weights_val,\n",
    "    )\n",
    "    \n",
    "    print(f\"Final loss: {sgd_iter_weighted_result['final_loss']:.6f}\")\n",
    "    print(f\"Total iterations: {sgd_iter_weighted_result['final_iteration']}\")\n",
    "    print(f\"Approximate epochs: {sgd_iter_weighted_result['approximate_epochs']:.2f}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ab499",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40f33022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images will be saved to: ../data/output/images\n"
     ]
    }
   ],
   "source": [
    "images_dir = OUTPUT_DIR / \"images\"\n",
    "print(f\"Images will be saved to: {images_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1177f4f1",
   "metadata": {},
   "source": [
    "## SGD fixed learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61a176bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SGD training with learning rate: 0.01\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.591072 | LR: 0.0100 | Patience:  0/50 | Time: 0.60s\n",
      "Epoch   20 | Train Loss: 0.591072 | LR: 0.0100 | Patience:  0/50 | Time: 0.60s\n",
      "Epoch   40 | Train Loss: 0.538788 | LR: 0.0100 | Patience:  0/50 | Time: 1.21s\n",
      "Epoch   40 | Train Loss: 0.538788 | LR: 0.0100 | Patience:  0/50 | Time: 1.21s\n",
      "Epoch   60 | Train Loss: 0.507421 | LR: 0.0100 | Patience:  0/50 | Time: 1.79s\n",
      "Epoch   60 | Train Loss: 0.507421 | LR: 0.0100 | Patience:  0/50 | Time: 1.79s\n",
      "Epoch   80 | Train Loss: 0.486837 | LR: 0.0100 | Patience:  0/50 | Time: 2.37s\n",
      "Epoch   80 | Train Loss: 0.486837 | LR: 0.0100 | Patience:  0/50 | Time: 2.37s\n",
      "Epoch  100 | Train Loss: 0.472418 | LR: 0.0100 | Patience:  0/50 | Time: 3.02s\n",
      "Epoch  100 | Train Loss: 0.472418 | LR: 0.0100 | Patience:  0/50 | Time: 3.02s\n",
      "Epoch  120 | Train Loss: 0.461812 | LR: 0.0100 | Patience:  0/50 | Time: 3.55s\n",
      "Epoch  120 | Train Loss: 0.461812 | LR: 0.0100 | Patience:  0/50 | Time: 3.55s\n",
      "Epoch  140 | Train Loss: 0.453708 | LR: 0.0100 | Patience:  0/50 | Time: 4.07s\n",
      "Epoch  140 | Train Loss: 0.453708 | LR: 0.0100 | Patience:  0/50 | Time: 4.07s\n",
      "Epoch  160 | Train Loss: 0.447323 | LR: 0.0100 | Patience:  0/50 | Time: 4.60s\n",
      "Epoch  160 | Train Loss: 0.447323 | LR: 0.0100 | Patience:  0/50 | Time: 4.60s\n",
      "Epoch  180 | Train Loss: 0.442163 | LR: 0.0100 | Patience:  0/50 | Time: 5.13s\n",
      "Epoch  180 | Train Loss: 0.442163 | LR: 0.0100 | Patience:  0/50 | Time: 5.13s\n",
      "Epoch  200 | Train Loss: 0.437904 | LR: 0.0100 | Patience:  0/50 | Time: 5.71s\n",
      "Epoch  200 | Train Loss: 0.437904 | LR: 0.0100 | Patience:  0/50 | Time: 5.71s\n",
      "Epoch  220 | Train Loss: 0.434324 | LR: 0.0100 | Patience:  0/50 | Time: 6.24s\n",
      "Epoch  220 | Train Loss: 0.434324 | LR: 0.0100 | Patience:  0/50 | Time: 6.24s\n",
      "Epoch  240 | Train Loss: 0.431266 | LR: 0.0100 | Patience:  0/50 | Time: 6.85s\n",
      "Epoch  240 | Train Loss: 0.431266 | LR: 0.0100 | Patience:  0/50 | Time: 6.85s\n",
      "Epoch  260 | Train Loss: 0.428619 | LR: 0.0100 | Patience:  0/50 | Time: 7.41s\n",
      "Epoch  260 | Train Loss: 0.428619 | LR: 0.0100 | Patience:  0/50 | Time: 7.41s\n",
      "Epoch  280 | Train Loss: 0.426300 | LR: 0.0100 | Patience:  0/50 | Time: 7.93s\n",
      "Epoch  280 | Train Loss: 0.426300 | LR: 0.0100 | Patience:  0/50 | Time: 7.93s\n",
      "Epoch  300 | Train Loss: 0.424247 | LR: 0.0100 | Patience:  0/50 | Time: 8.64s\n",
      "Epoch  300 | Train Loss: 0.424247 | LR: 0.0100 | Patience:  0/50 | Time: 8.64s\n",
      "Epoch  320 | Train Loss: 0.422413 | LR: 0.0100 | Patience:  0/50 | Time: 9.34s\n",
      "Epoch  320 | Train Loss: 0.422413 | LR: 0.0100 | Patience:  0/50 | Time: 9.34s\n",
      "Epoch  340 | Train Loss: 0.420761 | LR: 0.0100 | Patience:  0/50 | Time: 9.96s\n",
      "Epoch  340 | Train Loss: 0.420761 | LR: 0.0100 | Patience:  0/50 | Time: 9.96s\n",
      "Epoch  360 | Train Loss: 0.419262 | LR: 0.0100 | Patience:  0/50 | Time: 10.47s\n",
      "Epoch  360 | Train Loss: 0.419262 | LR: 0.0100 | Patience:  0/50 | Time: 10.47s\n",
      "Epoch  380 | Train Loss: 0.417894 | LR: 0.0100 | Patience:  0/50 | Time: 11.01s\n",
      "Epoch  380 | Train Loss: 0.417894 | LR: 0.0100 | Patience:  0/50 | Time: 11.01s\n",
      "Epoch  400 | Train Loss: 0.416639 | LR: 0.0100 | Patience:  0/50 | Time: 11.49s\n",
      "Epoch  400 | Train Loss: 0.416639 | LR: 0.0100 | Patience:  0/50 | Time: 11.49s\n",
      "Epoch  420 | Train Loss: 0.415481 | LR: 0.0100 | Patience:  0/50 | Time: 11.98s\n",
      "Epoch  420 | Train Loss: 0.415481 | LR: 0.0100 | Patience:  0/50 | Time: 11.98s\n",
      "Epoch  440 | Train Loss: 0.414409 | LR: 0.0100 | Patience:  0/50 | Time: 12.47s\n",
      "Epoch  440 | Train Loss: 0.414409 | LR: 0.0100 | Patience:  0/50 | Time: 12.47s\n",
      "Epoch  460 | Train Loss: 0.413412 | LR: 0.0100 | Patience:  0/50 | Time: 13.04s\n",
      "Epoch  460 | Train Loss: 0.413412 | LR: 0.0100 | Patience:  0/50 | Time: 13.04s\n",
      "Epoch  480 | Train Loss: 0.412483 | LR: 0.0100 | Patience:  0/50 | Time: 13.53s\n",
      "Epoch  480 | Train Loss: 0.412483 | LR: 0.0100 | Patience:  0/50 | Time: 13.53s\n",
      "Epoch  500 | Train Loss: 0.411614 | LR: 0.0100 | Patience:  0/50 | Time: 14.02s\n",
      "Epoch  500 | Train Loss: 0.411614 | LR: 0.0100 | Patience:  0/50 | Time: 14.02s\n",
      "Epoch  520 | Train Loss: 0.410800 | LR: 0.0100 | Patience:  0/50 | Time: 14.52s\n",
      "Epoch  520 | Train Loss: 0.410800 | LR: 0.0100 | Patience:  0/50 | Time: 14.52s\n",
      "Epoch  540 | Train Loss: 0.410034 | LR: 0.0100 | Patience:  0/50 | Time: 15.02s\n",
      "Epoch  540 | Train Loss: 0.410034 | LR: 0.0100 | Patience:  0/50 | Time: 15.02s\n",
      "Epoch  560 | Train Loss: 0.409314 | LR: 0.0100 | Patience:  0/50 | Time: 15.51s\n",
      "Epoch  560 | Train Loss: 0.409314 | LR: 0.0100 | Patience:  0/50 | Time: 15.51s\n",
      "Epoch  580 | Train Loss: 0.408634 | LR: 0.0100 | Patience:  0/50 | Time: 16.01s\n",
      "Epoch  580 | Train Loss: 0.408634 | LR: 0.0100 | Patience:  0/50 | Time: 16.01s\n",
      "Epoch  600 | Train Loss: 0.407992 | LR: 0.0100 | Patience:  0/50 | Time: 16.50s\n",
      "Epoch  600 | Train Loss: 0.407992 | LR: 0.0100 | Patience:  0/50 | Time: 16.50s\n",
      "Epoch  620 | Train Loss: 0.407385 | LR: 0.0100 | Patience:  0/50 | Time: 17.02s\n",
      "Epoch  620 | Train Loss: 0.407385 | LR: 0.0100 | Patience:  0/50 | Time: 17.02s\n",
      "Epoch  640 | Train Loss: 0.406810 | LR: 0.0100 | Patience:  0/50 | Time: 17.51s\n",
      "Epoch  640 | Train Loss: 0.406810 | LR: 0.0100 | Patience:  0/50 | Time: 17.51s\n",
      "Epoch  660 | Train Loss: 0.406265 | LR: 0.0100 | Patience:  0/50 | Time: 18.13s\n",
      "Epoch  660 | Train Loss: 0.406265 | LR: 0.0100 | Patience:  0/50 | Time: 18.13s\n",
      "Epoch  680 | Train Loss: 0.405747 | LR: 0.0100 | Patience:  0/50 | Time: 18.63s\n",
      "Epoch  680 | Train Loss: 0.405747 | LR: 0.0100 | Patience:  0/50 | Time: 18.63s\n",
      "Epoch  700 | Train Loss: 0.405255 | LR: 0.0100 | Patience:  0/50 | Time: 19.13s\n",
      "Epoch  700 | Train Loss: 0.405255 | LR: 0.0100 | Patience:  0/50 | Time: 19.13s\n",
      "Epoch  720 | Train Loss: 0.404788 | LR: 0.0100 | Patience:  0/50 | Time: 19.62s\n",
      "Epoch  720 | Train Loss: 0.404788 | LR: 0.0100 | Patience:  0/50 | Time: 19.62s\n",
      "Epoch  740 | Train Loss: 0.404343 | LR: 0.0100 | Patience:  0/50 | Time: 20.13s\n",
      "Epoch  740 | Train Loss: 0.404343 | LR: 0.0100 | Patience:  0/50 | Time: 20.13s\n",
      "Epoch  760 | Train Loss: 0.403920 | LR: 0.0100 | Patience:  0/50 | Time: 20.68s\n",
      "Epoch  760 | Train Loss: 0.403920 | LR: 0.0100 | Patience:  0/50 | Time: 20.68s\n",
      "Epoch  780 | Train Loss: 0.403517 | LR: 0.0100 | Patience:  0/50 | Time: 21.20s\n",
      "Epoch  780 | Train Loss: 0.403517 | LR: 0.0100 | Patience:  0/50 | Time: 21.20s\n",
      "Epoch  800 | Train Loss: 0.403132 | LR: 0.0100 | Patience:  0/50 | Time: 21.74s\n",
      "Epoch  800 | Train Loss: 0.403132 | LR: 0.0100 | Patience:  0/50 | Time: 21.74s\n",
      "Epoch  820 | Train Loss: 0.402766 | LR: 0.0100 | Patience:  0/50 | Time: 22.28s\n",
      "Epoch  820 | Train Loss: 0.402766 | LR: 0.0100 | Patience:  0/50 | Time: 22.28s\n",
      "Epoch  840 | Train Loss: 0.402416 | LR: 0.0100 | Patience:  0/50 | Time: 22.80s\n",
      "Epoch  840 | Train Loss: 0.402416 | LR: 0.0100 | Patience:  0/50 | Time: 22.80s\n",
      "Epoch  860 | Train Loss: 0.402082 | LR: 0.0100 | Patience:  0/50 | Time: 23.31s\n",
      "Epoch  860 | Train Loss: 0.402082 | LR: 0.0100 | Patience:  0/50 | Time: 23.31s\n",
      "Epoch  880 | Train Loss: 0.401764 | LR: 0.0100 | Patience:  0/50 | Time: 23.83s\n",
      "Epoch  880 | Train Loss: 0.401764 | LR: 0.0100 | Patience:  0/50 | Time: 23.83s\n",
      "Epoch  900 | Train Loss: 0.401459 | LR: 0.0100 | Patience:  0/50 | Time: 24.35s\n",
      "Epoch  900 | Train Loss: 0.401459 | LR: 0.0100 | Patience:  0/50 | Time: 24.35s\n",
      "Epoch  920 | Train Loss: 0.401168 | LR: 0.0100 | Patience:  0/50 | Time: 24.86s\n",
      "Epoch  920 | Train Loss: 0.401168 | LR: 0.0100 | Patience:  0/50 | Time: 24.86s\n",
      "Epoch  940 | Train Loss: 0.400890 | LR: 0.0100 | Patience:  0/50 | Time: 25.39s\n",
      "Epoch  940 | Train Loss: 0.400890 | LR: 0.0100 | Patience:  0/50 | Time: 25.39s\n",
      "Epoch  960 | Train Loss: 0.400624 | LR: 0.0100 | Patience:  0/50 | Time: 25.89s\n",
      "Epoch  960 | Train Loss: 0.400624 | LR: 0.0100 | Patience:  0/50 | Time: 25.89s\n",
      "Epoch  980 | Train Loss: 0.400370 | LR: 0.0100 | Patience:  0/50 | Time: 26.37s\n",
      "Epoch  980 | Train Loss: 0.400370 | LR: 0.0100 | Patience:  0/50 | Time: 26.37s\n",
      "Epoch 1000 | Train Loss: 0.400126 | LR: 0.0100 | Patience:  0/50 | Time: 26.87s\n",
      "Epoch 1000 | Train Loss: 0.400126 | LR: 0.0100 | Patience:  0/50 | Time: 26.87s\n",
      "Epoch 1020 | Train Loss: 0.399894 | LR: 0.0100 | Patience:  0/50 | Time: 27.36s\n",
      "Epoch 1020 | Train Loss: 0.399894 | LR: 0.0100 | Patience:  0/50 | Time: 27.36s\n",
      "Epoch 1040 | Train Loss: 0.399671 | LR: 0.0100 | Patience:  0/50 | Time: 27.85s\n",
      "Epoch 1040 | Train Loss: 0.399671 | LR: 0.0100 | Patience:  0/50 | Time: 27.85s\n",
      "Epoch 1060 | Train Loss: 0.399457 | LR: 0.0100 | Patience:  0/50 | Time: 28.36s\n",
      "Epoch 1060 | Train Loss: 0.399457 | LR: 0.0100 | Patience:  0/50 | Time: 28.36s\n",
      "Epoch 1080 | Train Loss: 0.399253 | LR: 0.0100 | Patience:  0/50 | Time: 28.85s\n",
      "Epoch 1080 | Train Loss: 0.399253 | LR: 0.0100 | Patience:  0/50 | Time: 28.85s\n",
      "Epoch 1100 | Train Loss: 0.399057 | LR: 0.0100 | Patience:  0/50 | Time: 29.34s\n",
      "Epoch 1100 | Train Loss: 0.399057 | LR: 0.0100 | Patience:  0/50 | Time: 29.34s\n",
      "Epoch 1120 | Train Loss: 0.398869 | LR: 0.0100 | Patience:  0/50 | Time: 29.82s\n",
      "Epoch 1120 | Train Loss: 0.398869 | LR: 0.0100 | Patience:  0/50 | Time: 29.82s\n",
      "Epoch 1140 | Train Loss: 0.398689 | LR: 0.0100 | Patience:  0/50 | Time: 30.32s\n",
      "Epoch 1140 | Train Loss: 0.398689 | LR: 0.0100 | Patience:  0/50 | Time: 30.32s\n",
      "Epoch 1160 | Train Loss: 0.398517 | LR: 0.0100 | Patience:  0/50 | Time: 30.84s\n",
      "Epoch 1160 | Train Loss: 0.398517 | LR: 0.0100 | Patience:  0/50 | Time: 30.84s\n",
      "Epoch 1180 | Train Loss: 0.398352 | LR: 0.0100 | Patience:  0/50 | Time: 31.36s\n",
      "Epoch 1180 | Train Loss: 0.398352 | LR: 0.0100 | Patience:  0/50 | Time: 31.36s\n",
      "Epoch 1200 | Train Loss: 0.398193 | LR: 0.0100 | Patience:  0/50 | Time: 31.86s\n",
      "Epoch 1200 | Train Loss: 0.398193 | LR: 0.0100 | Patience:  0/50 | Time: 31.86s\n",
      "Epoch 1220 | Train Loss: 0.398041 | LR: 0.0100 | Patience:  0/50 | Time: 32.38s\n",
      "Epoch 1220 | Train Loss: 0.398041 | LR: 0.0100 | Patience:  0/50 | Time: 32.38s\n",
      "Epoch 1240 | Train Loss: 0.397895 | LR: 0.0100 | Patience:  0/50 | Time: 32.90s\n",
      "Epoch 1240 | Train Loss: 0.397895 | LR: 0.0100 | Patience:  0/50 | Time: 32.90s\n",
      "Epoch 1260 | Train Loss: 0.397755 | LR: 0.0100 | Patience:  0/50 | Time: 33.40s\n",
      "Epoch 1260 | Train Loss: 0.397755 | LR: 0.0100 | Patience:  0/50 | Time: 33.40s\n",
      "Epoch 1280 | Train Loss: 0.397621 | LR: 0.0100 | Patience:  0/50 | Time: 33.90s\n",
      "Epoch 1280 | Train Loss: 0.397621 | LR: 0.0100 | Patience:  0/50 | Time: 33.90s\n",
      "Epoch 1300 | Train Loss: 0.397492 | LR: 0.0100 | Patience:  0/50 | Time: 34.41s\n",
      "Epoch 1300 | Train Loss: 0.397492 | LR: 0.0100 | Patience:  0/50 | Time: 34.41s\n",
      "Epoch 1320 | Train Loss: 0.397368 | LR: 0.0100 | Patience:  0/50 | Time: 34.92s\n",
      "Epoch 1320 | Train Loss: 0.397368 | LR: 0.0100 | Patience:  0/50 | Time: 34.92s\n",
      "Epoch 1340 | Train Loss: 0.397249 | LR: 0.0100 | Patience:  0/50 | Time: 35.42s\n",
      "Epoch 1340 | Train Loss: 0.397249 | LR: 0.0100 | Patience:  0/50 | Time: 35.42s\n",
      "Epoch 1360 | Train Loss: 0.397135 | LR: 0.0100 | Patience:  0/50 | Time: 35.93s\n",
      "Epoch 1360 | Train Loss: 0.397135 | LR: 0.0100 | Patience:  0/50 | Time: 35.93s\n",
      "Epoch 1380 | Train Loss: 0.397026 | LR: 0.0100 | Patience:  0/50 | Time: 36.51s\n",
      "Epoch 1380 | Train Loss: 0.397026 | LR: 0.0100 | Patience:  0/50 | Time: 36.51s\n",
      "Epoch 1400 | Train Loss: 0.396920 | LR: 0.0100 | Patience:  0/50 | Time: 37.13s\n",
      "Epoch 1400 | Train Loss: 0.396920 | LR: 0.0100 | Patience:  0/50 | Time: 37.13s\n",
      "Epoch 1420 | Train Loss: 0.396819 | LR: 0.0100 | Patience:  0/50 | Time: 37.65s\n",
      "Epoch 1420 | Train Loss: 0.396819 | LR: 0.0100 | Patience:  0/50 | Time: 37.65s\n",
      "Epoch 1440 | Train Loss: 0.396722 | LR: 0.0100 | Patience:  0/50 | Time: 38.18s\n",
      "Epoch 1440 | Train Loss: 0.396722 | LR: 0.0100 | Patience:  0/50 | Time: 38.18s\n",
      "Epoch 1460 | Train Loss: 0.396629 | LR: 0.0100 | Patience:  0/50 | Time: 38.71s\n",
      "Epoch 1460 | Train Loss: 0.396629 | LR: 0.0100 | Patience:  0/50 | Time: 38.71s\n",
      "Epoch 1480 | Train Loss: 0.396539 | LR: 0.0100 | Patience:  0/50 | Time: 39.22s\n",
      "Epoch 1480 | Train Loss: 0.396539 | LR: 0.0100 | Patience:  0/50 | Time: 39.22s\n",
      "Epoch 1500 | Train Loss: 0.396453 | LR: 0.0100 | Patience:  0/50 | Time: 39.72s\n",
      "Epoch 1500 | Train Loss: 0.396453 | LR: 0.0100 | Patience:  0/50 | Time: 39.72s\n",
      "Epoch 1520 | Train Loss: 0.396370 | LR: 0.0100 | Patience:  0/50 | Time: 40.23s\n",
      "Epoch 1520 | Train Loss: 0.396370 | LR: 0.0100 | Patience:  0/50 | Time: 40.23s\n",
      "Epoch 1540 | Train Loss: 0.396290 | LR: 0.0100 | Patience:  0/50 | Time: 40.71s\n",
      "Epoch 1540 | Train Loss: 0.396290 | LR: 0.0100 | Patience:  0/50 | Time: 40.71s\n",
      "Epoch 1560 | Train Loss: 0.396213 | LR: 0.0100 | Patience:  0/50 | Time: 41.20s\n",
      "Epoch 1560 | Train Loss: 0.396213 | LR: 0.0100 | Patience:  0/50 | Time: 41.20s\n",
      "Epoch 1580 | Train Loss: 0.396139 | LR: 0.0100 | Patience:  0/50 | Time: 41.69s\n",
      "Epoch 1580 | Train Loss: 0.396139 | LR: 0.0100 | Patience:  0/50 | Time: 41.69s\n",
      "Epoch 1600 | Train Loss: 0.396068 | LR: 0.0100 | Patience:  0/50 | Time: 42.21s\n",
      "Epoch 1600 | Train Loss: 0.396068 | LR: 0.0100 | Patience:  0/50 | Time: 42.21s\n",
      "Epoch 1620 | Train Loss: 0.396000 | LR: 0.0100 | Patience:  0/50 | Time: 42.81s\n",
      "Epoch 1620 | Train Loss: 0.396000 | LR: 0.0100 | Patience:  0/50 | Time: 42.81s\n",
      "Epoch 1640 | Train Loss: 0.395934 | LR: 0.0100 | Patience:  0/50 | Time: 43.37s\n",
      "Epoch 1640 | Train Loss: 0.395934 | LR: 0.0100 | Patience:  0/50 | Time: 43.37s\n",
      "Epoch 1660 | Train Loss: 0.395871 | LR: 0.0100 | Patience:  0/50 | Time: 43.93s\n",
      "Epoch 1660 | Train Loss: 0.395871 | LR: 0.0100 | Patience:  0/50 | Time: 43.93s\n",
      "Epoch 1680 | Train Loss: 0.395810 | LR: 0.0100 | Patience:  0/50 | Time: 44.49s\n",
      "Epoch 1680 | Train Loss: 0.395810 | LR: 0.0100 | Patience:  0/50 | Time: 44.49s\n",
      "Epoch 1700 | Train Loss: 0.395751 | LR: 0.0100 | Patience:  0/50 | Time: 45.01s\n",
      "Epoch 1700 | Train Loss: 0.395751 | LR: 0.0100 | Patience:  0/50 | Time: 45.01s\n",
      "Epoch 1720 | Train Loss: 0.395695 | LR: 0.0100 | Patience:  0/50 | Time: 45.61s\n",
      "Epoch 1720 | Train Loss: 0.395695 | LR: 0.0100 | Patience:  0/50 | Time: 45.61s\n",
      "Epoch 1740 | Train Loss: 0.395641 | LR: 0.0100 | Patience:  0/50 | Time: 46.17s\n",
      "Epoch 1740 | Train Loss: 0.395641 | LR: 0.0100 | Patience:  0/50 | Time: 46.17s\n",
      "Epoch 1760 | Train Loss: 0.395588 | LR: 0.0100 | Patience:  0/50 | Time: 46.75s\n",
      "Epoch 1760 | Train Loss: 0.395588 | LR: 0.0100 | Patience:  0/50 | Time: 46.75s\n",
      "Epoch 1780 | Train Loss: 0.395538 | LR: 0.0100 | Patience:  0/50 | Time: 47.44s\n",
      "Epoch 1780 | Train Loss: 0.395538 | LR: 0.0100 | Patience:  0/50 | Time: 47.44s\n",
      "Epoch 1800 | Train Loss: 0.395489 | LR: 0.0100 | Patience:  0/50 | Time: 48.03s\n",
      "Epoch 1800 | Train Loss: 0.395489 | LR: 0.0100 | Patience:  0/50 | Time: 48.03s\n",
      "Epoch 1820 | Train Loss: 0.395443 | LR: 0.0100 | Patience:  0/50 | Time: 48.72s\n",
      "Epoch 1820 | Train Loss: 0.395443 | LR: 0.0100 | Patience:  0/50 | Time: 48.72s\n",
      "Epoch 1840 | Train Loss: 0.395397 | LR: 0.0100 | Patience:  0/50 | Time: 49.32s\n",
      "Epoch 1840 | Train Loss: 0.395397 | LR: 0.0100 | Patience:  0/50 | Time: 49.32s\n",
      "Epoch 1860 | Train Loss: 0.395354 | LR: 0.0100 | Patience:  0/50 | Time: 49.89s\n",
      "Epoch 1860 | Train Loss: 0.395354 | LR: 0.0100 | Patience:  0/50 | Time: 49.89s\n",
      "Epoch 1880 | Train Loss: 0.395312 | LR: 0.0100 | Patience:  0/50 | Time: 50.45s\n",
      "Epoch 1880 | Train Loss: 0.395312 | LR: 0.0100 | Patience:  0/50 | Time: 50.45s\n",
      "Epoch 1900 | Train Loss: 0.395272 | LR: 0.0100 | Patience:  0/50 | Time: 51.04s\n",
      "Epoch 1900 | Train Loss: 0.395272 | LR: 0.0100 | Patience:  0/50 | Time: 51.04s\n",
      "Epoch 1920 | Train Loss: 0.395233 | LR: 0.0100 | Patience:  0/50 | Time: 51.79s\n",
      "Epoch 1920 | Train Loss: 0.395233 | LR: 0.0100 | Patience:  0/50 | Time: 51.79s\n",
      "Epoch 1940 | Train Loss: 0.395195 | LR: 0.0100 | Patience:  0/50 | Time: 52.41s\n",
      "Epoch 1940 | Train Loss: 0.395195 | LR: 0.0100 | Patience:  0/50 | Time: 52.41s\n",
      "Epoch 1960 | Train Loss: 0.395159 | LR: 0.0100 | Patience:  0/50 | Time: 53.08s\n",
      "Epoch 1960 | Train Loss: 0.395159 | LR: 0.0100 | Patience:  0/50 | Time: 53.08s\n",
      "Epoch 1980 | Train Loss: 0.395124 | LR: 0.0100 | Patience:  0/50 | Time: 53.71s\n",
      "Epoch 1980 | Train Loss: 0.395124 | LR: 0.0100 | Patience:  0/50 | Time: 53.71s\n",
      "Epoch 2000 | Train Loss: 0.395091 | LR: 0.0100 | Patience:  0/50 | Time: 54.28s\n",
      "Epoch 2000 | Train Loss: 0.395091 | LR: 0.0100 | Patience:  0/50 | Time: 54.28s\n",
      "Epoch 2020 | Train Loss: 0.395058 | LR: 0.0100 | Patience:  0/50 | Time: 54.85s\n",
      "Epoch 2020 | Train Loss: 0.395058 | LR: 0.0100 | Patience:  0/50 | Time: 54.85s\n",
      "Epoch 2040 | Train Loss: 0.395027 | LR: 0.0100 | Patience:  0/50 | Time: 55.43s\n",
      "Epoch 2040 | Train Loss: 0.395027 | LR: 0.0100 | Patience:  0/50 | Time: 55.43s\n",
      "Epoch 2060 | Train Loss: 0.394997 | LR: 0.0100 | Patience:  0/50 | Time: 55.94s\n",
      "Epoch 2060 | Train Loss: 0.394997 | LR: 0.0100 | Patience:  0/50 | Time: 55.94s\n",
      "Epoch 2080 | Train Loss: 0.394968 | LR: 0.0100 | Patience:  0/50 | Time: 56.46s\n",
      "Epoch 2080 | Train Loss: 0.394968 | LR: 0.0100 | Patience:  0/50 | Time: 56.46s\n",
      "Epoch 2100 | Train Loss: 0.394940 | LR: 0.0100 | Patience:  0/50 | Time: 57.04s\n",
      "Epoch 2100 | Train Loss: 0.394940 | LR: 0.0100 | Patience:  0/50 | Time: 57.04s\n",
      "Epoch 2120 | Train Loss: 0.394913 | LR: 0.0100 | Patience:  0/50 | Time: 57.61s\n",
      "Epoch 2120 | Train Loss: 0.394913 | LR: 0.0100 | Patience:  0/50 | Time: 57.61s\n",
      "Epoch 2140 | Train Loss: 0.394887 | LR: 0.0100 | Patience:  0/50 | Time: 58.14s\n",
      "Epoch 2140 | Train Loss: 0.394887 | LR: 0.0100 | Patience:  0/50 | Time: 58.14s\n",
      "Epoch 2160 | Train Loss: 0.394861 | LR: 0.0100 | Patience:  0/50 | Time: 58.73s\n",
      "Epoch 2160 | Train Loss: 0.394861 | LR: 0.0100 | Patience:  0/50 | Time: 58.73s\n",
      "Epoch 2180 | Train Loss: 0.394837 | LR: 0.0100 | Patience:  0/50 | Time: 59.32s\n",
      "Epoch 2180 | Train Loss: 0.394837 | LR: 0.0100 | Patience:  0/50 | Time: 59.32s\n",
      "Epoch 2200 | Train Loss: 0.394813 | LR: 0.0100 | Patience:  0/50 | Time: 59.85s\n",
      "Epoch 2200 | Train Loss: 0.394813 | LR: 0.0100 | Patience:  0/50 | Time: 59.85s\n",
      "Epoch 2220 | Train Loss: 0.394791 | LR: 0.0100 | Patience:  0/50 | Time: 60.38s\n",
      "Epoch 2220 | Train Loss: 0.394791 | LR: 0.0100 | Patience:  0/50 | Time: 60.38s\n",
      "Epoch 2240 | Train Loss: 0.394769 | LR: 0.0100 | Patience:  0/50 | Time: 60.95s\n",
      "Epoch 2240 | Train Loss: 0.394769 | LR: 0.0100 | Patience:  0/50 | Time: 60.95s\n",
      "Epoch 2260 | Train Loss: 0.394748 | LR: 0.0100 | Patience:  0/50 | Time: 61.52s\n",
      "Epoch 2260 | Train Loss: 0.394748 | LR: 0.0100 | Patience:  0/50 | Time: 61.52s\n",
      "Epoch 2280 | Train Loss: 0.394727 | LR: 0.0100 | Patience:  0/50 | Time: 62.10s\n",
      "Epoch 2280 | Train Loss: 0.394727 | LR: 0.0100 | Patience:  0/50 | Time: 62.10s\n",
      "Epoch 2300 | Train Loss: 0.394707 | LR: 0.0100 | Patience:  1/50 | Time: 62.72s\n",
      "Epoch 2300 | Train Loss: 0.394707 | LR: 0.0100 | Patience:  1/50 | Time: 62.72s\n",
      "Epoch 2320 | Train Loss: 0.394688 | LR: 0.0100 | Patience:  1/50 | Time: 63.51s\n",
      "Epoch 2320 | Train Loss: 0.394688 | LR: 0.0100 | Patience:  1/50 | Time: 63.51s\n",
      "Epoch 2340 | Train Loss: 0.394670 | LR: 0.0100 | Patience:  1/50 | Time: 64.00s\n",
      "Epoch 2340 | Train Loss: 0.394670 | LR: 0.0100 | Patience:  1/50 | Time: 64.00s\n",
      "Epoch 2360 | Train Loss: 0.394652 | LR: 0.0100 | Patience:  1/50 | Time: 64.57s\n",
      "Epoch 2360 | Train Loss: 0.394652 | LR: 0.0100 | Patience:  1/50 | Time: 64.57s\n",
      "Epoch 2380 | Train Loss: 0.394635 | LR: 0.0100 | Patience:  1/50 | Time: 65.11s\n",
      "Epoch 2380 | Train Loss: 0.394635 | LR: 0.0100 | Patience:  1/50 | Time: 65.11s\n",
      "Epoch 2400 | Train Loss: 0.394618 | LR: 0.0100 | Patience:  1/50 | Time: 65.64s\n",
      "Epoch 2400 | Train Loss: 0.394618 | LR: 0.0100 | Patience:  1/50 | Time: 65.64s\n",
      "Epoch 2420 | Train Loss: 0.394602 | LR: 0.0100 | Patience:  1/50 | Time: 66.20s\n",
      "Epoch 2420 | Train Loss: 0.394602 | LR: 0.0100 | Patience:  1/50 | Time: 66.20s\n",
      "Epoch 2440 | Train Loss: 0.394586 | LR: 0.0100 | Patience:  1/50 | Time: 66.78s\n",
      "Epoch 2440 | Train Loss: 0.394586 | LR: 0.0100 | Patience:  1/50 | Time: 66.78s\n",
      "Epoch 2460 | Train Loss: 0.394571 | LR: 0.0100 | Patience:  1/50 | Time: 67.45s\n",
      "Epoch 2460 | Train Loss: 0.394571 | LR: 0.0100 | Patience:  1/50 | Time: 67.45s\n",
      "Epoch 2480 | Train Loss: 0.394557 | LR: 0.0100 | Patience:  1/50 | Time: 68.04s\n",
      "Epoch 2480 | Train Loss: 0.394557 | LR: 0.0100 | Patience:  1/50 | Time: 68.04s\n",
      "Epoch 2500 | Train Loss: 0.394543 | LR: 0.0100 | Patience:  1/50 | Time: 68.57s\n",
      "Epoch 2500 | Train Loss: 0.394543 | LR: 0.0100 | Patience:  1/50 | Time: 68.57s\n",
      "Epoch 2520 | Train Loss: 0.394529 | LR: 0.0100 | Patience:  1/50 | Time: 69.17s\n",
      "Epoch 2520 | Train Loss: 0.394529 | LR: 0.0100 | Patience:  1/50 | Time: 69.17s\n",
      "Epoch 2540 | Train Loss: 0.394516 | LR: 0.0100 | Patience:  1/50 | Time: 69.68s\n",
      "Epoch 2540 | Train Loss: 0.394516 | LR: 0.0100 | Patience:  1/50 | Time: 69.68s\n",
      "Epoch 2560 | Train Loss: 0.394503 | LR: 0.0100 | Patience:  1/50 | Time: 70.40s\n",
      "Epoch 2560 | Train Loss: 0.394503 | LR: 0.0100 | Patience:  1/50 | Time: 70.40s\n",
      "Epoch 2580 | Train Loss: 0.394491 | LR: 0.0100 | Patience:  1/50 | Time: 70.96s\n",
      "Epoch 2580 | Train Loss: 0.394491 | LR: 0.0100 | Patience:  1/50 | Time: 70.96s\n",
      "Epoch 2600 | Train Loss: 0.394479 | LR: 0.0100 | Patience:  1/50 | Time: 71.49s\n",
      "Epoch 2600 | Train Loss: 0.394479 | LR: 0.0100 | Patience:  1/50 | Time: 71.49s\n",
      "Epoch 2620 | Train Loss: 0.394468 | LR: 0.0100 | Patience:  1/50 | Time: 72.01s\n",
      "Epoch 2620 | Train Loss: 0.394468 | LR: 0.0100 | Patience:  1/50 | Time: 72.01s\n",
      "Epoch 2640 | Train Loss: 0.394457 | LR: 0.0100 | Patience:  1/50 | Time: 72.60s\n",
      "Epoch 2640 | Train Loss: 0.394457 | LR: 0.0100 | Patience:  1/50 | Time: 72.60s\n",
      "Epoch 2660 | Train Loss: 0.394446 | LR: 0.0100 | Patience:  1/50 | Time: 73.19s\n",
      "Epoch 2660 | Train Loss: 0.394446 | LR: 0.0100 | Patience:  1/50 | Time: 73.19s\n",
      "Epoch 2680 | Train Loss: 0.394435 | LR: 0.0100 | Patience:  1/50 | Time: 73.66s\n",
      "Epoch 2680 | Train Loss: 0.394435 | LR: 0.0100 | Patience:  1/50 | Time: 73.66s\n",
      "Epoch 2700 | Train Loss: 0.394425 | LR: 0.0100 | Patience:  0/50 | Time: 74.15s\n",
      "Epoch 2700 | Train Loss: 0.394425 | LR: 0.0100 | Patience:  0/50 | Time: 74.15s\n",
      "Epoch 2720 | Train Loss: 0.394415 | LR: 0.0100 | Patience:  2/50 | Time: 74.66s\n",
      "Epoch 2720 | Train Loss: 0.394415 | LR: 0.0100 | Patience:  2/50 | Time: 74.66s\n",
      "Epoch 2740 | Train Loss: 0.394406 | LR: 0.0100 | Patience:  1/50 | Time: 75.15s\n",
      "Epoch 2740 | Train Loss: 0.394406 | LR: 0.0100 | Patience:  1/50 | Time: 75.15s\n",
      "Epoch 2760 | Train Loss: 0.394397 | LR: 0.0100 | Patience:  0/50 | Time: 75.71s\n",
      "Epoch 2760 | Train Loss: 0.394397 | LR: 0.0100 | Patience:  0/50 | Time: 75.71s\n",
      "Epoch 2780 | Train Loss: 0.394388 | LR: 0.0100 | Patience:  2/50 | Time: 76.29s\n",
      "Epoch 2780 | Train Loss: 0.394388 | LR: 0.0100 | Patience:  2/50 | Time: 76.29s\n",
      "Epoch 2800 | Train Loss: 0.394379 | LR: 0.0100 | Patience:  1/50 | Time: 77.05s\n",
      "Epoch 2800 | Train Loss: 0.394379 | LR: 0.0100 | Patience:  1/50 | Time: 77.05s\n",
      "Epoch 2820 | Train Loss: 0.394371 | LR: 0.0100 | Patience:  0/50 | Time: 77.70s\n",
      "Epoch 2820 | Train Loss: 0.394371 | LR: 0.0100 | Patience:  0/50 | Time: 77.70s\n",
      "Epoch 2840 | Train Loss: 0.394363 | LR: 0.0100 | Patience:  2/50 | Time: 78.27s\n",
      "Epoch 2840 | Train Loss: 0.394363 | LR: 0.0100 | Patience:  2/50 | Time: 78.27s\n",
      "Epoch 2860 | Train Loss: 0.394355 | LR: 0.0100 | Patience:  1/50 | Time: 78.87s\n",
      "Epoch 2860 | Train Loss: 0.394355 | LR: 0.0100 | Patience:  1/50 | Time: 78.87s\n",
      "Epoch 2880 | Train Loss: 0.394348 | LR: 0.0100 | Patience:  0/50 | Time: 79.41s\n",
      "Epoch 2880 | Train Loss: 0.394348 | LR: 0.0100 | Patience:  0/50 | Time: 79.41s\n",
      "Epoch 2900 | Train Loss: 0.394340 | LR: 0.0100 | Patience:  2/50 | Time: 79.91s\n",
      "Epoch 2900 | Train Loss: 0.394340 | LR: 0.0100 | Patience:  2/50 | Time: 79.91s\n",
      "Epoch 2920 | Train Loss: 0.394333 | LR: 0.0100 | Patience:  1/50 | Time: 80.42s\n",
      "Epoch 2920 | Train Loss: 0.394333 | LR: 0.0100 | Patience:  1/50 | Time: 80.42s\n",
      "Epoch 2940 | Train Loss: 0.394326 | LR: 0.0100 | Patience:  0/50 | Time: 80.91s\n",
      "Epoch 2940 | Train Loss: 0.394326 | LR: 0.0100 | Patience:  0/50 | Time: 80.91s\n",
      "Epoch 2960 | Train Loss: 0.394320 | LR: 0.0100 | Patience:  0/50 | Time: 81.39s\n",
      "Epoch 2960 | Train Loss: 0.394320 | LR: 0.0100 | Patience:  0/50 | Time: 81.39s\n",
      "Epoch 2980 | Train Loss: 0.394313 | LR: 0.0100 | Patience:  0/50 | Time: 81.96s\n",
      "Epoch 2980 | Train Loss: 0.394313 | LR: 0.0100 | Patience:  0/50 | Time: 81.96s\n",
      "Epoch 3000 | Train Loss: 0.394307 | LR: 0.0100 | Patience:  0/50 | Time: 82.49s\n",
      "Epoch 3000 | Train Loss: 0.394307 | LR: 0.0100 | Patience:  0/50 | Time: 82.49s\n",
      "Epoch 3020 | Train Loss: 0.394301 | LR: 0.0100 | Patience:  0/50 | Time: 83.01s\n",
      "Epoch 3020 | Train Loss: 0.394301 | LR: 0.0100 | Patience:  0/50 | Time: 83.01s\n",
      "Epoch 3040 | Train Loss: 0.394295 | LR: 0.0100 | Patience:  0/50 | Time: 83.53s\n",
      "Epoch 3040 | Train Loss: 0.394295 | LR: 0.0100 | Patience:  0/50 | Time: 83.53s\n",
      "Epoch 3060 | Train Loss: 0.394289 | LR: 0.0100 | Patience:  0/50 | Time: 84.06s\n",
      "Epoch 3060 | Train Loss: 0.394289 | LR: 0.0100 | Patience:  0/50 | Time: 84.06s\n",
      "Epoch 3080 | Train Loss: 0.394284 | LR: 0.0100 | Patience:  0/50 | Time: 84.60s\n",
      "Epoch 3080 | Train Loss: 0.394284 | LR: 0.0100 | Patience:  0/50 | Time: 84.60s\n",
      "Epoch 3100 | Train Loss: 0.394278 | LR: 0.0100 | Patience:  0/50 | Time: 85.11s\n",
      "Epoch 3100 | Train Loss: 0.394278 | LR: 0.0100 | Patience:  0/50 | Time: 85.11s\n",
      "Epoch 3120 | Train Loss: 0.394273 | LR: 0.0100 | Patience:  0/50 | Time: 85.74s\n",
      "Epoch 3120 | Train Loss: 0.394273 | LR: 0.0100 | Patience:  0/50 | Time: 85.74s\n",
      "Epoch 3140 | Train Loss: 0.394268 | LR: 0.0100 | Patience:  0/50 | Time: 86.25s\n",
      "Epoch 3140 | Train Loss: 0.394268 | LR: 0.0100 | Patience:  0/50 | Time: 86.25s\n",
      "Epoch 3160 | Train Loss: 0.394263 | LR: 0.0100 | Patience:  0/50 | Time: 87.04s\n",
      "Epoch 3160 | Train Loss: 0.394263 | LR: 0.0100 | Patience:  0/50 | Time: 87.04s\n",
      "Epoch 3180 | Train Loss: 0.394258 | LR: 0.0100 | Patience:  0/50 | Time: 87.58s\n",
      "Epoch 3180 | Train Loss: 0.394258 | LR: 0.0100 | Patience:  0/50 | Time: 87.58s\n",
      "Epoch 3200 | Train Loss: 0.394253 | LR: 0.0100 | Patience:  0/50 | Time: 88.10s\n",
      "Epoch 3200 | Train Loss: 0.394253 | LR: 0.0100 | Patience:  0/50 | Time: 88.10s\n",
      "Epoch 3220 | Train Loss: 0.394249 | LR: 0.0100 | Patience:  0/50 | Time: 88.70s\n",
      "Epoch 3220 | Train Loss: 0.394249 | LR: 0.0100 | Patience:  0/50 | Time: 88.70s\n",
      "Epoch 3240 | Train Loss: 0.394245 | LR: 0.0100 | Patience:  0/50 | Time: 89.22s\n",
      "Epoch 3240 | Train Loss: 0.394245 | LR: 0.0100 | Patience:  0/50 | Time: 89.22s\n",
      "Epoch 3260 | Train Loss: 0.394240 | LR: 0.0100 | Patience:  0/50 | Time: 89.78s\n",
      "Epoch 3260 | Train Loss: 0.394240 | LR: 0.0100 | Patience:  0/50 | Time: 89.78s\n",
      "Epoch 3280 | Train Loss: 0.394236 | LR: 0.0100 | Patience:  0/50 | Time: 90.37s\n",
      "Epoch 3280 | Train Loss: 0.394236 | LR: 0.0100 | Patience:  0/50 | Time: 90.37s\n",
      "Epoch 3300 | Train Loss: 0.394232 | LR: 0.0100 | Patience:  5/50 | Time: 90.92s\n",
      "Epoch 3300 | Train Loss: 0.394232 | LR: 0.0100 | Patience:  5/50 | Time: 90.92s\n",
      "Epoch 3320 | Train Loss: 0.394228 | LR: 0.0100 | Patience:  1/50 | Time: 91.45s\n",
      "Epoch 3320 | Train Loss: 0.394228 | LR: 0.0100 | Patience:  1/50 | Time: 91.45s\n",
      "Epoch 3340 | Train Loss: 0.394224 | LR: 0.0100 | Patience:  3/50 | Time: 92.09s\n",
      "Epoch 3340 | Train Loss: 0.394224 | LR: 0.0100 | Patience:  3/50 | Time: 92.09s\n",
      "Epoch 3360 | Train Loss: 0.394221 | LR: 0.0100 | Patience:  5/50 | Time: 92.66s\n",
      "Epoch 3360 | Train Loss: 0.394221 | LR: 0.0100 | Patience:  5/50 | Time: 92.66s\n",
      "Epoch 3380 | Train Loss: 0.394217 | LR: 0.0100 | Patience:  1/50 | Time: 93.22s\n",
      "Epoch 3380 | Train Loss: 0.394217 | LR: 0.0100 | Patience:  1/50 | Time: 93.22s\n",
      "Epoch 3400 | Train Loss: 0.394214 | LR: 0.0100 | Patience:  3/50 | Time: 93.74s\n",
      "Epoch 3400 | Train Loss: 0.394214 | LR: 0.0100 | Patience:  3/50 | Time: 93.74s\n",
      "Epoch 3420 | Train Loss: 0.394210 | LR: 0.0100 | Patience:  5/50 | Time: 94.30s\n",
      "Epoch 3420 | Train Loss: 0.394210 | LR: 0.0100 | Patience:  5/50 | Time: 94.30s\n",
      "Epoch 3440 | Train Loss: 0.394207 | LR: 0.0100 | Patience:  5/50 | Time: 94.85s\n",
      "Epoch 3440 | Train Loss: 0.394207 | LR: 0.0100 | Patience:  5/50 | Time: 94.85s\n",
      "Epoch 3460 | Train Loss: 0.394204 | LR: 0.0100 | Patience:  4/50 | Time: 95.46s\n",
      "Epoch 3460 | Train Loss: 0.394204 | LR: 0.0100 | Patience:  4/50 | Time: 95.46s\n",
      "Epoch 3480 | Train Loss: 0.394201 | LR: 0.0100 | Patience:  3/50 | Time: 95.99s\n",
      "Epoch 3480 | Train Loss: 0.394201 | LR: 0.0100 | Patience:  3/50 | Time: 95.99s\n",
      "Epoch 3500 | Train Loss: 0.394198 | LR: 0.0100 | Patience:  2/50 | Time: 96.64s\n",
      "Epoch 3500 | Train Loss: 0.394198 | LR: 0.0100 | Patience:  2/50 | Time: 96.64s\n",
      "Epoch 3520 | Train Loss: 0.394195 | LR: 0.0100 | Patience:  1/50 | Time: 97.20s\n",
      "Epoch 3520 | Train Loss: 0.394195 | LR: 0.0100 | Patience:  1/50 | Time: 97.20s\n",
      "Epoch 3540 | Train Loss: 0.394192 | LR: 0.0100 | Patience:  7/50 | Time: 97.77s\n",
      "Epoch 3540 | Train Loss: 0.394192 | LR: 0.0100 | Patience:  7/50 | Time: 97.77s\n",
      "Epoch 3560 | Train Loss: 0.394189 | LR: 0.0100 | Patience:  3/50 | Time: 98.27s\n",
      "Epoch 3560 | Train Loss: 0.394189 | LR: 0.0100 | Patience:  3/50 | Time: 98.27s\n",
      "Epoch 3580 | Train Loss: 0.394186 | LR: 0.0100 | Patience:  7/50 | Time: 98.77s\n",
      "Epoch 3580 | Train Loss: 0.394186 | LR: 0.0100 | Patience:  7/50 | Time: 98.77s\n",
      "Epoch 3600 | Train Loss: 0.394184 | LR: 0.0100 | Patience:  3/50 | Time: 99.27s\n",
      "Epoch 3600 | Train Loss: 0.394184 | LR: 0.0100 | Patience:  3/50 | Time: 99.27s\n",
      "Epoch 3620 | Train Loss: 0.394181 | LR: 0.0100 | Patience:  7/50 | Time: 99.79s\n",
      "Epoch 3620 | Train Loss: 0.394181 | LR: 0.0100 | Patience:  7/50 | Time: 99.79s\n",
      "Epoch 3640 | Train Loss: 0.394179 | LR: 0.0100 | Patience:  3/50 | Time: 100.31s\n",
      "Epoch 3640 | Train Loss: 0.394179 | LR: 0.0100 | Patience:  3/50 | Time: 100.31s\n",
      "Epoch 3660 | Train Loss: 0.394176 | LR: 0.0100 | Patience:  5/50 | Time: 100.82s\n",
      "Epoch 3660 | Train Loss: 0.394176 | LR: 0.0100 | Patience:  5/50 | Time: 100.82s\n",
      "Epoch 3680 | Train Loss: 0.394174 | LR: 0.0100 | Patience:  7/50 | Time: 101.36s\n",
      "Epoch 3680 | Train Loss: 0.394174 | LR: 0.0100 | Patience:  7/50 | Time: 101.36s\n",
      "Epoch 3700 | Train Loss: 0.394171 | LR: 0.0100 | Patience:  0/50 | Time: 101.94s\n",
      "Epoch 3700 | Train Loss: 0.394171 | LR: 0.0100 | Patience:  0/50 | Time: 101.94s\n",
      "Epoch 3720 | Train Loss: 0.394169 | LR: 0.0100 | Patience:  2/50 | Time: 102.56s\n",
      "Epoch 3720 | Train Loss: 0.394169 | LR: 0.0100 | Patience:  2/50 | Time: 102.56s\n",
      "Epoch 3740 | Train Loss: 0.394167 | LR: 0.0100 | Patience:  3/50 | Time: 103.08s\n",
      "Epoch 3740 | Train Loss: 0.394167 | LR: 0.0100 | Patience:  3/50 | Time: 103.08s\n",
      "Epoch 3760 | Train Loss: 0.394165 | LR: 0.0100 | Patience:  3/50 | Time: 103.64s\n",
      "Epoch 3760 | Train Loss: 0.394165 | LR: 0.0100 | Patience:  3/50 | Time: 103.64s\n",
      "Epoch 3780 | Train Loss: 0.394163 | LR: 0.0100 | Patience:  3/50 | Time: 104.20s\n",
      "Epoch 3780 | Train Loss: 0.394163 | LR: 0.0100 | Patience:  3/50 | Time: 104.20s\n",
      "Epoch 3800 | Train Loss: 0.394161 | LR: 0.0100 | Patience:  3/50 | Time: 104.88s\n",
      "Epoch 3800 | Train Loss: 0.394161 | LR: 0.0100 | Patience:  3/50 | Time: 104.88s\n",
      "Epoch 3820 | Train Loss: 0.394159 | LR: 0.0100 | Patience:  2/50 | Time: 105.45s\n",
      "Epoch 3820 | Train Loss: 0.394159 | LR: 0.0100 | Patience:  2/50 | Time: 105.45s\n",
      "Epoch 3840 | Train Loss: 0.394157 | LR: 0.0100 | Patience:  0/50 | Time: 105.97s\n",
      "Epoch 3840 | Train Loss: 0.394157 | LR: 0.0100 | Patience:  0/50 | Time: 105.97s\n",
      "Epoch 3860 | Train Loss: 0.394155 | LR: 0.0100 | Patience:  9/50 | Time: 106.60s\n",
      "Epoch 3860 | Train Loss: 0.394155 | LR: 0.0100 | Patience:  9/50 | Time: 106.60s\n",
      "Epoch 3880 | Train Loss: 0.394153 | LR: 0.0100 | Patience:  7/50 | Time: 107.21s\n",
      "Epoch 3880 | Train Loss: 0.394153 | LR: 0.0100 | Patience:  7/50 | Time: 107.21s\n",
      "Epoch 3900 | Train Loss: 0.394151 | LR: 0.0100 | Patience:  5/50 | Time: 107.82s\n",
      "Epoch 3900 | Train Loss: 0.394151 | LR: 0.0100 | Patience:  5/50 | Time: 107.82s\n",
      "Epoch 3920 | Train Loss: 0.394149 | LR: 0.0100 | Patience:  1/50 | Time: 108.41s\n",
      "Epoch 3920 | Train Loss: 0.394149 | LR: 0.0100 | Patience:  1/50 | Time: 108.41s\n",
      "Epoch 3940 | Train Loss: 0.394148 | LR: 0.0100 | Patience:  9/50 | Time: 109.04s\n",
      "Epoch 3940 | Train Loss: 0.394148 | LR: 0.0100 | Patience:  9/50 | Time: 109.04s\n",
      "Epoch 3960 | Train Loss: 0.394146 | LR: 0.0100 | Patience:  5/50 | Time: 109.68s\n",
      "Epoch 3960 | Train Loss: 0.394146 | LR: 0.0100 | Patience:  5/50 | Time: 109.68s\n",
      "Epoch 3980 | Train Loss: 0.394144 | LR: 0.0100 | Patience:  0/50 | Time: 110.23s\n",
      "Epoch 3980 | Train Loss: 0.394144 | LR: 0.0100 | Patience:  0/50 | Time: 110.23s\n",
      "Epoch 4000 | Train Loss: 0.394143 | LR: 0.0100 | Patience:  7/50 | Time: 110.78s\n",
      "Epoch 4000 | Train Loss: 0.394143 | LR: 0.0100 | Patience:  7/50 | Time: 110.78s\n",
      "Epoch 4020 | Train Loss: 0.394141 | LR: 0.0100 | Patience:  1/50 | Time: 111.53s\n",
      "Epoch 4020 | Train Loss: 0.394141 | LR: 0.0100 | Patience:  1/50 | Time: 111.53s\n",
      "Epoch 4040 | Train Loss: 0.394139 | LR: 0.0100 | Patience:  8/50 | Time: 112.13s\n",
      "Epoch 4040 | Train Loss: 0.394139 | LR: 0.0100 | Patience:  8/50 | Time: 112.13s\n",
      "Epoch 4060 | Train Loss: 0.394138 | LR: 0.0100 | Patience:  0/50 | Time: 112.71s\n",
      "Epoch 4060 | Train Loss: 0.394138 | LR: 0.0100 | Patience:  0/50 | Time: 112.71s\n",
      "Epoch 4080 | Train Loss: 0.394136 | LR: 0.0100 | Patience:  6/50 | Time: 113.27s\n",
      "Epoch 4080 | Train Loss: 0.394136 | LR: 0.0100 | Patience:  6/50 | Time: 113.27s\n",
      "Epoch 4100 | Train Loss: 0.394135 | LR: 0.0100 | Patience: 12/50 | Time: 113.90s\n",
      "Epoch 4100 | Train Loss: 0.394135 | LR: 0.0100 | Patience: 12/50 | Time: 113.90s\n",
      "Epoch 4120 | Train Loss: 0.394134 | LR: 0.0100 | Patience:  3/50 | Time: 114.43s\n",
      "Epoch 4120 | Train Loss: 0.394134 | LR: 0.0100 | Patience:  3/50 | Time: 114.43s\n",
      "Epoch 4140 | Train Loss: 0.394132 | LR: 0.0100 | Patience:  8/50 | Time: 115.08s\n",
      "Epoch 4140 | Train Loss: 0.394132 | LR: 0.0100 | Patience:  8/50 | Time: 115.08s\n",
      "Epoch 4160 | Train Loss: 0.394131 | LR: 0.0100 | Patience: 13/50 | Time: 115.64s\n",
      "Epoch 4160 | Train Loss: 0.394131 | LR: 0.0100 | Patience: 13/50 | Time: 115.64s\n",
      "Epoch 4180 | Train Loss: 0.394130 | LR: 0.0100 | Patience:  2/50 | Time: 116.20s\n",
      "Epoch 4180 | Train Loss: 0.394130 | LR: 0.0100 | Patience:  2/50 | Time: 116.20s\n",
      "Epoch 4200 | Train Loss: 0.394128 | LR: 0.0100 | Patience:  6/50 | Time: 116.74s\n",
      "Epoch 4200 | Train Loss: 0.394128 | LR: 0.0100 | Patience:  6/50 | Time: 116.74s\n",
      "Epoch 4220 | Train Loss: 0.394127 | LR: 0.0100 | Patience: 10/50 | Time: 117.26s\n",
      "Epoch 4220 | Train Loss: 0.394127 | LR: 0.0100 | Patience: 10/50 | Time: 117.26s\n",
      "Epoch 4240 | Train Loss: 0.394126 | LR: 0.0100 | Patience: 14/50 | Time: 117.89s\n",
      "Epoch 4240 | Train Loss: 0.394126 | LR: 0.0100 | Patience: 14/50 | Time: 117.89s\n",
      "Epoch 4260 | Train Loss: 0.394124 | LR: 0.0100 | Patience:  0/50 | Time: 118.44s\n",
      "Epoch 4260 | Train Loss: 0.394124 | LR: 0.0100 | Patience:  0/50 | Time: 118.44s\n",
      "Epoch 4280 | Train Loss: 0.394123 | LR: 0.0100 | Patience:  3/50 | Time: 118.99s\n",
      "Epoch 4280 | Train Loss: 0.394123 | LR: 0.0100 | Patience:  3/50 | Time: 118.99s\n",
      "Epoch 4300 | Train Loss: 0.394122 | LR: 0.0100 | Patience:  6/50 | Time: 119.52s\n",
      "Epoch 4300 | Train Loss: 0.394122 | LR: 0.0100 | Patience:  6/50 | Time: 119.52s\n",
      "Epoch 4320 | Train Loss: 0.394121 | LR: 0.0100 | Patience:  8/50 | Time: 120.04s\n",
      "Epoch 4320 | Train Loss: 0.394121 | LR: 0.0100 | Patience:  8/50 | Time: 120.04s\n",
      "Epoch 4340 | Train Loss: 0.394120 | LR: 0.0100 | Patience: 10/50 | Time: 120.53s\n",
      "Epoch 4340 | Train Loss: 0.394120 | LR: 0.0100 | Patience: 10/50 | Time: 120.53s\n",
      "Epoch 4360 | Train Loss: 0.394119 | LR: 0.0100 | Patience: 12/50 | Time: 121.09s\n",
      "Epoch 4360 | Train Loss: 0.394119 | LR: 0.0100 | Patience: 12/50 | Time: 121.09s\n",
      "Epoch 4380 | Train Loss: 0.394118 | LR: 0.0100 | Patience: 13/50 | Time: 121.71s\n",
      "Epoch 4380 | Train Loss: 0.394118 | LR: 0.0100 | Patience: 13/50 | Time: 121.71s\n",
      "Epoch 4400 | Train Loss: 0.394117 | LR: 0.0100 | Patience: 14/50 | Time: 122.41s\n",
      "Epoch 4400 | Train Loss: 0.394117 | LR: 0.0100 | Patience: 14/50 | Time: 122.41s\n",
      "Epoch 4420 | Train Loss: 0.394116 | LR: 0.0100 | Patience: 15/50 | Time: 123.05s\n",
      "Epoch 4420 | Train Loss: 0.394116 | LR: 0.0100 | Patience: 15/50 | Time: 123.05s\n",
      "Epoch 4440 | Train Loss: 0.394114 | LR: 0.0100 | Patience: 15/50 | Time: 123.65s\n",
      "Epoch 4440 | Train Loss: 0.394114 | LR: 0.0100 | Patience: 15/50 | Time: 123.65s\n",
      "Epoch 4460 | Train Loss: 0.394113 | LR: 0.0100 | Patience: 15/50 | Time: 124.17s\n",
      "Epoch 4460 | Train Loss: 0.394113 | LR: 0.0100 | Patience: 15/50 | Time: 124.17s\n",
      "Epoch 4480 | Train Loss: 0.394112 | LR: 0.0100 | Patience: 15/50 | Time: 124.67s\n",
      "Epoch 4480 | Train Loss: 0.394112 | LR: 0.0100 | Patience: 15/50 | Time: 124.67s\n",
      "Epoch 4500 | Train Loss: 0.394112 | LR: 0.0100 | Patience: 14/50 | Time: 125.22s\n",
      "Epoch 4500 | Train Loss: 0.394112 | LR: 0.0100 | Patience: 14/50 | Time: 125.22s\n",
      "Epoch 4520 | Train Loss: 0.394111 | LR: 0.0100 | Patience: 13/50 | Time: 125.78s\n",
      "Epoch 4520 | Train Loss: 0.394111 | LR: 0.0100 | Patience: 13/50 | Time: 125.78s\n",
      "Epoch 4540 | Train Loss: 0.394110 | LR: 0.0100 | Patience: 11/50 | Time: 126.39s\n",
      "Epoch 4540 | Train Loss: 0.394110 | LR: 0.0100 | Patience: 11/50 | Time: 126.39s\n",
      "Epoch 4560 | Train Loss: 0.394109 | LR: 0.0100 | Patience:  9/50 | Time: 127.12s\n",
      "Epoch 4560 | Train Loss: 0.394109 | LR: 0.0100 | Patience:  9/50 | Time: 127.12s\n",
      "Epoch 4580 | Train Loss: 0.394108 | LR: 0.0100 | Patience:  7/50 | Time: 127.69s\n",
      "Epoch 4580 | Train Loss: 0.394108 | LR: 0.0100 | Patience:  7/50 | Time: 127.69s\n",
      "Epoch 4600 | Train Loss: 0.394107 | LR: 0.0100 | Patience:  4/50 | Time: 128.30s\n",
      "Epoch 4600 | Train Loss: 0.394107 | LR: 0.0100 | Patience:  4/50 | Time: 128.30s\n",
      "Epoch 4620 | Train Loss: 0.394106 | LR: 0.0100 | Patience:  1/50 | Time: 128.85s\n",
      "Epoch 4620 | Train Loss: 0.394106 | LR: 0.0100 | Patience:  1/50 | Time: 128.85s\n",
      "Epoch 4640 | Train Loss: 0.394105 | LR: 0.0100 | Patience: 21/50 | Time: 129.39s\n",
      "Epoch 4640 | Train Loss: 0.394105 | LR: 0.0100 | Patience: 21/50 | Time: 129.39s\n",
      "Epoch 4660 | Train Loss: 0.394104 | LR: 0.0100 | Patience: 17/50 | Time: 129.96s\n",
      "Epoch 4660 | Train Loss: 0.394104 | LR: 0.0100 | Patience: 17/50 | Time: 129.96s\n",
      "Epoch 4680 | Train Loss: 0.394103 | LR: 0.0100 | Patience: 13/50 | Time: 130.49s\n",
      "Epoch 4680 | Train Loss: 0.394103 | LR: 0.0100 | Patience: 13/50 | Time: 130.49s\n",
      "Epoch 4700 | Train Loss: 0.394103 | LR: 0.0100 | Patience:  8/50 | Time: 131.01s\n",
      "Epoch 4700 | Train Loss: 0.394103 | LR: 0.0100 | Patience:  8/50 | Time: 131.01s\n",
      "Epoch 4720 | Train Loss: 0.394102 | LR: 0.0100 | Patience:  3/50 | Time: 131.54s\n",
      "Epoch 4720 | Train Loss: 0.394102 | LR: 0.0100 | Patience:  3/50 | Time: 131.54s\n",
      "Epoch 4740 | Train Loss: 0.394101 | LR: 0.0100 | Patience: 23/50 | Time: 132.05s\n",
      "Epoch 4740 | Train Loss: 0.394101 | LR: 0.0100 | Patience: 23/50 | Time: 132.05s\n",
      "Epoch 4760 | Train Loss: 0.394100 | LR: 0.0100 | Patience: 17/50 | Time: 132.61s\n",
      "Epoch 4760 | Train Loss: 0.394100 | LR: 0.0100 | Patience: 17/50 | Time: 132.61s\n",
      "Epoch 4780 | Train Loss: 0.394099 | LR: 0.0100 | Patience: 11/50 | Time: 133.14s\n",
      "Epoch 4780 | Train Loss: 0.394099 | LR: 0.0100 | Patience: 11/50 | Time: 133.14s\n",
      "Epoch 4800 | Train Loss: 0.394099 | LR: 0.0100 | Patience:  4/50 | Time: 133.64s\n",
      "Epoch 4800 | Train Loss: 0.394099 | LR: 0.0100 | Patience:  4/50 | Time: 133.64s\n",
      "Epoch 4820 | Train Loss: 0.394098 | LR: 0.0100 | Patience: 24/50 | Time: 134.15s\n",
      "Epoch 4820 | Train Loss: 0.394098 | LR: 0.0100 | Patience: 24/50 | Time: 134.15s\n",
      "Epoch 4840 | Train Loss: 0.394097 | LR: 0.0100 | Patience: 17/50 | Time: 134.65s\n",
      "Epoch 4840 | Train Loss: 0.394097 | LR: 0.0100 | Patience: 17/50 | Time: 134.65s\n",
      "Epoch 4860 | Train Loss: 0.394096 | LR: 0.0100 | Patience:  9/50 | Time: 135.18s\n",
      "Epoch 4860 | Train Loss: 0.394096 | LR: 0.0100 | Patience:  9/50 | Time: 135.18s\n",
      "Epoch 4880 | Train Loss: 0.394096 | LR: 0.0100 | Patience:  1/50 | Time: 135.68s\n",
      "Epoch 4880 | Train Loss: 0.394096 | LR: 0.0100 | Patience:  1/50 | Time: 135.68s\n",
      "Epoch 4900 | Train Loss: 0.394095 | LR: 0.0100 | Patience: 21/50 | Time: 136.18s\n",
      "Epoch 4900 | Train Loss: 0.394095 | LR: 0.0100 | Patience: 21/50 | Time: 136.18s\n",
      "Epoch 4920 | Train Loss: 0.394094 | LR: 0.0100 | Patience: 12/50 | Time: 136.70s\n",
      "Epoch 4920 | Train Loss: 0.394094 | LR: 0.0100 | Patience: 12/50 | Time: 136.70s\n",
      "Epoch 4940 | Train Loss: 0.394094 | LR: 0.0100 | Patience:  3/50 | Time: 137.24s\n",
      "Epoch 4940 | Train Loss: 0.394094 | LR: 0.0100 | Patience:  3/50 | Time: 137.24s\n",
      "Epoch 4960 | Train Loss: 0.394093 | LR: 0.0100 | Patience: 23/50 | Time: 137.88s\n",
      "Epoch 4960 | Train Loss: 0.394093 | LR: 0.0100 | Patience: 23/50 | Time: 137.88s\n",
      "Epoch 4980 | Train Loss: 0.394092 | LR: 0.0100 | Patience: 13/50 | Time: 138.41s\n",
      "Epoch 4980 | Train Loss: 0.394092 | LR: 0.0100 | Patience: 13/50 | Time: 138.41s\n",
      "Epoch 5000 | Train Loss: 0.394092 | LR: 0.0100 | Patience:  3/50 | Time: 138.90s\n",
      "Epoch 5000 | Train Loss: 0.394092 | LR: 0.0100 | Patience:  3/50 | Time: 138.90s\n",
      "Epoch 5020 | Train Loss: 0.394091 | LR: 0.0100 | Patience: 23/50 | Time: 139.41s\n",
      "Epoch 5020 | Train Loss: 0.394091 | LR: 0.0100 | Patience: 23/50 | Time: 139.41s\n",
      "Epoch 5040 | Train Loss: 0.394090 | LR: 0.0100 | Patience: 12/50 | Time: 139.90s\n",
      "Epoch 5040 | Train Loss: 0.394090 | LR: 0.0100 | Patience: 12/50 | Time: 139.90s\n",
      "Epoch 5060 | Train Loss: 0.394090 | LR: 0.0100 | Patience:  0/50 | Time: 140.42s\n",
      "Epoch 5060 | Train Loss: 0.394090 | LR: 0.0100 | Patience:  0/50 | Time: 140.42s\n",
      "Epoch 5080 | Train Loss: 0.394089 | LR: 0.0100 | Patience: 20/50 | Time: 140.91s\n",
      "Epoch 5080 | Train Loss: 0.394089 | LR: 0.0100 | Patience: 20/50 | Time: 140.91s\n",
      "Epoch 5100 | Train Loss: 0.394088 | LR: 0.0100 | Patience:  8/50 | Time: 141.50s\n",
      "Epoch 5100 | Train Loss: 0.394088 | LR: 0.0100 | Patience:  8/50 | Time: 141.50s\n",
      "Epoch 5120 | Train Loss: 0.394088 | LR: 0.0100 | Patience: 28/50 | Time: 142.03s\n",
      "Epoch 5120 | Train Loss: 0.394088 | LR: 0.0100 | Patience: 28/50 | Time: 142.03s\n",
      "Epoch 5140 | Train Loss: 0.394087 | LR: 0.0100 | Patience: 15/50 | Time: 142.53s\n",
      "Epoch 5140 | Train Loss: 0.394087 | LR: 0.0100 | Patience: 15/50 | Time: 142.53s\n",
      "Epoch 5160 | Train Loss: 0.394087 | LR: 0.0100 | Patience:  1/50 | Time: 143.02s\n",
      "Epoch 5160 | Train Loss: 0.394087 | LR: 0.0100 | Patience:  1/50 | Time: 143.02s\n",
      "Epoch 5180 | Train Loss: 0.394086 | LR: 0.0100 | Patience: 21/50 | Time: 143.52s\n",
      "Epoch 5180 | Train Loss: 0.394086 | LR: 0.0100 | Patience: 21/50 | Time: 143.52s\n",
      "Epoch 5200 | Train Loss: 0.394085 | LR: 0.0100 | Patience:  7/50 | Time: 144.01s\n",
      "Epoch 5200 | Train Loss: 0.394085 | LR: 0.0100 | Patience:  7/50 | Time: 144.01s\n",
      "Epoch 5220 | Train Loss: 0.394085 | LR: 0.0100 | Patience: 27/50 | Time: 144.52s\n",
      "Epoch 5220 | Train Loss: 0.394085 | LR: 0.0100 | Patience: 27/50 | Time: 144.52s\n",
      "Epoch 5240 | Train Loss: 0.394084 | LR: 0.0100 | Patience: 12/50 | Time: 145.02s\n",
      "Epoch 5240 | Train Loss: 0.394084 | LR: 0.0100 | Patience: 12/50 | Time: 145.02s\n",
      "Epoch 5260 | Train Loss: 0.394084 | LR: 0.0100 | Patience: 32/50 | Time: 145.52s\n",
      "Epoch 5260 | Train Loss: 0.394084 | LR: 0.0100 | Patience: 32/50 | Time: 145.52s\n",
      "Epoch 5280 | Train Loss: 0.394083 | LR: 0.0100 | Patience: 16/50 | Time: 146.05s\n",
      "Epoch 5280 | Train Loss: 0.394083 | LR: 0.0100 | Patience: 16/50 | Time: 146.05s\n",
      "Epoch 5300 | Train Loss: 0.394083 | LR: 0.0100 | Patience:  0/50 | Time: 146.56s\n",
      "Epoch 5300 | Train Loss: 0.394083 | LR: 0.0100 | Patience:  0/50 | Time: 146.56s\n",
      "Epoch 5320 | Train Loss: 0.394082 | LR: 0.0100 | Patience: 20/50 | Time: 147.05s\n",
      "Epoch 5320 | Train Loss: 0.394082 | LR: 0.0100 | Patience: 20/50 | Time: 147.05s\n",
      "Epoch 5340 | Train Loss: 0.394081 | LR: 0.0100 | Patience:  3/50 | Time: 147.54s\n",
      "Epoch 5340 | Train Loss: 0.394081 | LR: 0.0100 | Patience:  3/50 | Time: 147.54s\n",
      "Epoch 5360 | Train Loss: 0.394081 | LR: 0.0100 | Patience: 23/50 | Time: 148.03s\n",
      "Epoch 5360 | Train Loss: 0.394081 | LR: 0.0100 | Patience: 23/50 | Time: 148.03s\n",
      "Epoch 5380 | Train Loss: 0.394080 | LR: 0.0100 | Patience:  5/50 | Time: 148.55s\n",
      "Epoch 5380 | Train Loss: 0.394080 | LR: 0.0100 | Patience:  5/50 | Time: 148.55s\n",
      "Epoch 5400 | Train Loss: 0.394080 | LR: 0.0100 | Patience: 25/50 | Time: 149.06s\n",
      "Epoch 5400 | Train Loss: 0.394080 | LR: 0.0100 | Patience: 25/50 | Time: 149.06s\n",
      "Epoch 5420 | Train Loss: 0.394079 | LR: 0.0100 | Patience:  7/50 | Time: 149.57s\n",
      "Epoch 5420 | Train Loss: 0.394079 | LR: 0.0100 | Patience:  7/50 | Time: 149.57s\n",
      "Epoch 5440 | Train Loss: 0.394079 | LR: 0.0100 | Patience: 27/50 | Time: 150.09s\n",
      "Epoch 5440 | Train Loss: 0.394079 | LR: 0.0100 | Patience: 27/50 | Time: 150.09s\n",
      "Epoch 5460 | Train Loss: 0.394078 | LR: 0.0100 | Patience:  8/50 | Time: 150.66s\n",
      "Epoch 5460 | Train Loss: 0.394078 | LR: 0.0100 | Patience:  8/50 | Time: 150.66s\n",
      "Epoch 5480 | Train Loss: 0.394078 | LR: 0.0100 | Patience: 28/50 | Time: 151.19s\n",
      "Epoch 5480 | Train Loss: 0.394078 | LR: 0.0100 | Patience: 28/50 | Time: 151.19s\n",
      "Epoch 5500 | Train Loss: 0.394077 | LR: 0.0100 | Patience:  8/50 | Time: 151.68s\n",
      "Epoch 5500 | Train Loss: 0.394077 | LR: 0.0100 | Patience:  8/50 | Time: 151.68s\n",
      "Epoch 5520 | Train Loss: 0.394077 | LR: 0.0100 | Patience: 28/50 | Time: 152.18s\n",
      "Epoch 5520 | Train Loss: 0.394077 | LR: 0.0100 | Patience: 28/50 | Time: 152.18s\n",
      "Epoch 5540 | Train Loss: 0.394076 | LR: 0.0100 | Patience:  7/50 | Time: 152.68s\n",
      "Epoch 5540 | Train Loss: 0.394076 | LR: 0.0100 | Patience:  7/50 | Time: 152.68s\n",
      "Epoch 5560 | Train Loss: 0.394076 | LR: 0.0100 | Patience: 27/50 | Time: 153.17s\n",
      "Epoch 5560 | Train Loss: 0.394076 | LR: 0.0100 | Patience: 27/50 | Time: 153.17s\n",
      "Epoch 5580 | Train Loss: 0.394075 | LR: 0.0100 | Patience:  6/50 | Time: 153.67s\n",
      "Epoch 5580 | Train Loss: 0.394075 | LR: 0.0100 | Patience:  6/50 | Time: 153.67s\n",
      "Epoch 5600 | Train Loss: 0.394075 | LR: 0.0100 | Patience: 26/50 | Time: 154.16s\n",
      "Epoch 5600 | Train Loss: 0.394075 | LR: 0.0100 | Patience: 26/50 | Time: 154.16s\n",
      "Epoch 5620 | Train Loss: 0.394074 | LR: 0.0100 | Patience:  4/50 | Time: 154.67s\n",
      "Epoch 5620 | Train Loss: 0.394074 | LR: 0.0100 | Patience:  4/50 | Time: 154.67s\n",
      "Epoch 5640 | Train Loss: 0.394074 | LR: 0.0100 | Patience: 24/50 | Time: 155.20s\n",
      "Epoch 5640 | Train Loss: 0.394074 | LR: 0.0100 | Patience: 24/50 | Time: 155.20s\n",
      "Epoch 5660 | Train Loss: 0.394073 | LR: 0.0100 | Patience:  1/50 | Time: 155.72s\n",
      "Epoch 5660 | Train Loss: 0.394073 | LR: 0.0100 | Patience:  1/50 | Time: 155.72s\n",
      "Epoch 5680 | Train Loss: 0.394073 | LR: 0.0100 | Patience: 21/50 | Time: 156.21s\n",
      "Epoch 5680 | Train Loss: 0.394073 | LR: 0.0100 | Patience: 21/50 | Time: 156.21s\n",
      "Epoch 5700 | Train Loss: 0.394073 | LR: 0.0100 | Patience: 41/50 | Time: 156.88s\n",
      "Epoch 5700 | Train Loss: 0.394073 | LR: 0.0100 | Patience: 41/50 | Time: 156.88s\n",
      "Epoch 5720 | Train Loss: 0.394072 | LR: 0.0100 | Patience: 17/50 | Time: 157.37s\n",
      "Epoch 5720 | Train Loss: 0.394072 | LR: 0.0100 | Patience: 17/50 | Time: 157.37s\n",
      "Epoch 5740 | Train Loss: 0.394072 | LR: 0.0100 | Patience: 37/50 | Time: 157.90s\n",
      "Epoch 5740 | Train Loss: 0.394072 | LR: 0.0100 | Patience: 37/50 | Time: 157.90s\n",
      "Epoch 5760 | Train Loss: 0.394071 | LR: 0.0100 | Patience: 13/50 | Time: 158.44s\n",
      "Epoch 5760 | Train Loss: 0.394071 | LR: 0.0100 | Patience: 13/50 | Time: 158.44s\n",
      "Epoch 5780 | Train Loss: 0.394071 | LR: 0.0100 | Patience: 33/50 | Time: 158.97s\n",
      "Epoch 5780 | Train Loss: 0.394071 | LR: 0.0100 | Patience: 33/50 | Time: 158.97s\n",
      "Epoch 5800 | Train Loss: 0.394070 | LR: 0.0100 | Patience:  8/50 | Time: 159.50s\n",
      "Epoch 5800 | Train Loss: 0.394070 | LR: 0.0100 | Patience:  8/50 | Time: 159.50s\n",
      "Epoch 5820 | Train Loss: 0.394070 | LR: 0.0100 | Patience: 28/50 | Time: 160.04s\n",
      "Epoch 5820 | Train Loss: 0.394070 | LR: 0.0100 | Patience: 28/50 | Time: 160.04s\n",
      "Epoch 5840 | Train Loss: 0.394069 | LR: 0.0100 | Patience:  2/50 | Time: 160.60s\n",
      "Epoch 5840 | Train Loss: 0.394069 | LR: 0.0100 | Patience:  2/50 | Time: 160.60s\n",
      "Epoch 5860 | Train Loss: 0.394069 | LR: 0.0100 | Patience: 22/50 | Time: 161.12s\n",
      "Epoch 5860 | Train Loss: 0.394069 | LR: 0.0100 | Patience: 22/50 | Time: 161.12s\n",
      "Epoch 5880 | Train Loss: 0.394069 | LR: 0.0100 | Patience: 42/50 | Time: 161.67s\n",
      "Epoch 5880 | Train Loss: 0.394069 | LR: 0.0100 | Patience: 42/50 | Time: 161.67s\n",
      "Epoch 5900 | Train Loss: 0.394068 | LR: 0.0100 | Patience: 15/50 | Time: 162.21s\n",
      "Epoch 5900 | Train Loss: 0.394068 | LR: 0.0100 | Patience: 15/50 | Time: 162.21s\n",
      "Epoch 5920 | Train Loss: 0.394068 | LR: 0.0100 | Patience: 35/50 | Time: 162.75s\n",
      "Epoch 5920 | Train Loss: 0.394068 | LR: 0.0100 | Patience: 35/50 | Time: 162.75s\n",
      "Epoch 5940 | Train Loss: 0.394067 | LR: 0.0100 | Patience:  7/50 | Time: 163.26s\n",
      "Epoch 5940 | Train Loss: 0.394067 | LR: 0.0100 | Patience:  7/50 | Time: 163.26s\n",
      "Epoch 5960 | Train Loss: 0.394067 | LR: 0.0100 | Patience: 27/50 | Time: 163.77s\n",
      "Epoch 5960 | Train Loss: 0.394067 | LR: 0.0100 | Patience: 27/50 | Time: 163.77s\n",
      "Epoch 5980 | Train Loss: 0.394066 | LR: 0.0100 | Patience: 47/50 | Time: 164.27s\n",
      "Epoch 5980 | Train Loss: 0.394066 | LR: 0.0100 | Patience: 47/50 | Time: 164.27s\n",
      "Epoch 6000 | Train Loss: 0.394066 | LR: 0.0100 | Patience: 18/50 | Time: 164.79s\n",
      "Epoch 6000 | Train Loss: 0.394066 | LR: 0.0100 | Patience: 18/50 | Time: 164.79s\n",
      "Epoch 6020 | Train Loss: 0.394066 | LR: 0.0100 | Patience: 38/50 | Time: 165.31s\n",
      "Epoch 6020 | Train Loss: 0.394066 | LR: 0.0100 | Patience: 38/50 | Time: 165.31s\n",
      "Epoch 6040 | Train Loss: 0.394065 | LR: 0.0100 | Patience:  9/50 | Time: 165.83s\n",
      "Epoch 6040 | Train Loss: 0.394065 | LR: 0.0100 | Patience:  9/50 | Time: 165.83s\n",
      "Epoch 6060 | Train Loss: 0.394065 | LR: 0.0100 | Patience: 29/50 | Time: 166.33s\n",
      "Epoch 6060 | Train Loss: 0.394065 | LR: 0.0100 | Patience: 29/50 | Time: 166.33s\n",
      "Epoch 6080 | Train Loss: 0.394064 | LR: 0.0100 | Patience: 49/50 | Time: 166.85s\n",
      "Epoch 6080 | Train Loss: 0.394064 | LR: 0.0100 | Patience: 49/50 | Time: 166.85s\n",
      "Epoch 6100 | Train Loss: 0.394064 | LR: 0.0100 | Patience: 19/50 | Time: 167.35s\n",
      "Epoch 6100 | Train Loss: 0.394064 | LR: 0.0100 | Patience: 19/50 | Time: 167.35s\n",
      "Epoch 6120 | Train Loss: 0.394064 | LR: 0.0100 | Patience: 39/50 | Time: 167.87s\n",
      "Epoch 6120 | Train Loss: 0.394064 | LR: 0.0100 | Patience: 39/50 | Time: 167.87s\n",
      "\n",
      "Early stopping triggered at epoch 6131\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.394063\n",
      "Total training time: 168.14s\n",
      "Average time per epoch: 0.0274s\n",
      "Starting SGD training with learning rate: 0.1\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Early stopping triggered at epoch 6131\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.394063\n",
      "Total training time: 168.14s\n",
      "Average time per epoch: 0.0274s\n",
      "Starting SGD training with learning rate: 0.1\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.437259 | LR: 0.1000 | Patience:  0/50 | Time: 0.49s\n",
      "Epoch   20 | Train Loss: 0.437259 | LR: 0.1000 | Patience:  0/50 | Time: 0.49s\n",
      "Epoch   40 | Train Loss: 0.416077 | LR: 0.1000 | Patience:  0/50 | Time: 0.98s\n",
      "Epoch   40 | Train Loss: 0.416077 | LR: 0.1000 | Patience:  0/50 | Time: 0.98s\n",
      "Epoch   60 | Train Loss: 0.407629 | LR: 0.1000 | Patience:  0/50 | Time: 1.48s\n",
      "Epoch   60 | Train Loss: 0.407629 | LR: 0.1000 | Patience:  0/50 | Time: 1.48s\n",
      "Epoch   80 | Train Loss: 0.402895 | LR: 0.1000 | Patience:  0/50 | Time: 1.98s\n",
      "Epoch   80 | Train Loss: 0.402895 | LR: 0.1000 | Patience:  0/50 | Time: 1.98s\n",
      "Epoch  100 | Train Loss: 0.399970 | LR: 0.1000 | Patience:  0/50 | Time: 2.49s\n",
      "Epoch  100 | Train Loss: 0.399970 | LR: 0.1000 | Patience:  0/50 | Time: 2.49s\n",
      "Epoch  120 | Train Loss: 0.398089 | LR: 0.1000 | Patience:  0/50 | Time: 2.99s\n",
      "Epoch  120 | Train Loss: 0.398089 | LR: 0.1000 | Patience:  0/50 | Time: 2.99s\n",
      "Epoch  140 | Train Loss: 0.396850 | LR: 0.1000 | Patience:  0/50 | Time: 3.49s\n",
      "Epoch  140 | Train Loss: 0.396850 | LR: 0.1000 | Patience:  0/50 | Time: 3.49s\n",
      "Epoch  160 | Train Loss: 0.396020 | LR: 0.1000 | Patience:  0/50 | Time: 4.00s\n",
      "Epoch  160 | Train Loss: 0.396020 | LR: 0.1000 | Patience:  0/50 | Time: 4.00s\n",
      "Epoch  180 | Train Loss: 0.395456 | LR: 0.1000 | Patience:  0/50 | Time: 4.50s\n",
      "Epoch  180 | Train Loss: 0.395456 | LR: 0.1000 | Patience:  0/50 | Time: 4.50s\n",
      "Epoch  200 | Train Loss: 0.395067 | LR: 0.1000 | Patience:  0/50 | Time: 5.02s\n",
      "Epoch  200 | Train Loss: 0.395067 | LR: 0.1000 | Patience:  0/50 | Time: 5.02s\n",
      "Epoch  220 | Train Loss: 0.394796 | LR: 0.1000 | Patience:  0/50 | Time: 5.53s\n",
      "Epoch  220 | Train Loss: 0.394796 | LR: 0.1000 | Patience:  0/50 | Time: 5.53s\n",
      "Epoch  240 | Train Loss: 0.394605 | LR: 0.1000 | Patience:  0/50 | Time: 6.03s\n",
      "Epoch  240 | Train Loss: 0.394605 | LR: 0.1000 | Patience:  0/50 | Time: 6.03s\n",
      "Epoch  260 | Train Loss: 0.394469 | LR: 0.1000 | Patience:  0/50 | Time: 6.53s\n",
      "Epoch  260 | Train Loss: 0.394469 | LR: 0.1000 | Patience:  0/50 | Time: 6.53s\n",
      "Epoch  280 | Train Loss: 0.394371 | LR: 0.1000 | Patience:  0/50 | Time: 7.02s\n",
      "Epoch  280 | Train Loss: 0.394371 | LR: 0.1000 | Patience:  0/50 | Time: 7.02s\n",
      "Epoch  300 | Train Loss: 0.394299 | LR: 0.1000 | Patience:  0/50 | Time: 7.54s\n",
      "Epoch  300 | Train Loss: 0.394299 | LR: 0.1000 | Patience:  0/50 | Time: 7.54s\n",
      "Epoch  320 | Train Loss: 0.394247 | LR: 0.1000 | Patience:  0/50 | Time: 8.03s\n",
      "Epoch  320 | Train Loss: 0.394247 | LR: 0.1000 | Patience:  0/50 | Time: 8.03s\n",
      "Epoch  340 | Train Loss: 0.394208 | LR: 0.1000 | Patience:  0/50 | Time: 8.55s\n",
      "Epoch  340 | Train Loss: 0.394208 | LR: 0.1000 | Patience:  0/50 | Time: 8.55s\n",
      "Epoch  360 | Train Loss: 0.394178 | LR: 0.1000 | Patience:  0/50 | Time: 9.06s\n",
      "Epoch  360 | Train Loss: 0.394178 | LR: 0.1000 | Patience:  0/50 | Time: 9.06s\n",
      "Epoch  380 | Train Loss: 0.394155 | LR: 0.1000 | Patience:  0/50 | Time: 9.57s\n",
      "Epoch  380 | Train Loss: 0.394155 | LR: 0.1000 | Patience:  0/50 | Time: 9.57s\n",
      "Epoch  400 | Train Loss: 0.394137 | LR: 0.1000 | Patience:  1/50 | Time: 10.07s\n",
      "Epoch  400 | Train Loss: 0.394137 | LR: 0.1000 | Patience:  1/50 | Time: 10.07s\n",
      "Epoch  420 | Train Loss: 0.394123 | LR: 0.1000 | Patience:  1/50 | Time: 10.57s\n",
      "Epoch  420 | Train Loss: 0.394123 | LR: 0.1000 | Patience:  1/50 | Time: 10.57s\n",
      "Epoch  440 | Train Loss: 0.394111 | LR: 0.1000 | Patience:  1/50 | Time: 11.07s\n",
      "Epoch  440 | Train Loss: 0.394111 | LR: 0.1000 | Patience:  1/50 | Time: 11.07s\n",
      "Epoch  460 | Train Loss: 0.394101 | LR: 0.1000 | Patience:  1/50 | Time: 11.58s\n",
      "Epoch  460 | Train Loss: 0.394101 | LR: 0.1000 | Patience:  1/50 | Time: 11.58s\n",
      "Epoch  480 | Train Loss: 0.394093 | LR: 0.1000 | Patience:  0/50 | Time: 12.08s\n",
      "Epoch  480 | Train Loss: 0.394093 | LR: 0.1000 | Patience:  0/50 | Time: 12.08s\n",
      "Epoch  500 | Train Loss: 0.394086 | LR: 0.1000 | Patience:  2/50 | Time: 12.59s\n",
      "Epoch  500 | Train Loss: 0.394086 | LR: 0.1000 | Patience:  2/50 | Time: 12.59s\n",
      "Epoch  520 | Train Loss: 0.394080 | LR: 0.1000 | Patience:  2/50 | Time: 13.12s\n",
      "Epoch  520 | Train Loss: 0.394080 | LR: 0.1000 | Patience:  2/50 | Time: 13.12s\n",
      "Epoch  540 | Train Loss: 0.394074 | LR: 0.1000 | Patience:  2/50 | Time: 13.61s\n",
      "Epoch  540 | Train Loss: 0.394074 | LR: 0.1000 | Patience:  2/50 | Time: 13.61s\n",
      "Epoch  560 | Train Loss: 0.394069 | LR: 0.1000 | Patience:  0/50 | Time: 14.12s\n",
      "Epoch  560 | Train Loss: 0.394069 | LR: 0.1000 | Patience:  0/50 | Time: 14.12s\n",
      "Epoch  580 | Train Loss: 0.394065 | LR: 0.1000 | Patience:  0/50 | Time: 14.61s\n",
      "Epoch  580 | Train Loss: 0.394065 | LR: 0.1000 | Patience:  0/50 | Time: 14.61s\n",
      "Epoch  600 | Train Loss: 0.394061 | LR: 0.1000 | Patience:  0/50 | Time: 15.12s\n",
      "Epoch  600 | Train Loss: 0.394061 | LR: 0.1000 | Patience:  0/50 | Time: 15.12s\n",
      "Epoch  620 | Train Loss: 0.394057 | LR: 0.1000 | Patience:  3/50 | Time: 15.60s\n",
      "Epoch  620 | Train Loss: 0.394057 | LR: 0.1000 | Patience:  3/50 | Time: 15.60s\n",
      "Epoch  640 | Train Loss: 0.394053 | LR: 0.1000 | Patience:  5/50 | Time: 16.17s\n",
      "Epoch  640 | Train Loss: 0.394053 | LR: 0.1000 | Patience:  5/50 | Time: 16.17s\n",
      "Epoch  660 | Train Loss: 0.394049 | LR: 0.1000 | Patience:  1/50 | Time: 16.68s\n",
      "Epoch  660 | Train Loss: 0.394049 | LR: 0.1000 | Patience:  1/50 | Time: 16.68s\n",
      "Epoch  680 | Train Loss: 0.394046 | LR: 0.1000 | Patience:  1/50 | Time: 17.22s\n",
      "Epoch  680 | Train Loss: 0.394046 | LR: 0.1000 | Patience:  1/50 | Time: 17.22s\n",
      "Epoch  700 | Train Loss: 0.394043 | LR: 0.1000 | Patience:  0/50 | Time: 17.75s\n",
      "Epoch  700 | Train Loss: 0.394043 | LR: 0.1000 | Patience:  0/50 | Time: 17.75s\n",
      "Epoch  720 | Train Loss: 0.394040 | LR: 0.1000 | Patience:  6/50 | Time: 18.26s\n",
      "Epoch  720 | Train Loss: 0.394040 | LR: 0.1000 | Patience:  6/50 | Time: 18.26s\n",
      "Epoch  740 | Train Loss: 0.394037 | LR: 0.1000 | Patience:  4/50 | Time: 18.76s\n",
      "Epoch  740 | Train Loss: 0.394037 | LR: 0.1000 | Patience:  4/50 | Time: 18.76s\n",
      "Epoch  760 | Train Loss: 0.394034 | LR: 0.1000 | Patience:  0/50 | Time: 19.28s\n",
      "Epoch  760 | Train Loss: 0.394034 | LR: 0.1000 | Patience:  0/50 | Time: 19.28s\n",
      "Epoch  780 | Train Loss: 0.394032 | LR: 0.1000 | Patience:  4/50 | Time: 19.79s\n",
      "Epoch  780 | Train Loss: 0.394032 | LR: 0.1000 | Patience:  4/50 | Time: 19.79s\n",
      "Epoch  800 | Train Loss: 0.394029 | LR: 0.1000 | Patience:  7/50 | Time: 20.33s\n",
      "Epoch  800 | Train Loss: 0.394029 | LR: 0.1000 | Patience:  7/50 | Time: 20.33s\n",
      "Epoch  820 | Train Loss: 0.394027 | LR: 0.1000 | Patience:  0/50 | Time: 20.88s\n",
      "Epoch  820 | Train Loss: 0.394027 | LR: 0.1000 | Patience:  0/50 | Time: 20.88s\n",
      "Epoch  840 | Train Loss: 0.394025 | LR: 0.1000 | Patience:  2/50 | Time: 21.39s\n",
      "Epoch  840 | Train Loss: 0.394025 | LR: 0.1000 | Patience:  2/50 | Time: 21.39s\n",
      "Epoch  860 | Train Loss: 0.394023 | LR: 0.1000 | Patience:  2/50 | Time: 21.90s\n",
      "Epoch  860 | Train Loss: 0.394023 | LR: 0.1000 | Patience:  2/50 | Time: 21.90s\n",
      "Epoch  880 | Train Loss: 0.394021 | LR: 0.1000 | Patience:  2/50 | Time: 22.42s\n",
      "Epoch  880 | Train Loss: 0.394021 | LR: 0.1000 | Patience:  2/50 | Time: 22.42s\n",
      "Epoch  900 | Train Loss: 0.394019 | LR: 0.1000 | Patience:  1/50 | Time: 22.94s\n",
      "Epoch  900 | Train Loss: 0.394019 | LR: 0.1000 | Patience:  1/50 | Time: 22.94s\n",
      "Epoch  920 | Train Loss: 0.394017 | LR: 0.1000 | Patience: 10/50 | Time: 23.47s\n",
      "Epoch  920 | Train Loss: 0.394017 | LR: 0.1000 | Patience: 10/50 | Time: 23.47s\n",
      "Epoch  940 | Train Loss: 0.394015 | LR: 0.1000 | Patience:  8/50 | Time: 23.98s\n",
      "Epoch  940 | Train Loss: 0.394015 | LR: 0.1000 | Patience:  8/50 | Time: 23.98s\n",
      "Epoch  960 | Train Loss: 0.394013 | LR: 0.1000 | Patience:  5/50 | Time: 24.51s\n",
      "Epoch  960 | Train Loss: 0.394013 | LR: 0.1000 | Patience:  5/50 | Time: 24.51s\n",
      "Epoch  980 | Train Loss: 0.394011 | LR: 0.1000 | Patience:  1/50 | Time: 25.07s\n",
      "Epoch  980 | Train Loss: 0.394011 | LR: 0.1000 | Patience:  1/50 | Time: 25.07s\n",
      "Epoch 1000 | Train Loss: 0.394010 | LR: 0.1000 | Patience:  8/50 | Time: 25.58s\n",
      "Epoch 1000 | Train Loss: 0.394010 | LR: 0.1000 | Patience:  8/50 | Time: 25.58s\n",
      "Epoch 1020 | Train Loss: 0.394008 | LR: 0.1000 | Patience:  2/50 | Time: 26.10s\n",
      "Epoch 1020 | Train Loss: 0.394008 | LR: 0.1000 | Patience:  2/50 | Time: 26.10s\n",
      "Epoch 1040 | Train Loss: 0.394007 | LR: 0.1000 | Patience:  8/50 | Time: 26.61s\n",
      "Epoch 1040 | Train Loss: 0.394007 | LR: 0.1000 | Patience:  8/50 | Time: 26.61s\n",
      "Epoch 1060 | Train Loss: 0.394005 | LR: 0.1000 | Patience:  0/50 | Time: 27.11s\n",
      "Epoch 1060 | Train Loss: 0.394005 | LR: 0.1000 | Patience:  0/50 | Time: 27.11s\n",
      "Epoch 1080 | Train Loss: 0.394004 | LR: 0.1000 | Patience:  5/50 | Time: 27.62s\n",
      "Epoch 1080 | Train Loss: 0.394004 | LR: 0.1000 | Patience:  5/50 | Time: 27.62s\n",
      "Epoch 1100 | Train Loss: 0.394002 | LR: 0.1000 | Patience: 10/50 | Time: 28.16s\n",
      "Epoch 1100 | Train Loss: 0.394002 | LR: 0.1000 | Patience: 10/50 | Time: 28.16s\n",
      "Epoch 1120 | Train Loss: 0.394001 | LR: 0.1000 | Patience: 15/50 | Time: 28.66s\n",
      "Epoch 1120 | Train Loss: 0.394001 | LR: 0.1000 | Patience: 15/50 | Time: 28.66s\n",
      "Epoch 1140 | Train Loss: 0.394000 | LR: 0.1000 | Patience:  3/50 | Time: 29.17s\n",
      "Epoch 1140 | Train Loss: 0.394000 | LR: 0.1000 | Patience:  3/50 | Time: 29.17s\n",
      "Epoch 1160 | Train Loss: 0.393999 | LR: 0.1000 | Patience:  6/50 | Time: 29.70s\n",
      "Epoch 1160 | Train Loss: 0.393999 | LR: 0.1000 | Patience:  6/50 | Time: 29.70s\n",
      "Epoch 1180 | Train Loss: 0.393997 | LR: 0.1000 | Patience:  9/50 | Time: 30.24s\n",
      "Epoch 1180 | Train Loss: 0.393997 | LR: 0.1000 | Patience:  9/50 | Time: 30.24s\n",
      "Epoch 1200 | Train Loss: 0.393996 | LR: 0.1000 | Patience: 11/50 | Time: 30.74s\n",
      "Epoch 1200 | Train Loss: 0.393996 | LR: 0.1000 | Patience: 11/50 | Time: 30.74s\n",
      "Epoch 1220 | Train Loss: 0.393995 | LR: 0.1000 | Patience: 12/50 | Time: 31.27s\n",
      "Epoch 1220 | Train Loss: 0.393995 | LR: 0.1000 | Patience: 12/50 | Time: 31.27s\n",
      "Epoch 1240 | Train Loss: 0.393994 | LR: 0.1000 | Patience: 13/50 | Time: 31.79s\n",
      "Epoch 1240 | Train Loss: 0.393994 | LR: 0.1000 | Patience: 13/50 | Time: 31.79s\n",
      "Epoch 1260 | Train Loss: 0.393993 | LR: 0.1000 | Patience: 13/50 | Time: 32.30s\n",
      "Epoch 1260 | Train Loss: 0.393993 | LR: 0.1000 | Patience: 13/50 | Time: 32.30s\n",
      "Epoch 1280 | Train Loss: 0.393992 | LR: 0.1000 | Patience: 12/50 | Time: 32.83s\n",
      "Epoch 1280 | Train Loss: 0.393992 | LR: 0.1000 | Patience: 12/50 | Time: 32.83s\n",
      "Epoch 1300 | Train Loss: 0.393991 | LR: 0.1000 | Patience: 11/50 | Time: 33.34s\n",
      "Epoch 1300 | Train Loss: 0.393991 | LR: 0.1000 | Patience: 11/50 | Time: 33.34s\n",
      "Epoch 1320 | Train Loss: 0.393990 | LR: 0.1000 | Patience:  9/50 | Time: 33.87s\n",
      "Epoch 1320 | Train Loss: 0.393990 | LR: 0.1000 | Patience:  9/50 | Time: 33.87s\n",
      "Epoch 1340 | Train Loss: 0.393989 | LR: 0.1000 | Patience:  6/50 | Time: 34.43s\n",
      "Epoch 1340 | Train Loss: 0.393989 | LR: 0.1000 | Patience:  6/50 | Time: 34.43s\n",
      "Epoch 1360 | Train Loss: 0.393989 | LR: 0.1000 | Patience:  2/50 | Time: 34.94s\n",
      "Epoch 1360 | Train Loss: 0.393989 | LR: 0.1000 | Patience:  2/50 | Time: 34.94s\n",
      "Epoch 1380 | Train Loss: 0.393988 | LR: 0.1000 | Patience: 22/50 | Time: 35.46s\n",
      "Epoch 1380 | Train Loss: 0.393988 | LR: 0.1000 | Patience: 22/50 | Time: 35.46s\n",
      "Epoch 1400 | Train Loss: 0.393987 | LR: 0.1000 | Patience: 17/50 | Time: 36.00s\n",
      "Epoch 1400 | Train Loss: 0.393987 | LR: 0.1000 | Patience: 17/50 | Time: 36.00s\n",
      "Epoch 1420 | Train Loss: 0.393986 | LR: 0.1000 | Patience: 11/50 | Time: 36.53s\n",
      "Epoch 1420 | Train Loss: 0.393986 | LR: 0.1000 | Patience: 11/50 | Time: 36.53s\n",
      "Epoch 1440 | Train Loss: 0.393986 | LR: 0.1000 | Patience:  3/50 | Time: 37.05s\n",
      "Epoch 1440 | Train Loss: 0.393986 | LR: 0.1000 | Patience:  3/50 | Time: 37.05s\n",
      "Epoch 1460 | Train Loss: 0.393985 | LR: 0.1000 | Patience: 23/50 | Time: 37.57s\n",
      "Epoch 1460 | Train Loss: 0.393985 | LR: 0.1000 | Patience: 23/50 | Time: 37.57s\n",
      "Epoch 1480 | Train Loss: 0.393984 | LR: 0.1000 | Patience: 14/50 | Time: 38.10s\n",
      "Epoch 1480 | Train Loss: 0.393984 | LR: 0.1000 | Patience: 14/50 | Time: 38.10s\n",
      "Epoch 1500 | Train Loss: 0.393983 | LR: 0.1000 | Patience:  4/50 | Time: 38.63s\n",
      "Epoch 1500 | Train Loss: 0.393983 | LR: 0.1000 | Patience:  4/50 | Time: 38.63s\n",
      "Epoch 1520 | Train Loss: 0.393983 | LR: 0.1000 | Patience: 24/50 | Time: 39.14s\n",
      "Epoch 1520 | Train Loss: 0.393983 | LR: 0.1000 | Patience: 24/50 | Time: 39.14s\n",
      "Epoch 1540 | Train Loss: 0.393982 | LR: 0.1000 | Patience: 12/50 | Time: 39.66s\n",
      "Epoch 1540 | Train Loss: 0.393982 | LR: 0.1000 | Patience: 12/50 | Time: 39.66s\n",
      "Epoch 1560 | Train Loss: 0.393982 | LR: 0.1000 | Patience: 32/50 | Time: 40.19s\n",
      "Epoch 1560 | Train Loss: 0.393982 | LR: 0.1000 | Patience: 32/50 | Time: 40.19s\n",
      "Epoch 1580 | Train Loss: 0.393981 | LR: 0.1000 | Patience: 18/50 | Time: 40.74s\n",
      "Epoch 1580 | Train Loss: 0.393981 | LR: 0.1000 | Patience: 18/50 | Time: 40.74s\n",
      "Epoch 1600 | Train Loss: 0.393980 | LR: 0.1000 | Patience:  2/50 | Time: 41.25s\n",
      "Epoch 1600 | Train Loss: 0.393980 | LR: 0.1000 | Patience:  2/50 | Time: 41.25s\n",
      "Epoch 1620 | Train Loss: 0.393980 | LR: 0.1000 | Patience: 22/50 | Time: 41.77s\n",
      "Epoch 1620 | Train Loss: 0.393980 | LR: 0.1000 | Patience: 22/50 | Time: 41.77s\n",
      "Epoch 1640 | Train Loss: 0.393979 | LR: 0.1000 | Patience:  4/50 | Time: 42.35s\n",
      "Epoch 1640 | Train Loss: 0.393979 | LR: 0.1000 | Patience:  4/50 | Time: 42.35s\n",
      "Epoch 1660 | Train Loss: 0.393979 | LR: 0.1000 | Patience: 24/50 | Time: 42.90s\n",
      "Epoch 1660 | Train Loss: 0.393979 | LR: 0.1000 | Patience: 24/50 | Time: 42.90s\n",
      "Epoch 1680 | Train Loss: 0.393978 | LR: 0.1000 | Patience:  3/50 | Time: 43.48s\n",
      "Epoch 1680 | Train Loss: 0.393978 | LR: 0.1000 | Patience:  3/50 | Time: 43.48s\n",
      "Epoch 1700 | Train Loss: 0.393978 | LR: 0.1000 | Patience: 23/50 | Time: 44.04s\n",
      "Epoch 1700 | Train Loss: 0.393978 | LR: 0.1000 | Patience: 23/50 | Time: 44.04s\n",
      "Epoch 1720 | Train Loss: 0.393978 | LR: 0.1000 | Patience: 43/50 | Time: 44.64s\n",
      "Epoch 1720 | Train Loss: 0.393978 | LR: 0.1000 | Patience: 43/50 | Time: 44.64s\n",
      "Epoch 1740 | Train Loss: 0.393977 | LR: 0.1000 | Patience: 19/50 | Time: 45.18s\n",
      "Epoch 1740 | Train Loss: 0.393977 | LR: 0.1000 | Patience: 19/50 | Time: 45.18s\n",
      "Epoch 1760 | Train Loss: 0.393977 | LR: 0.1000 | Patience: 39/50 | Time: 45.83s\n",
      "Epoch 1760 | Train Loss: 0.393977 | LR: 0.1000 | Patience: 39/50 | Time: 45.83s\n",
      "Epoch 1780 | Train Loss: 0.393976 | LR: 0.1000 | Patience: 12/50 | Time: 46.40s\n",
      "Epoch 1780 | Train Loss: 0.393976 | LR: 0.1000 | Patience: 12/50 | Time: 46.40s\n",
      "Epoch 1800 | Train Loss: 0.393976 | LR: 0.1000 | Patience: 32/50 | Time: 47.01s\n",
      "Epoch 1800 | Train Loss: 0.393976 | LR: 0.1000 | Patience: 32/50 | Time: 47.01s\n",
      "\n",
      "Early stopping triggered at epoch 1818\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393975\n",
      "Total training time: 47.50s\n",
      "Average time per epoch: 0.0261s\n",
      "Starting SGD training with learning rate: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Early stopping triggered at epoch 1818\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393975\n",
      "Total training time: 47.50s\n",
      "Average time per epoch: 0.0261s\n",
      "Starting SGD training with learning rate: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.400138 | LR: 0.5000 | Patience:  0/50 | Time: 0.55s\n",
      "Epoch   20 | Train Loss: 0.400138 | LR: 0.5000 | Patience:  0/50 | Time: 0.55s\n",
      "Epoch   40 | Train Loss: 0.395080 | LR: 0.5000 | Patience:  0/50 | Time: 1.27s\n",
      "Epoch   40 | Train Loss: 0.395080 | LR: 0.5000 | Patience:  0/50 | Time: 1.27s\n",
      "Epoch   60 | Train Loss: 0.394311 | LR: 0.5000 | Patience:  0/50 | Time: 1.81s\n",
      "Epoch   60 | Train Loss: 0.394311 | LR: 0.5000 | Patience:  0/50 | Time: 1.81s\n",
      "Epoch   80 | Train Loss: 0.394150 | LR: 0.5000 | Patience:  0/50 | Time: 2.33s\n",
      "Epoch   80 | Train Loss: 0.394150 | LR: 0.5000 | Patience:  0/50 | Time: 2.33s\n",
      "Epoch  100 | Train Loss: 0.394098 | LR: 0.5000 | Patience:  0/50 | Time: 2.85s\n",
      "Epoch  100 | Train Loss: 0.394098 | LR: 0.5000 | Patience:  0/50 | Time: 2.85s\n",
      "Epoch  120 | Train Loss: 0.394071 | LR: 0.5000 | Patience:  0/50 | Time: 3.36s\n",
      "Epoch  120 | Train Loss: 0.394071 | LR: 0.5000 | Patience:  0/50 | Time: 3.36s\n",
      "Epoch  140 | Train Loss: 0.394052 | LR: 0.5000 | Patience:  0/50 | Time: 3.89s\n",
      "Epoch  140 | Train Loss: 0.394052 | LR: 0.5000 | Patience:  0/50 | Time: 3.89s\n",
      "Epoch  160 | Train Loss: 0.394037 | LR: 0.5000 | Patience:  0/50 | Time: 4.39s\n",
      "Epoch  160 | Train Loss: 0.394037 | LR: 0.5000 | Patience:  0/50 | Time: 4.39s\n",
      "Epoch  180 | Train Loss: 0.394025 | LR: 0.5000 | Patience:  0/50 | Time: 4.93s\n",
      "Epoch  180 | Train Loss: 0.394025 | LR: 0.5000 | Patience:  0/50 | Time: 4.93s\n",
      "Epoch  200 | Train Loss: 0.394015 | LR: 0.5000 | Patience:  0/50 | Time: 5.45s\n",
      "Epoch  200 | Train Loss: 0.394015 | LR: 0.5000 | Patience:  0/50 | Time: 5.45s\n",
      "Epoch  220 | Train Loss: 0.394007 | LR: 0.5000 | Patience:  2/50 | Time: 6.00s\n",
      "Epoch  220 | Train Loss: 0.394007 | LR: 0.5000 | Patience:  2/50 | Time: 6.00s\n",
      "Epoch  240 | Train Loss: 0.394000 | LR: 0.5000 | Patience:  2/50 | Time: 6.54s\n",
      "Epoch  240 | Train Loss: 0.394000 | LR: 0.5000 | Patience:  2/50 | Time: 6.54s\n",
      "Epoch  260 | Train Loss: 0.393995 | LR: 0.5000 | Patience:  2/50 | Time: 7.05s\n",
      "Epoch  260 | Train Loss: 0.393995 | LR: 0.5000 | Patience:  2/50 | Time: 7.05s\n",
      "Epoch  280 | Train Loss: 0.393990 | LR: 0.5000 | Patience:  3/50 | Time: 7.55s\n",
      "Epoch  280 | Train Loss: 0.393990 | LR: 0.5000 | Patience:  3/50 | Time: 7.55s\n",
      "Epoch  300 | Train Loss: 0.393986 | LR: 0.5000 | Patience:  1/50 | Time: 8.05s\n",
      "Epoch  300 | Train Loss: 0.393986 | LR: 0.5000 | Patience:  1/50 | Time: 8.05s\n",
      "Epoch  320 | Train Loss: 0.393983 | LR: 0.5000 | Patience:  2/50 | Time: 8.58s\n",
      "Epoch  320 | Train Loss: 0.393983 | LR: 0.5000 | Patience:  2/50 | Time: 8.58s\n",
      "Epoch  340 | Train Loss: 0.393980 | LR: 0.5000 | Patience:  7/50 | Time: 9.11s\n",
      "Epoch  340 | Train Loss: 0.393980 | LR: 0.5000 | Patience:  7/50 | Time: 9.11s\n",
      "Epoch  360 | Train Loss: 0.393978 | LR: 0.5000 | Patience:  1/50 | Time: 9.60s\n",
      "Epoch  360 | Train Loss: 0.393978 | LR: 0.5000 | Patience:  1/50 | Time: 9.60s\n",
      "Epoch  380 | Train Loss: 0.393976 | LR: 0.5000 | Patience:  0/50 | Time: 10.15s\n",
      "Epoch  380 | Train Loss: 0.393976 | LR: 0.5000 | Patience:  0/50 | Time: 10.15s\n",
      "Epoch  400 | Train Loss: 0.393974 | LR: 0.5000 | Patience:  8/50 | Time: 10.66s\n",
      "Epoch  400 | Train Loss: 0.393974 | LR: 0.5000 | Patience:  8/50 | Time: 10.66s\n",
      "Epoch  420 | Train Loss: 0.393972 | LR: 0.5000 | Patience:  1/50 | Time: 11.18s\n",
      "Epoch  420 | Train Loss: 0.393972 | LR: 0.5000 | Patience:  1/50 | Time: 11.18s\n",
      "Epoch  440 | Train Loss: 0.393971 | LR: 0.5000 | Patience:  5/50 | Time: 11.71s\n",
      "Epoch  440 | Train Loss: 0.393971 | LR: 0.5000 | Patience:  5/50 | Time: 11.71s\n",
      "Epoch  460 | Train Loss: 0.393970 | LR: 0.5000 | Patience:  7/50 | Time: 12.25s\n",
      "Epoch  460 | Train Loss: 0.393970 | LR: 0.5000 | Patience:  7/50 | Time: 12.25s\n",
      "Epoch  480 | Train Loss: 0.393969 | LR: 0.5000 | Patience:  6/50 | Time: 12.80s\n",
      "Epoch  480 | Train Loss: 0.393969 | LR: 0.5000 | Patience:  6/50 | Time: 12.80s\n",
      "Epoch  500 | Train Loss: 0.393968 | LR: 0.5000 | Patience:  1/50 | Time: 13.33s\n",
      "Epoch  500 | Train Loss: 0.393968 | LR: 0.5000 | Patience:  1/50 | Time: 13.33s\n",
      "Epoch  520 | Train Loss: 0.393968 | LR: 0.5000 | Patience: 21/50 | Time: 13.86s\n",
      "Epoch  520 | Train Loss: 0.393968 | LR: 0.5000 | Patience: 21/50 | Time: 13.86s\n",
      "Epoch  540 | Train Loss: 0.393967 | LR: 0.5000 | Patience: 11/50 | Time: 14.39s\n",
      "Epoch  540 | Train Loss: 0.393967 | LR: 0.5000 | Patience: 11/50 | Time: 14.39s\n",
      "Epoch  560 | Train Loss: 0.393967 | LR: 0.5000 | Patience: 31/50 | Time: 14.91s\n",
      "Epoch  560 | Train Loss: 0.393967 | LR: 0.5000 | Patience: 31/50 | Time: 14.91s\n",
      "Epoch  580 | Train Loss: 0.393966 | LR: 0.5000 | Patience: 12/50 | Time: 15.42s\n",
      "Epoch  580 | Train Loss: 0.393966 | LR: 0.5000 | Patience: 12/50 | Time: 15.42s\n",
      "Epoch  600 | Train Loss: 0.393966 | LR: 0.5000 | Patience: 32/50 | Time: 15.96s\n",
      "Epoch  600 | Train Loss: 0.393966 | LR: 0.5000 | Patience: 32/50 | Time: 15.96s\n",
      "\n",
      "Early stopping triggered at epoch 618\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393965\n",
      "Total training time: 16.43s\n",
      "Average time per epoch: 0.0266s\n",
      "\n",
      "Early stopping triggered at epoch 618\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393965\n",
      "Total training time: 16.43s\n",
      "Average time per epoch: 0.0266s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SGD Fixed Learning Rate comparison saved!\n"
     ]
    }
   ],
   "source": [
    "# SGD Fixed Learning Rate - Loss Convergence Comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sgd_configs = [\n",
    "    {\"learning_rate\": 0.01, \"color\": \"blue\", \"label\": \"LR 0.01\"},\n",
    "    {\"learning_rate\": 0.10, \"color\": \"green\", \"label\": \"LR 0.1\"},\n",
    "    {\"learning_rate\": 0.5, \"color\": \"orange\", \"label\": \"LR 0.5\"}, \n",
    "]\n",
    "\n",
    "for config in sgd_configs:\n",
    "    model = WeightedLogisticRegression()\n",
    "    optimizer = SGDOptimizer(\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        log_interval=20,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    result = optimizer.optimize(\n",
    "        model, X_train_scaled, y_train, \n",
    "        weights_train=weights_train\n",
    "    )\n",
    "    \n",
    "    plt.plot(result['epoch_history'], result['loss_history'], \n",
    "             color=config[\"color\"], linewidth=2, label=config[\"label\"])\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Training Loss', fontsize=12)\n",
    "plt.title('SGD Fixed Learning Rate - Loss Convergence', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(images_dir / \"sgd_fixed_lr_convergence.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✅ SGD Fixed Learning Rate comparison saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3809fd",
   "metadata": {},
   "source": [
    "## SGD backtracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e050250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SGD with Backtracking Line Search\n",
      "Initial step size: 0.1, c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.437816 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 0.95s\n",
      "Epoch   20 | Train Loss: 0.437816 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 0.95s\n",
      "Epoch   40 | Train Loss: 0.416375 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 1.82s\n",
      "Epoch   40 | Train Loss: 0.416375 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 1.82s\n",
      "Epoch   60 | Train Loss: 0.407811 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 2.65s\n",
      "Epoch   60 | Train Loss: 0.407811 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 2.65s\n",
      "Epoch   80 | Train Loss: 0.403010 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 3.52s\n",
      "Epoch   80 | Train Loss: 0.403010 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 3.52s\n",
      "Epoch  100 | Train Loss: 0.400044 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 4.39s\n",
      "Epoch  100 | Train Loss: 0.400044 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 4.39s\n",
      "Epoch  120 | Train Loss: 0.398138 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 5.23s\n",
      "Epoch  120 | Train Loss: 0.398138 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 5.23s\n",
      "Epoch  140 | Train Loss: 0.396885 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 6.09s\n",
      "Epoch  140 | Train Loss: 0.396885 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 6.09s\n",
      "Epoch  160 | Train Loss: 0.396046 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 7.00s\n",
      "Epoch  160 | Train Loss: 0.396046 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 7.00s\n",
      "Epoch  180 | Train Loss: 0.395476 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 7.84s\n",
      "Epoch  180 | Train Loss: 0.395476 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 7.84s\n",
      "Epoch  200 | Train Loss: 0.395084 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 8.67s\n",
      "Epoch  200 | Train Loss: 0.395084 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 8.67s\n",
      "Epoch  220 | Train Loss: 0.394811 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 9.50s\n",
      "Epoch  220 | Train Loss: 0.394811 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 9.50s\n",
      "Epoch  240 | Train Loss: 0.394619 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 10.31s\n",
      "Epoch  240 | Train Loss: 0.394619 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 10.31s\n",
      "Epoch  260 | Train Loss: 0.394482 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 11.21s\n",
      "Epoch  260 | Train Loss: 0.394482 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 11.21s\n",
      "Epoch  280 | Train Loss: 0.394384 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 11.99s\n",
      "Epoch  280 | Train Loss: 0.394384 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 11.99s\n",
      "Epoch  300 | Train Loss: 0.394312 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 12.76s\n",
      "Epoch  300 | Train Loss: 0.394312 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 12.76s\n",
      "Epoch  320 | Train Loss: 0.394259 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 13.56s\n",
      "Epoch  320 | Train Loss: 0.394259 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 13.56s\n",
      "Epoch  340 | Train Loss: 0.394220 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 14.35s\n",
      "Epoch  340 | Train Loss: 0.394220 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 14.35s\n",
      "Epoch  360 | Train Loss: 0.394190 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 15.20s\n",
      "Epoch  360 | Train Loss: 0.394190 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 15.20s\n",
      "Epoch  380 | Train Loss: 0.394166 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 15.98s\n",
      "Epoch  380 | Train Loss: 0.394166 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 15.98s\n",
      "Epoch  400 | Train Loss: 0.394148 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 16.77s\n",
      "Epoch  400 | Train Loss: 0.394148 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 16.77s\n",
      "Epoch  420 | Train Loss: 0.394134 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 17.62s\n",
      "Epoch  420 | Train Loss: 0.394134 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 17.62s\n",
      "Epoch  440 | Train Loss: 0.394122 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 18.45s\n",
      "Epoch  440 | Train Loss: 0.394122 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 18.45s\n",
      "Epoch  460 | Train Loss: 0.394112 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 19.57s\n",
      "Epoch  460 | Train Loss: 0.394112 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 19.57s\n",
      "Epoch  480 | Train Loss: 0.394104 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 20.47s\n",
      "Epoch  480 | Train Loss: 0.394104 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 20.47s\n",
      "Epoch  500 | Train Loss: 0.394096 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 21.23s\n",
      "Epoch  500 | Train Loss: 0.394096 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 21.23s\n",
      "Epoch  520 | Train Loss: 0.394090 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 21.97s\n",
      "Epoch  520 | Train Loss: 0.394090 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 21.97s\n",
      "Epoch  540 | Train Loss: 0.394084 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 22.72s\n",
      "Epoch  540 | Train Loss: 0.394084 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 22.72s\n",
      "Epoch  560 | Train Loss: 0.394079 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 23.47s\n",
      "Epoch  560 | Train Loss: 0.394079 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 23.47s\n",
      "Epoch  580 | Train Loss: 0.394074 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 24.26s\n",
      "Epoch  580 | Train Loss: 0.394074 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 24.26s\n",
      "Epoch  600 | Train Loss: 0.394069 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 25.01s\n",
      "Epoch  600 | Train Loss: 0.394069 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 25.01s\n",
      "Epoch  620 | Train Loss: 0.394065 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 25.75s\n",
      "Epoch  620 | Train Loss: 0.394065 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 25.75s\n",
      "Epoch  640 | Train Loss: 0.394061 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 26.65s\n",
      "Epoch  640 | Train Loss: 0.394061 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 26.65s\n",
      "Epoch  660 | Train Loss: 0.394057 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 27.43s\n",
      "Epoch  660 | Train Loss: 0.394057 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 27.43s\n",
      "Epoch  680 | Train Loss: 0.394054 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 28.38s\n",
      "Epoch  680 | Train Loss: 0.394054 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 28.38s\n",
      "Epoch  700 | Train Loss: 0.394051 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 29.15s\n",
      "Epoch  700 | Train Loss: 0.394051 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 29.15s\n",
      "Epoch  720 | Train Loss: 0.394047 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 30.02s\n",
      "Epoch  720 | Train Loss: 0.394047 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 30.02s\n",
      "Epoch  740 | Train Loss: 0.394044 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 30.81s\n",
      "Epoch  740 | Train Loss: 0.394044 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 30.81s\n",
      "Epoch  760 | Train Loss: 0.394041 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 31.66s\n",
      "Epoch  760 | Train Loss: 0.394041 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 31.66s\n",
      "Epoch  780 | Train Loss: 0.394039 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 32.42s\n",
      "Epoch  780 | Train Loss: 0.394039 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 32.42s\n",
      "Epoch  800 | Train Loss: 0.394036 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 33.49s\n",
      "Epoch  800 | Train Loss: 0.394036 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 33.49s\n",
      "Epoch  820 | Train Loss: 0.394033 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 34.47s\n",
      "Epoch  820 | Train Loss: 0.394033 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 34.47s\n",
      "Epoch  840 | Train Loss: 0.394031 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 35.40s\n",
      "Epoch  840 | Train Loss: 0.394031 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 35.40s\n",
      "Epoch  860 | Train Loss: 0.394029 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 36.18s\n",
      "Epoch  860 | Train Loss: 0.394029 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 36.18s\n",
      "Epoch  880 | Train Loss: 0.394026 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  7/50 | Time: 37.00s\n",
      "Epoch  880 | Train Loss: 0.394026 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  7/50 | Time: 37.00s\n",
      "Epoch  900 | Train Loss: 0.394024 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 37.99s\n",
      "Epoch  900 | Train Loss: 0.394024 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 37.99s\n",
      "Epoch  920 | Train Loss: 0.394022 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 38.83s\n",
      "Epoch  920 | Train Loss: 0.394022 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 38.83s\n",
      "Epoch  940 | Train Loss: 0.394020 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 39.60s\n",
      "Epoch  940 | Train Loss: 0.394020 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 39.60s\n",
      "Epoch  960 | Train Loss: 0.394018 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 40.40s\n",
      "Epoch  960 | Train Loss: 0.394018 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 40.40s\n",
      "Epoch  980 | Train Loss: 0.394016 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 41.17s\n",
      "Epoch  980 | Train Loss: 0.394016 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 41.17s\n",
      "Epoch 1000 | Train Loss: 0.394015 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 41.93s\n",
      "Epoch 1000 | Train Loss: 0.394015 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 41.93s\n",
      "Epoch 1020 | Train Loss: 0.394013 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 42.75s\n",
      "Epoch 1020 | Train Loss: 0.394013 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 42.75s\n",
      "Epoch 1040 | Train Loss: 0.394011 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 43.68s\n",
      "Epoch 1040 | Train Loss: 0.394011 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 43.68s\n",
      "Epoch 1060 | Train Loss: 0.394010 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 44.59s\n",
      "Epoch 1060 | Train Loss: 0.394010 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 44.59s\n",
      "Epoch 1080 | Train Loss: 0.394008 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 45.59s\n",
      "Epoch 1080 | Train Loss: 0.394008 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 45.59s\n",
      "Epoch 1100 | Train Loss: 0.394007 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 46.45s\n",
      "Epoch 1100 | Train Loss: 0.394007 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 46.45s\n",
      "Epoch 1120 | Train Loss: 0.394005 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 47.48s\n",
      "Epoch 1120 | Train Loss: 0.394005 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 47.48s\n",
      "Epoch 1140 | Train Loss: 0.394004 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 48.32s\n",
      "Epoch 1140 | Train Loss: 0.394004 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 48.32s\n",
      "Epoch 1160 | Train Loss: 0.394002 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 49.10s\n",
      "Epoch 1160 | Train Loss: 0.394002 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 49.10s\n",
      "Epoch 1180 | Train Loss: 0.394001 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 14/50 | Time: 49.86s\n",
      "Epoch 1180 | Train Loss: 0.394001 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 14/50 | Time: 49.86s\n",
      "Epoch 1200 | Train Loss: 0.394000 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 50.73s\n",
      "Epoch 1200 | Train Loss: 0.394000 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 50.73s\n",
      "Epoch 1220 | Train Loss: 0.393999 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 51.62s\n",
      "Epoch 1220 | Train Loss: 0.393999 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 51.62s\n",
      "Epoch 1240 | Train Loss: 0.393998 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 52.69s\n",
      "Epoch 1240 | Train Loss: 0.393998 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 52.69s\n",
      "Epoch 1260 | Train Loss: 0.393997 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 53.62s\n",
      "Epoch 1260 | Train Loss: 0.393997 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 53.62s\n",
      "Epoch 1280 | Train Loss: 0.393995 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 54.66s\n",
      "Epoch 1280 | Train Loss: 0.393995 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 54.66s\n",
      "Epoch 1300 | Train Loss: 0.393994 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 55.72s\n",
      "Epoch 1300 | Train Loss: 0.393994 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 55.72s\n",
      "Epoch 1320 | Train Loss: 0.393993 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 56.62s\n",
      "Epoch 1320 | Train Loss: 0.393993 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 56.62s\n",
      "Epoch 1340 | Train Loss: 0.393992 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 57.48s\n",
      "Epoch 1340 | Train Loss: 0.393992 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 57.48s\n",
      "Epoch 1360 | Train Loss: 0.393992 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 58.39s\n",
      "Epoch 1360 | Train Loss: 0.393992 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 58.39s\n",
      "Epoch 1380 | Train Loss: 0.393991 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 59.25s\n",
      "Epoch 1380 | Train Loss: 0.393991 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 59.25s\n",
      "Epoch 1400 | Train Loss: 0.393990 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 23/50 | Time: 60.13s\n",
      "Epoch 1400 | Train Loss: 0.393990 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 23/50 | Time: 60.13s\n",
      "Epoch 1420 | Train Loss: 0.393989 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 19/50 | Time: 61.00s\n",
      "Epoch 1420 | Train Loss: 0.393989 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 19/50 | Time: 61.00s\n",
      "Epoch 1440 | Train Loss: 0.393988 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 14/50 | Time: 61.77s\n",
      "Epoch 1440 | Train Loss: 0.393988 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 14/50 | Time: 61.77s\n",
      "Epoch 1460 | Train Loss: 0.393987 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 62.57s\n",
      "Epoch 1460 | Train Loss: 0.393987 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 62.57s\n",
      "Epoch 1480 | Train Loss: 0.393987 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 63.42s\n",
      "Epoch 1480 | Train Loss: 0.393987 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 63.42s\n",
      "Epoch 1500 | Train Loss: 0.393986 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 21/50 | Time: 64.20s\n",
      "Epoch 1500 | Train Loss: 0.393986 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 21/50 | Time: 64.20s\n",
      "Epoch 1520 | Train Loss: 0.393985 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 13/50 | Time: 64.97s\n",
      "Epoch 1520 | Train Loss: 0.393985 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 13/50 | Time: 64.97s\n",
      "Epoch 1540 | Train Loss: 0.393985 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 65.75s\n",
      "Epoch 1540 | Train Loss: 0.393985 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 65.75s\n",
      "Epoch 1560 | Train Loss: 0.393984 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 23/50 | Time: 66.53s\n",
      "Epoch 1560 | Train Loss: 0.393984 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 23/50 | Time: 66.53s\n",
      "Epoch 1580 | Train Loss: 0.393983 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 12/50 | Time: 67.32s\n",
      "Epoch 1580 | Train Loss: 0.393983 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 12/50 | Time: 67.32s\n",
      "Epoch 1600 | Train Loss: 0.393983 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 32/50 | Time: 68.17s\n",
      "Epoch 1600 | Train Loss: 0.393983 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 32/50 | Time: 68.17s\n",
      "Epoch 1620 | Train Loss: 0.393982 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 19/50 | Time: 68.98s\n",
      "Epoch 1620 | Train Loss: 0.393982 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 19/50 | Time: 68.98s\n",
      "Epoch 1640 | Train Loss: 0.393981 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 69.80s\n",
      "Epoch 1640 | Train Loss: 0.393981 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 69.80s\n",
      "Epoch 1660 | Train Loss: 0.393981 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 24/50 | Time: 70.62s\n",
      "Epoch 1660 | Train Loss: 0.393981 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 24/50 | Time: 70.62s\n",
      "Epoch 1680 | Train Loss: 0.393980 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  7/50 | Time: 71.50s\n",
      "Epoch 1680 | Train Loss: 0.393980 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  7/50 | Time: 71.50s\n",
      "Epoch 1700 | Train Loss: 0.393980 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 27/50 | Time: 72.42s\n",
      "Epoch 1700 | Train Loss: 0.393980 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 27/50 | Time: 72.42s\n",
      "Epoch 1720 | Train Loss: 0.393979 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 73.25s\n",
      "Epoch 1720 | Train Loss: 0.393979 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 73.25s\n",
      "Epoch 1740 | Train Loss: 0.393979 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 28/50 | Time: 74.05s\n",
      "Epoch 1740 | Train Loss: 0.393979 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 28/50 | Time: 74.05s\n",
      "Epoch 1760 | Train Loss: 0.393978 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 74.80s\n",
      "Epoch 1760 | Train Loss: 0.393978 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 74.80s\n",
      "Epoch 1780 | Train Loss: 0.393978 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 26/50 | Time: 75.55s\n",
      "Epoch 1780 | Train Loss: 0.393978 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 26/50 | Time: 75.55s\n",
      "Epoch 1800 | Train Loss: 0.393977 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 76.31s\n",
      "Epoch 1800 | Train Loss: 0.393977 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 76.31s\n",
      "Epoch 1820 | Train Loss: 0.393977 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 21/50 | Time: 77.12s\n",
      "Epoch 1820 | Train Loss: 0.393977 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 21/50 | Time: 77.12s\n",
      "Epoch 1840 | Train Loss: 0.393977 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 41/50 | Time: 77.96s\n",
      "Epoch 1840 | Train Loss: 0.393977 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 41/50 | Time: 77.96s\n",
      "Epoch 1860 | Train Loss: 0.393976 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 12/50 | Time: 78.77s\n",
      "Epoch 1860 | Train Loss: 0.393976 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 12/50 | Time: 78.77s\n",
      "Epoch 1880 | Train Loss: 0.393976 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 32/50 | Time: 79.53s\n",
      "Epoch 1880 | Train Loss: 0.393976 | Step Size: 0.100000 | Avg Step: 0.100000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 32/50 | Time: 79.53s\n",
      "\n",
      "Early stopping triggered at epoch 1898\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393976\n",
      "Total training time: 80.21s\n",
      "Average time per epoch: 0.0423s\n",
      "Average step size: 0.100000\n",
      "Final step size: 0.100000\n",
      "Average backtrack iterations: 0.00\n",
      "Starting SGD with Backtracking Line Search\n",
      "Initial step size: 1.0, c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Early stopping triggered at epoch 1898\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393976\n",
      "Total training time: 80.21s\n",
      "Average time per epoch: 0.0423s\n",
      "Average step size: 0.100000\n",
      "Final step size: 0.100000\n",
      "Average backtrack iterations: 0.00\n",
      "Starting SGD with Backtracking Line Search\n",
      "Initial step size: 1.0, c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.395073 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 0.74s\n",
      "Epoch   20 | Train Loss: 0.395073 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 0.74s\n",
      "Epoch   40 | Train Loss: 0.394152 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 1.47s\n",
      "Epoch   40 | Train Loss: 0.394152 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 1.47s\n",
      "Epoch   60 | Train Loss: 0.394071 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 2.30s\n",
      "Epoch   60 | Train Loss: 0.394071 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 2.30s\n",
      "Epoch   80 | Train Loss: 0.394035 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 3.06s\n",
      "Epoch   80 | Train Loss: 0.394035 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 3.06s\n",
      "Epoch  100 | Train Loss: 0.394013 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 3.87s\n",
      "Epoch  100 | Train Loss: 0.394013 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 3.87s\n",
      "Epoch  120 | Train Loss: 0.393998 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 4.69s\n",
      "Epoch  120 | Train Loss: 0.393998 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 4.69s\n",
      "Epoch  140 | Train Loss: 0.393988 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 5.45s\n",
      "Epoch  140 | Train Loss: 0.393988 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 5.45s\n",
      "Epoch  160 | Train Loss: 0.393981 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 6.29s\n",
      "Epoch  160 | Train Loss: 0.393981 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 6.29s\n",
      "Epoch  180 | Train Loss: 0.393976 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 7.12s\n",
      "Epoch  180 | Train Loss: 0.393976 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 7.12s\n",
      "Epoch  200 | Train Loss: 0.393973 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 8.01s\n",
      "Epoch  200 | Train Loss: 0.393973 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 8.01s\n",
      "Epoch  220 | Train Loss: 0.393970 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 8.79s\n",
      "Epoch  220 | Train Loss: 0.393970 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 8.79s\n",
      "Epoch  240 | Train Loss: 0.393968 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 9.71s\n",
      "Epoch  240 | Train Loss: 0.393968 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 9.71s\n",
      "Epoch  260 | Train Loss: 0.393967 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 10.45s\n",
      "Epoch  260 | Train Loss: 0.393967 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 10.45s\n",
      "Epoch  280 | Train Loss: 0.393966 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 20/50 | Time: 11.23s\n",
      "Epoch  280 | Train Loss: 0.393966 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 20/50 | Time: 11.23s\n",
      "Epoch  300 | Train Loss: 0.393965 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 18/50 | Time: 12.01s\n",
      "Epoch  300 | Train Loss: 0.393965 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 18/50 | Time: 12.01s\n",
      "Epoch  320 | Train Loss: 0.393965 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 12.76s\n",
      "Epoch  320 | Train Loss: 0.393965 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 12.76s\n",
      "Epoch  340 | Train Loss: 0.393964 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 26/50 | Time: 13.55s\n",
      "Epoch  340 | Train Loss: 0.393964 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 26/50 | Time: 13.55s\n",
      "Epoch  360 | Train Loss: 0.393964 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 46/50 | Time: 14.32s\n",
      "\n",
      "Early stopping triggered at epoch 364\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393964\n",
      "Total training time: 14.46s\n",
      "Average time per epoch: 0.0397s\n",
      "Average step size: 1.000000\n",
      "Final step size: 1.000000\n",
      "Average backtrack iterations: 0.00\n",
      "Starting SGD with Backtracking Line Search\n",
      "Initial step size: 5.0, c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch  360 | Train Loss: 0.393964 | Step Size: 1.000000 | Avg Step: 1.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 46/50 | Time: 14.32s\n",
      "\n",
      "Early stopping triggered at epoch 364\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393964\n",
      "Total training time: 14.46s\n",
      "Average time per epoch: 0.0397s\n",
      "Average step size: 1.000000\n",
      "Final step size: 1.000000\n",
      "Average backtrack iterations: 0.00\n",
      "Starting SGD with Backtracking Line Search\n",
      "Initial step size: 5.0, c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.394011 | Step Size: 5.000000 | Avg Step: 4.875000 | Backtrack:  0 | Avg BT: 0.1 | Patience:  0/50 | Time: 0.76s\n",
      "Epoch   20 | Train Loss: 0.394011 | Step Size: 5.000000 | Avg Step: 4.875000 | Backtrack:  0 | Avg BT: 0.1 | Patience:  0/50 | Time: 0.76s\n",
      "Epoch   40 | Train Loss: 0.393971 | Step Size: 5.000000 | Avg Step: 5.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 1.59s\n",
      "Epoch   40 | Train Loss: 0.393971 | Step Size: 5.000000 | Avg Step: 5.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 1.59s\n",
      "Epoch   60 | Train Loss: 0.393965 | Step Size: 5.000000 | Avg Step: 5.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 2.33s\n",
      "Epoch   60 | Train Loss: 0.393965 | Step Size: 5.000000 | Avg Step: 5.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 2.33s\n",
      "Epoch   80 | Train Loss: 0.393964 | Step Size: 5.000000 | Avg Step: 5.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 3.07s\n",
      "Epoch   80 | Train Loss: 0.393964 | Step Size: 5.000000 | Avg Step: 5.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 3.07s\n",
      "Epoch  100 | Train Loss: 0.393963 | Step Size: 5.000000 | Avg Step: 5.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 30/50 | Time: 3.83s\n",
      "Epoch  100 | Train Loss: 0.393963 | Step Size: 5.000000 | Avg Step: 5.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 30/50 | Time: 3.83s\n",
      "Epoch  120 | Train Loss: 0.393963 | Step Size: 5.000000 | Avg Step: 5.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 50/50 | Time: 4.57s\n",
      "\n",
      "Early stopping triggered at epoch 120\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393963\n",
      "Total training time: 4.57s\n",
      "Average time per epoch: 0.0381s\n",
      "Average step size: 4.979167\n",
      "Final step size: 5.000000\n",
      "Average backtrack iterations: 0.01\n",
      "Starting SGD with Backtracking Line Search\n",
      "Initial step size: 10.0, c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch  120 | Train Loss: 0.393963 | Step Size: 5.000000 | Avg Step: 5.000000 | Backtrack:  0 | Avg BT: 0.0 | Patience: 50/50 | Time: 4.57s\n",
      "\n",
      "Early stopping triggered at epoch 120\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393963\n",
      "Total training time: 4.57s\n",
      "Average time per epoch: 0.0381s\n",
      "Average step size: 4.979167\n",
      "Final step size: 5.000000\n",
      "Average backtrack iterations: 0.01\n",
      "Starting SGD with Backtracking Line Search\n",
      "Initial step size: 10.0, c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.394012 | Step Size: 5.000000 | Avg Step: 5.125000 | Backtrack:  1 | Avg BT: 1.0 | Patience:  0/50 | Time: 1.01s\n",
      "Epoch   20 | Train Loss: 0.394012 | Step Size: 5.000000 | Avg Step: 5.125000 | Backtrack:  1 | Avg BT: 1.0 | Patience:  0/50 | Time: 1.01s\n",
      "Epoch   40 | Train Loss: 0.393970 | Step Size: 5.000000 | Avg Step: 5.500000 | Backtrack:  1 | Avg BT: 0.9 | Patience:  0/50 | Time: 2.18s\n",
      "Epoch   40 | Train Loss: 0.393970 | Step Size: 5.000000 | Avg Step: 5.500000 | Backtrack:  1 | Avg BT: 0.9 | Patience:  0/50 | Time: 2.18s\n",
      "Epoch   60 | Train Loss: 0.393965 | Step Size: 5.000000 | Avg Step: 5.750000 | Backtrack:  1 | Avg BT: 0.8 | Patience:  2/50 | Time: 3.23s\n",
      "Epoch   60 | Train Loss: 0.393965 | Step Size: 5.000000 | Avg Step: 5.750000 | Backtrack:  1 | Avg BT: 0.8 | Patience:  2/50 | Time: 3.23s\n",
      "Epoch   80 | Train Loss: 0.393964 | Step Size: 5.000000 | Avg Step: 5.750000 | Backtrack:  1 | Avg BT: 0.8 | Patience:  5/50 | Time: 4.21s\n",
      "Epoch   80 | Train Loss: 0.393964 | Step Size: 5.000000 | Avg Step: 5.750000 | Backtrack:  1 | Avg BT: 0.8 | Patience:  5/50 | Time: 4.21s\n",
      "Epoch  100 | Train Loss: 0.393963 | Step Size: 5.000000 | Avg Step: 5.750000 | Backtrack:  1 | Avg BT: 0.8 | Patience: 25/50 | Time: 5.19s\n",
      "Epoch  100 | Train Loss: 0.393963 | Step Size: 5.000000 | Avg Step: 5.750000 | Backtrack:  1 | Avg BT: 0.8 | Patience: 25/50 | Time: 5.19s\n",
      "Epoch  120 | Train Loss: 0.393963 | Step Size: 5.000000 | Avg Step: 5.750000 | Backtrack:  1 | Avg BT: 0.8 | Patience: 45/50 | Time: 6.25s\n",
      "Epoch  120 | Train Loss: 0.393963 | Step Size: 5.000000 | Avg Step: 5.750000 | Backtrack:  1 | Avg BT: 0.8 | Patience: 45/50 | Time: 6.25s\n",
      "\n",
      "Early stopping triggered at epoch 125\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393963\n",
      "Total training time: 6.50s\n",
      "Average time per epoch: 0.0520s\n",
      "Average step size: 5.620000\n",
      "Final step size: 5.000000\n",
      "Average backtrack iterations: 0.88\n",
      "\n",
      "Early stopping triggered at epoch 125\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393963\n",
      "Total training time: 6.50s\n",
      "Average time per epoch: 0.0520s\n",
      "Average step size: 5.620000\n",
      "Final step size: 5.000000\n",
      "Average backtrack iterations: 0.88\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SGD Backtracking comparison saved!\n"
     ]
    }
   ],
   "source": [
    "# SGD Backtracking - Loss Convergence Comparison  \n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sgd_bt_configs = [\n",
    "    {\"init_lr\": 0.1, \"color\": \"blue\", \"label\": \"Init LR 0.1\"},\n",
    "    {\"init_lr\": 1.0, \"color\": \"orange\", \"label\": \"Init LR 1.0\"},\n",
    "    {\"init_lr\": 5.0, \"color\": \"green\", \"label\": \"Init LR 5.0\"},\n",
    "    {\"init_lr\": 10.0, \"color\": \"red\", \"label\": \"Init LR 10.0\"},\n",
    "]\n",
    "\n",
    "for config in sgd_bt_configs:\n",
    "    model = WeightedLogisticRegression()\n",
    "    optimizer = SGDBacktrackingOptimizer(\n",
    "        initial_learning_rate=config[\"init_lr\"],\n",
    "        log_interval=20,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    result = optimizer.optimize(\n",
    "        model, X_train_scaled, y_train,\n",
    "        weights_train=weights_train\n",
    "    )\n",
    "    \n",
    "    plt.plot(result['epoch_history'], result['loss_history'], \n",
    "             color=config[\"color\"], linewidth=2, label=config[\"label\"])\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Training Loss', fontsize=12)\n",
    "plt.title('SGD Backtracking - Loss Convergence', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(images_dir / \"sgd_backtracking_convergence.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✅ SGD Backtracking comparison saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4912865",
   "metadata": {},
   "source": [
    "## AGD fixed learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17b60399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AGD training with learning rate: 0.01, momentum: 0.9\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.446318 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 0.63s\n",
      "Epoch   20 | Train Loss: 0.446318 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 0.63s\n",
      "Epoch   40 | Train Loss: 0.417985 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 1.17s\n",
      "Epoch   40 | Train Loss: 0.417985 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 1.17s\n",
      "Epoch   60 | Train Loss: 0.408830 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 1.71s\n",
      "Epoch   60 | Train Loss: 0.408830 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 1.71s\n",
      "Epoch   80 | Train Loss: 0.403193 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 2.26s\n",
      "Epoch   80 | Train Loss: 0.403193 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 2.26s\n",
      "Epoch  100 | Train Loss: 0.399901 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 2.77s\n",
      "Epoch  100 | Train Loss: 0.399901 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 2.77s\n",
      "Epoch  120 | Train Loss: 0.397905 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 3.29s\n",
      "Epoch  120 | Train Loss: 0.397905 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 3.29s\n",
      "Epoch  140 | Train Loss: 0.396647 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 3.81s\n",
      "Epoch  140 | Train Loss: 0.396647 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 3.81s\n",
      "Epoch  160 | Train Loss: 0.395836 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 4.54s\n",
      "Epoch  160 | Train Loss: 0.395836 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 4.54s\n",
      "Epoch  180 | Train Loss: 0.395303 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 5.11s\n",
      "Epoch  180 | Train Loss: 0.395303 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 5.11s\n",
      "Epoch  200 | Train Loss: 0.394947 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 5.67s\n",
      "Epoch  200 | Train Loss: 0.394947 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 5.67s\n",
      "Epoch  220 | Train Loss: 0.394705 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 6.29s\n",
      "Epoch  220 | Train Loss: 0.394705 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 6.29s\n",
      "Epoch  240 | Train Loss: 0.394538 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 6.90s\n",
      "Epoch  240 | Train Loss: 0.394538 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 6.90s\n",
      "Epoch  260 | Train Loss: 0.394421 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 7.48s\n",
      "Epoch  260 | Train Loss: 0.394421 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 7.48s\n",
      "Epoch  280 | Train Loss: 0.394339 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 8.04s\n",
      "Epoch  280 | Train Loss: 0.394339 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 8.04s\n",
      "Epoch  300 | Train Loss: 0.394279 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 8.60s\n",
      "Epoch  300 | Train Loss: 0.394279 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 8.60s\n",
      "Epoch  320 | Train Loss: 0.394235 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 9.25s\n",
      "Epoch  320 | Train Loss: 0.394235 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 9.25s\n",
      "Epoch  340 | Train Loss: 0.394203 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 9.89s\n",
      "Epoch  340 | Train Loss: 0.394203 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 9.89s\n",
      "Epoch  360 | Train Loss: 0.394178 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 10.43s\n",
      "Epoch  360 | Train Loss: 0.394178 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 10.43s\n",
      "Epoch  380 | Train Loss: 0.394159 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 11.01s\n",
      "Epoch  380 | Train Loss: 0.394159 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 11.01s\n",
      "Epoch  400 | Train Loss: 0.394143 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 11.60s\n",
      "Epoch  400 | Train Loss: 0.394143 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 11.60s\n",
      "Epoch  420 | Train Loss: 0.394131 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 12.12s\n",
      "Epoch  420 | Train Loss: 0.394131 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 12.12s\n",
      "Epoch  440 | Train Loss: 0.394120 | LR: 0.0100 | Momentum: 0.900 | Patience:  2/50 | Time: 12.72s\n",
      "Epoch  440 | Train Loss: 0.394120 | LR: 0.0100 | Momentum: 0.900 | Patience:  2/50 | Time: 12.72s\n",
      "Epoch  460 | Train Loss: 0.394112 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 13.40s\n",
      "Epoch  460 | Train Loss: 0.394112 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 13.40s\n",
      "Epoch  480 | Train Loss: 0.394104 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 14.13s\n",
      "Epoch  480 | Train Loss: 0.394104 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 14.13s\n",
      "Epoch  500 | Train Loss: 0.394097 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 14.74s\n",
      "Epoch  500 | Train Loss: 0.394097 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 14.74s\n",
      "Epoch  520 | Train Loss: 0.394091 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 15.40s\n",
      "Epoch  520 | Train Loss: 0.394091 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 15.40s\n",
      "Epoch  540 | Train Loss: 0.394085 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 16.15s\n",
      "Epoch  540 | Train Loss: 0.394085 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 16.15s\n",
      "Epoch  560 | Train Loss: 0.394080 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 16.80s\n",
      "Epoch  560 | Train Loss: 0.394080 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 16.80s\n",
      "Epoch  580 | Train Loss: 0.394075 | LR: 0.0100 | Momentum: 0.900 | Patience:  2/50 | Time: 17.42s\n",
      "Epoch  580 | Train Loss: 0.394075 | LR: 0.0100 | Momentum: 0.900 | Patience:  2/50 | Time: 17.42s\n",
      "Epoch  600 | Train Loss: 0.394070 | LR: 0.0100 | Momentum: 0.900 | Patience:  2/50 | Time: 17.95s\n",
      "Epoch  600 | Train Loss: 0.394070 | LR: 0.0100 | Momentum: 0.900 | Patience:  2/50 | Time: 17.95s\n",
      "Epoch  620 | Train Loss: 0.394066 | LR: 0.0100 | Momentum: 0.900 | Patience:  2/50 | Time: 18.56s\n",
      "Epoch  620 | Train Loss: 0.394066 | LR: 0.0100 | Momentum: 0.900 | Patience:  2/50 | Time: 18.56s\n",
      "Epoch  640 | Train Loss: 0.394062 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 19.13s\n",
      "Epoch  640 | Train Loss: 0.394062 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 19.13s\n",
      "Epoch  660 | Train Loss: 0.394058 | LR: 0.0100 | Momentum: 0.900 | Patience:  3/50 | Time: 19.69s\n",
      "Epoch  660 | Train Loss: 0.394058 | LR: 0.0100 | Momentum: 0.900 | Patience:  3/50 | Time: 19.69s\n",
      "Epoch  680 | Train Loss: 0.394055 | LR: 0.0100 | Momentum: 0.900 | Patience:  5/50 | Time: 20.25s\n",
      "Epoch  680 | Train Loss: 0.394055 | LR: 0.0100 | Momentum: 0.900 | Patience:  5/50 | Time: 20.25s\n",
      "Epoch  700 | Train Loss: 0.394051 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 20.80s\n",
      "Epoch  700 | Train Loss: 0.394051 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 20.80s\n",
      "Epoch  720 | Train Loss: 0.394048 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 21.32s\n",
      "Epoch  720 | Train Loss: 0.394048 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 21.32s\n",
      "Epoch  740 | Train Loss: 0.394045 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 21.88s\n",
      "Epoch  740 | Train Loss: 0.394045 | LR: 0.0100 | Momentum: 0.900 | Patience:  0/50 | Time: 21.88s\n",
      "Epoch  760 | Train Loss: 0.394042 | LR: 0.0100 | Momentum: 0.900 | Patience:  6/50 | Time: 22.45s\n",
      "Epoch  760 | Train Loss: 0.394042 | LR: 0.0100 | Momentum: 0.900 | Patience:  6/50 | Time: 22.45s\n",
      "Epoch  780 | Train Loss: 0.394039 | LR: 0.0100 | Momentum: 0.900 | Patience:  5/50 | Time: 23.03s\n",
      "Epoch  780 | Train Loss: 0.394039 | LR: 0.0100 | Momentum: 0.900 | Patience:  5/50 | Time: 23.03s\n",
      "Epoch  800 | Train Loss: 0.394036 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 23.58s\n",
      "Epoch  800 | Train Loss: 0.394036 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 23.58s\n",
      "Epoch  820 | Train Loss: 0.394033 | LR: 0.0100 | Momentum: 0.900 | Patience:  5/50 | Time: 24.15s\n",
      "Epoch  820 | Train Loss: 0.394033 | LR: 0.0100 | Momentum: 0.900 | Patience:  5/50 | Time: 24.15s\n",
      "Epoch  840 | Train Loss: 0.394031 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 24.69s\n",
      "Epoch  840 | Train Loss: 0.394031 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 24.69s\n",
      "Epoch  860 | Train Loss: 0.394028 | LR: 0.0100 | Momentum: 0.900 | Patience:  3/50 | Time: 25.21s\n",
      "Epoch  860 | Train Loss: 0.394028 | LR: 0.0100 | Momentum: 0.900 | Patience:  3/50 | Time: 25.21s\n",
      "Epoch  880 | Train Loss: 0.394026 | LR: 0.0100 | Momentum: 0.900 | Patience:  5/50 | Time: 25.82s\n",
      "Epoch  880 | Train Loss: 0.394026 | LR: 0.0100 | Momentum: 0.900 | Patience:  5/50 | Time: 25.82s\n",
      "Epoch  900 | Train Loss: 0.394024 | LR: 0.0100 | Momentum: 0.900 | Patience:  7/50 | Time: 26.46s\n",
      "Epoch  900 | Train Loss: 0.394024 | LR: 0.0100 | Momentum: 0.900 | Patience:  7/50 | Time: 26.46s\n",
      "Epoch  920 | Train Loss: 0.394022 | LR: 0.0100 | Momentum: 0.900 | Patience:  7/50 | Time: 27.01s\n",
      "Epoch  920 | Train Loss: 0.394022 | LR: 0.0100 | Momentum: 0.900 | Patience:  7/50 | Time: 27.01s\n",
      "Epoch  940 | Train Loss: 0.394020 | LR: 0.0100 | Momentum: 0.900 | Patience:  7/50 | Time: 27.65s\n",
      "Epoch  940 | Train Loss: 0.394020 | LR: 0.0100 | Momentum: 0.900 | Patience:  7/50 | Time: 27.65s\n",
      "Epoch  960 | Train Loss: 0.394018 | LR: 0.0100 | Momentum: 0.900 | Patience:  6/50 | Time: 28.38s\n",
      "Epoch  960 | Train Loss: 0.394018 | LR: 0.0100 | Momentum: 0.900 | Patience:  6/50 | Time: 28.38s\n",
      "Epoch  980 | Train Loss: 0.394016 | LR: 0.0100 | Momentum: 0.900 | Patience:  4/50 | Time: 28.94s\n",
      "Epoch  980 | Train Loss: 0.394016 | LR: 0.0100 | Momentum: 0.900 | Patience:  4/50 | Time: 28.94s\n",
      "Epoch 1000 | Train Loss: 0.394014 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 29.48s\n",
      "Epoch 1000 | Train Loss: 0.394014 | LR: 0.0100 | Momentum: 0.900 | Patience:  1/50 | Time: 29.48s\n",
      "Epoch 1020 | Train Loss: 0.394012 | LR: 0.0100 | Momentum: 0.900 | Patience:  9/50 | Time: 30.02s\n",
      "Epoch 1020 | Train Loss: 0.394012 | LR: 0.0100 | Momentum: 0.900 | Patience:  9/50 | Time: 30.02s\n",
      "Epoch 1040 | Train Loss: 0.394010 | LR: 0.0100 | Momentum: 0.900 | Patience:  5/50 | Time: 30.56s\n",
      "Epoch 1040 | Train Loss: 0.394010 | LR: 0.0100 | Momentum: 0.900 | Patience:  5/50 | Time: 30.56s\n",
      "Epoch 1060 | Train Loss: 0.394009 | LR: 0.0100 | Momentum: 0.900 | Patience: 12/50 | Time: 31.11s\n",
      "Epoch 1060 | Train Loss: 0.394009 | LR: 0.0100 | Momentum: 0.900 | Patience: 12/50 | Time: 31.11s\n",
      "Epoch 1080 | Train Loss: 0.394007 | LR: 0.0100 | Momentum: 0.900 | Patience:  6/50 | Time: 31.67s\n",
      "Epoch 1080 | Train Loss: 0.394007 | LR: 0.0100 | Momentum: 0.900 | Patience:  6/50 | Time: 31.67s\n",
      "Epoch 1100 | Train Loss: 0.394006 | LR: 0.0100 | Momentum: 0.900 | Patience: 12/50 | Time: 32.23s\n",
      "Epoch 1100 | Train Loss: 0.394006 | LR: 0.0100 | Momentum: 0.900 | Patience: 12/50 | Time: 32.23s\n",
      "Epoch 1120 | Train Loss: 0.394004 | LR: 0.0100 | Momentum: 0.900 | Patience:  4/50 | Time: 32.82s\n",
      "Epoch 1120 | Train Loss: 0.394004 | LR: 0.0100 | Momentum: 0.900 | Patience:  4/50 | Time: 32.82s\n",
      "Epoch 1140 | Train Loss: 0.394003 | LR: 0.0100 | Momentum: 0.900 | Patience:  9/50 | Time: 33.44s\n",
      "Epoch 1140 | Train Loss: 0.394003 | LR: 0.0100 | Momentum: 0.900 | Patience:  9/50 | Time: 33.44s\n",
      "Epoch 1160 | Train Loss: 0.394002 | LR: 0.0100 | Momentum: 0.900 | Patience: 14/50 | Time: 34.08s\n",
      "Epoch 1160 | Train Loss: 0.394002 | LR: 0.0100 | Momentum: 0.900 | Patience: 14/50 | Time: 34.08s\n",
      "Epoch 1180 | Train Loss: 0.394000 | LR: 0.0100 | Momentum: 0.900 | Patience:  2/50 | Time: 34.75s\n",
      "Epoch 1180 | Train Loss: 0.394000 | LR: 0.0100 | Momentum: 0.900 | Patience:  2/50 | Time: 34.75s\n",
      "Epoch 1200 | Train Loss: 0.393999 | LR: 0.0100 | Momentum: 0.900 | Patience:  5/50 | Time: 35.53s\n",
      "Epoch 1200 | Train Loss: 0.393999 | LR: 0.0100 | Momentum: 0.900 | Patience:  5/50 | Time: 35.53s\n",
      "Epoch 1220 | Train Loss: 0.393998 | LR: 0.0100 | Momentum: 0.900 | Patience:  8/50 | Time: 36.12s\n",
      "Epoch 1220 | Train Loss: 0.393998 | LR: 0.0100 | Momentum: 0.900 | Patience:  8/50 | Time: 36.12s\n",
      "Epoch 1240 | Train Loss: 0.393997 | LR: 0.0100 | Momentum: 0.900 | Patience: 10/50 | Time: 36.71s\n",
      "Epoch 1240 | Train Loss: 0.393997 | LR: 0.0100 | Momentum: 0.900 | Patience: 10/50 | Time: 36.71s\n",
      "Epoch 1260 | Train Loss: 0.393996 | LR: 0.0100 | Momentum: 0.900 | Patience: 12/50 | Time: 37.40s\n",
      "Epoch 1260 | Train Loss: 0.393996 | LR: 0.0100 | Momentum: 0.900 | Patience: 12/50 | Time: 37.40s\n",
      "Epoch 1280 | Train Loss: 0.393995 | LR: 0.0100 | Momentum: 0.900 | Patience: 13/50 | Time: 38.12s\n",
      "Epoch 1280 | Train Loss: 0.393995 | LR: 0.0100 | Momentum: 0.900 | Patience: 13/50 | Time: 38.12s\n",
      "Epoch 1300 | Train Loss: 0.393994 | LR: 0.0100 | Momentum: 0.900 | Patience: 13/50 | Time: 38.71s\n",
      "Epoch 1300 | Train Loss: 0.393994 | LR: 0.0100 | Momentum: 0.900 | Patience: 13/50 | Time: 38.71s\n",
      "Epoch 1320 | Train Loss: 0.393993 | LR: 0.0100 | Momentum: 0.900 | Patience: 13/50 | Time: 39.32s\n",
      "Epoch 1320 | Train Loss: 0.393993 | LR: 0.0100 | Momentum: 0.900 | Patience: 13/50 | Time: 39.32s\n",
      "Epoch 1340 | Train Loss: 0.393992 | LR: 0.0100 | Momentum: 0.900 | Patience: 12/50 | Time: 39.95s\n",
      "Epoch 1340 | Train Loss: 0.393992 | LR: 0.0100 | Momentum: 0.900 | Patience: 12/50 | Time: 39.95s\n",
      "Epoch 1360 | Train Loss: 0.393991 | LR: 0.0100 | Momentum: 0.900 | Patience: 10/50 | Time: 40.77s\n",
      "Epoch 1360 | Train Loss: 0.393991 | LR: 0.0100 | Momentum: 0.900 | Patience: 10/50 | Time: 40.77s\n",
      "Epoch 1380 | Train Loss: 0.393990 | LR: 0.0100 | Momentum: 0.900 | Patience:  7/50 | Time: 41.32s\n",
      "Epoch 1380 | Train Loss: 0.393990 | LR: 0.0100 | Momentum: 0.900 | Patience:  7/50 | Time: 41.32s\n",
      "Epoch 1400 | Train Loss: 0.393989 | LR: 0.0100 | Momentum: 0.900 | Patience:  3/50 | Time: 41.99s\n",
      "Epoch 1400 | Train Loss: 0.393989 | LR: 0.0100 | Momentum: 0.900 | Patience:  3/50 | Time: 41.99s\n",
      "Epoch 1420 | Train Loss: 0.393988 | LR: 0.0100 | Momentum: 0.900 | Patience: 23/50 | Time: 42.56s\n",
      "Epoch 1420 | Train Loss: 0.393988 | LR: 0.0100 | Momentum: 0.900 | Patience: 23/50 | Time: 42.56s\n",
      "Epoch 1440 | Train Loss: 0.393987 | LR: 0.0100 | Momentum: 0.900 | Patience: 18/50 | Time: 43.09s\n",
      "Epoch 1440 | Train Loss: 0.393987 | LR: 0.0100 | Momentum: 0.900 | Patience: 18/50 | Time: 43.09s\n",
      "Epoch 1460 | Train Loss: 0.393987 | LR: 0.0100 | Momentum: 0.900 | Patience: 12/50 | Time: 43.66s\n",
      "Epoch 1460 | Train Loss: 0.393987 | LR: 0.0100 | Momentum: 0.900 | Patience: 12/50 | Time: 43.66s\n",
      "Epoch 1480 | Train Loss: 0.393986 | LR: 0.0100 | Momentum: 0.900 | Patience:  5/50 | Time: 44.26s\n",
      "Epoch 1480 | Train Loss: 0.393986 | LR: 0.0100 | Momentum: 0.900 | Patience:  5/50 | Time: 44.26s\n",
      "Epoch 1500 | Train Loss: 0.393985 | LR: 0.0100 | Momentum: 0.900 | Patience: 25/50 | Time: 44.77s\n",
      "Epoch 1500 | Train Loss: 0.393985 | LR: 0.0100 | Momentum: 0.900 | Patience: 25/50 | Time: 44.77s\n",
      "Epoch 1520 | Train Loss: 0.393984 | LR: 0.0100 | Momentum: 0.900 | Patience: 17/50 | Time: 45.37s\n",
      "Epoch 1520 | Train Loss: 0.393984 | LR: 0.0100 | Momentum: 0.900 | Patience: 17/50 | Time: 45.37s\n",
      "Epoch 1540 | Train Loss: 0.393984 | LR: 0.0100 | Momentum: 0.900 | Patience:  7/50 | Time: 45.92s\n",
      "Epoch 1540 | Train Loss: 0.393984 | LR: 0.0100 | Momentum: 0.900 | Patience:  7/50 | Time: 45.92s\n",
      "Epoch 1560 | Train Loss: 0.393983 | LR: 0.0100 | Momentum: 0.900 | Patience: 27/50 | Time: 46.76s\n",
      "Epoch 1560 | Train Loss: 0.393983 | LR: 0.0100 | Momentum: 0.900 | Patience: 27/50 | Time: 46.76s\n",
      "Epoch 1580 | Train Loss: 0.393982 | LR: 0.0100 | Momentum: 0.900 | Patience: 16/50 | Time: 47.39s\n",
      "Epoch 1580 | Train Loss: 0.393982 | LR: 0.0100 | Momentum: 0.900 | Patience: 16/50 | Time: 47.39s\n",
      "Epoch 1600 | Train Loss: 0.393982 | LR: 0.0100 | Momentum: 0.900 | Patience:  3/50 | Time: 47.92s\n",
      "Epoch 1600 | Train Loss: 0.393982 | LR: 0.0100 | Momentum: 0.900 | Patience:  3/50 | Time: 47.92s\n",
      "Epoch 1620 | Train Loss: 0.393981 | LR: 0.0100 | Momentum: 0.900 | Patience: 23/50 | Time: 48.43s\n",
      "Epoch 1620 | Train Loss: 0.393981 | LR: 0.0100 | Momentum: 0.900 | Patience: 23/50 | Time: 48.43s\n",
      "Epoch 1640 | Train Loss: 0.393981 | LR: 0.0100 | Momentum: 0.900 | Patience:  8/50 | Time: 48.96s\n",
      "Epoch 1640 | Train Loss: 0.393981 | LR: 0.0100 | Momentum: 0.900 | Patience:  8/50 | Time: 48.96s\n",
      "Epoch 1660 | Train Loss: 0.393980 | LR: 0.0100 | Momentum: 0.900 | Patience: 28/50 | Time: 49.67s\n",
      "Epoch 1660 | Train Loss: 0.393980 | LR: 0.0100 | Momentum: 0.900 | Patience: 28/50 | Time: 49.67s\n",
      "Epoch 1680 | Train Loss: 0.393980 | LR: 0.0100 | Momentum: 0.900 | Patience: 11/50 | Time: 50.26s\n",
      "Epoch 1680 | Train Loss: 0.393980 | LR: 0.0100 | Momentum: 0.900 | Patience: 11/50 | Time: 50.26s\n",
      "Epoch 1700 | Train Loss: 0.393979 | LR: 0.0100 | Momentum: 0.900 | Patience: 31/50 | Time: 50.81s\n",
      "Epoch 1700 | Train Loss: 0.393979 | LR: 0.0100 | Momentum: 0.900 | Patience: 31/50 | Time: 50.81s\n",
      "Epoch 1720 | Train Loss: 0.393979 | LR: 0.0100 | Momentum: 0.900 | Patience: 11/50 | Time: 51.35s\n",
      "Epoch 1720 | Train Loss: 0.393979 | LR: 0.0100 | Momentum: 0.900 | Patience: 11/50 | Time: 51.35s\n",
      "Epoch 1740 | Train Loss: 0.393978 | LR: 0.0100 | Momentum: 0.900 | Patience: 31/50 | Time: 51.98s\n",
      "Epoch 1740 | Train Loss: 0.393978 | LR: 0.0100 | Momentum: 0.900 | Patience: 31/50 | Time: 51.98s\n",
      "Epoch 1760 | Train Loss: 0.393978 | LR: 0.0100 | Momentum: 0.900 | Patience:  8/50 | Time: 52.64s\n",
      "Epoch 1760 | Train Loss: 0.393978 | LR: 0.0100 | Momentum: 0.900 | Patience:  8/50 | Time: 52.64s\n",
      "Epoch 1780 | Train Loss: 0.393977 | LR: 0.0100 | Momentum: 0.900 | Patience: 28/50 | Time: 53.20s\n",
      "Epoch 1780 | Train Loss: 0.393977 | LR: 0.0100 | Momentum: 0.900 | Patience: 28/50 | Time: 53.20s\n",
      "Epoch 1800 | Train Loss: 0.393977 | LR: 0.0100 | Momentum: 0.900 | Patience:  2/50 | Time: 53.74s\n",
      "Epoch 1800 | Train Loss: 0.393977 | LR: 0.0100 | Momentum: 0.900 | Patience:  2/50 | Time: 53.74s\n",
      "Epoch 1820 | Train Loss: 0.393976 | LR: 0.0100 | Momentum: 0.900 | Patience: 22/50 | Time: 54.26s\n",
      "Epoch 1820 | Train Loss: 0.393976 | LR: 0.0100 | Momentum: 0.900 | Patience: 22/50 | Time: 54.26s\n",
      "Epoch 1840 | Train Loss: 0.393976 | LR: 0.0100 | Momentum: 0.900 | Patience: 42/50 | Time: 54.95s\n",
      "Epoch 1840 | Train Loss: 0.393976 | LR: 0.0100 | Momentum: 0.900 | Patience: 42/50 | Time: 54.95s\n",
      "Epoch 1860 | Train Loss: 0.393976 | LR: 0.0100 | Momentum: 0.900 | Patience: 12/50 | Time: 55.65s\n",
      "Epoch 1860 | Train Loss: 0.393976 | LR: 0.0100 | Momentum: 0.900 | Patience: 12/50 | Time: 55.65s\n",
      "Epoch 1880 | Train Loss: 0.393975 | LR: 0.0100 | Momentum: 0.900 | Patience: 32/50 | Time: 56.24s\n",
      "Epoch 1880 | Train Loss: 0.393975 | LR: 0.0100 | Momentum: 0.900 | Patience: 32/50 | Time: 56.24s\n",
      "\n",
      "Early stopping triggered at epoch 1898\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393975\n",
      "Total training time: 56.78s\n",
      "Average time per epoch: 0.0299s\n",
      "Starting AGD training with learning rate: 0.01, momentum: 0.95\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Early stopping triggered at epoch 1898\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393975\n",
      "Total training time: 56.78s\n",
      "Average time per epoch: 0.0299s\n",
      "Starting AGD training with learning rate: 0.01, momentum: 0.95\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.431320 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 0.51s\n",
      "Epoch   20 | Train Loss: 0.431320 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 0.51s\n",
      "Epoch   40 | Train Loss: 0.422527 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 1.05s\n",
      "Epoch   40 | Train Loss: 0.422527 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 1.05s\n",
      "Epoch   60 | Train Loss: 0.406608 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 1.60s\n",
      "Epoch   60 | Train Loss: 0.406608 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 1.60s\n",
      "Epoch   80 | Train Loss: 0.396309 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 2.12s\n",
      "Epoch   80 | Train Loss: 0.396309 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 2.12s\n",
      "Epoch  100 | Train Loss: 0.395209 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 2.63s\n",
      "Epoch  100 | Train Loss: 0.395209 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 2.63s\n",
      "Epoch  120 | Train Loss: 0.394562 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 3.16s\n",
      "Epoch  120 | Train Loss: 0.394562 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 3.16s\n",
      "Epoch  140 | Train Loss: 0.394242 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 3.68s\n",
      "Epoch  140 | Train Loss: 0.394242 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 3.68s\n",
      "Epoch  160 | Train Loss: 0.394218 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 4.18s\n",
      "Epoch  160 | Train Loss: 0.394218 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 4.18s\n",
      "Epoch  180 | Train Loss: 0.394170 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 4.68s\n",
      "Epoch  180 | Train Loss: 0.394170 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 4.68s\n",
      "Epoch  200 | Train Loss: 0.394139 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 5.20s\n",
      "Epoch  200 | Train Loss: 0.394139 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 5.20s\n",
      "Epoch  220 | Train Loss: 0.394124 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 5.74s\n",
      "Epoch  220 | Train Loss: 0.394124 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 5.74s\n",
      "Epoch  240 | Train Loss: 0.394109 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 6.31s\n",
      "Epoch  240 | Train Loss: 0.394109 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 6.31s\n",
      "Epoch  260 | Train Loss: 0.394097 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 6.86s\n",
      "Epoch  260 | Train Loss: 0.394097 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 6.86s\n",
      "Epoch  280 | Train Loss: 0.394086 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 7.44s\n",
      "Epoch  280 | Train Loss: 0.394086 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 7.44s\n",
      "Epoch  300 | Train Loss: 0.394076 | LR: 0.0100 | Momentum: 0.950 | Patience:  2/50 | Time: 7.99s\n",
      "Epoch  300 | Train Loss: 0.394076 | LR: 0.0100 | Momentum: 0.950 | Patience:  2/50 | Time: 7.99s\n",
      "Epoch  320 | Train Loss: 0.394067 | LR: 0.0100 | Momentum: 0.950 | Patience:  1/50 | Time: 8.52s\n",
      "Epoch  320 | Train Loss: 0.394067 | LR: 0.0100 | Momentum: 0.950 | Patience:  1/50 | Time: 8.52s\n",
      "Epoch  340 | Train Loss: 0.394059 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 9.02s\n",
      "Epoch  340 | Train Loss: 0.394059 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 9.02s\n",
      "Epoch  360 | Train Loss: 0.394052 | LR: 0.0100 | Momentum: 0.950 | Patience:  2/50 | Time: 9.61s\n",
      "Epoch  360 | Train Loss: 0.394052 | LR: 0.0100 | Momentum: 0.950 | Patience:  2/50 | Time: 9.61s\n",
      "Epoch  380 | Train Loss: 0.394046 | LR: 0.0100 | Momentum: 0.950 | Patience:  2/50 | Time: 10.15s\n",
      "Epoch  380 | Train Loss: 0.394046 | LR: 0.0100 | Momentum: 0.950 | Patience:  2/50 | Time: 10.15s\n",
      "Epoch  400 | Train Loss: 0.394039 | LR: 0.0100 | Momentum: 0.950 | Patience:  2/50 | Time: 10.86s\n",
      "Epoch  400 | Train Loss: 0.394039 | LR: 0.0100 | Momentum: 0.950 | Patience:  2/50 | Time: 10.86s\n",
      "Epoch  420 | Train Loss: 0.394034 | LR: 0.0100 | Momentum: 0.950 | Patience:  2/50 | Time: 11.50s\n",
      "Epoch  420 | Train Loss: 0.394034 | LR: 0.0100 | Momentum: 0.950 | Patience:  2/50 | Time: 11.50s\n",
      "Epoch  440 | Train Loss: 0.394029 | LR: 0.0100 | Momentum: 0.950 | Patience:  1/50 | Time: 12.06s\n",
      "Epoch  440 | Train Loss: 0.394029 | LR: 0.0100 | Momentum: 0.950 | Patience:  1/50 | Time: 12.06s\n",
      "Epoch  460 | Train Loss: 0.394024 | LR: 0.0100 | Momentum: 0.950 | Patience:  1/50 | Time: 12.61s\n",
      "Epoch  460 | Train Loss: 0.394024 | LR: 0.0100 | Momentum: 0.950 | Patience:  1/50 | Time: 12.61s\n",
      "Epoch  480 | Train Loss: 0.394020 | LR: 0.0100 | Momentum: 0.950 | Patience:  1/50 | Time: 13.15s\n",
      "Epoch  480 | Train Loss: 0.394020 | LR: 0.0100 | Momentum: 0.950 | Patience:  1/50 | Time: 13.15s\n",
      "Epoch  500 | Train Loss: 0.394016 | LR: 0.0100 | Momentum: 0.950 | Patience:  5/50 | Time: 13.71s\n",
      "Epoch  500 | Train Loss: 0.394016 | LR: 0.0100 | Momentum: 0.950 | Patience:  5/50 | Time: 13.71s\n",
      "Epoch  520 | Train Loss: 0.394012 | LR: 0.0100 | Momentum: 0.950 | Patience:  1/50 | Time: 14.38s\n",
      "Epoch  520 | Train Loss: 0.394012 | LR: 0.0100 | Momentum: 0.950 | Patience:  1/50 | Time: 14.38s\n",
      "Epoch  540 | Train Loss: 0.394009 | LR: 0.0100 | Momentum: 0.950 | Patience:  2/50 | Time: 15.07s\n",
      "Epoch  540 | Train Loss: 0.394009 | LR: 0.0100 | Momentum: 0.950 | Patience:  2/50 | Time: 15.07s\n",
      "Epoch  560 | Train Loss: 0.394006 | LR: 0.0100 | Momentum: 0.950 | Patience:  1/50 | Time: 15.72s\n",
      "Epoch  560 | Train Loss: 0.394006 | LR: 0.0100 | Momentum: 0.950 | Patience:  1/50 | Time: 15.72s\n",
      "Epoch  580 | Train Loss: 0.394003 | LR: 0.0100 | Momentum: 0.950 | Patience:  7/50 | Time: 16.30s\n",
      "Epoch  580 | Train Loss: 0.394003 | LR: 0.0100 | Momentum: 0.950 | Patience:  7/50 | Time: 16.30s\n",
      "Epoch  600 | Train Loss: 0.394000 | LR: 0.0100 | Momentum: 0.950 | Patience:  3/50 | Time: 16.83s\n",
      "Epoch  600 | Train Loss: 0.394000 | LR: 0.0100 | Momentum: 0.950 | Patience:  3/50 | Time: 16.83s\n",
      "Epoch  620 | Train Loss: 0.393998 | LR: 0.0100 | Momentum: 0.950 | Patience:  6/50 | Time: 17.37s\n",
      "Epoch  620 | Train Loss: 0.393998 | LR: 0.0100 | Momentum: 0.950 | Patience:  6/50 | Time: 17.37s\n",
      "Epoch  640 | Train Loss: 0.393996 | LR: 0.0100 | Momentum: 0.950 | Patience:  8/50 | Time: 17.91s\n",
      "Epoch  640 | Train Loss: 0.393996 | LR: 0.0100 | Momentum: 0.950 | Patience:  8/50 | Time: 17.91s\n",
      "Epoch  660 | Train Loss: 0.393994 | LR: 0.0100 | Momentum: 0.950 | Patience:  8/50 | Time: 18.44s\n",
      "Epoch  660 | Train Loss: 0.393994 | LR: 0.0100 | Momentum: 0.950 | Patience:  8/50 | Time: 18.44s\n",
      "Epoch  680 | Train Loss: 0.393992 | LR: 0.0100 | Momentum: 0.950 | Patience:  7/50 | Time: 18.97s\n",
      "Epoch  680 | Train Loss: 0.393992 | LR: 0.0100 | Momentum: 0.950 | Patience:  7/50 | Time: 18.97s\n",
      "Epoch  700 | Train Loss: 0.393990 | LR: 0.0100 | Momentum: 0.950 | Patience:  4/50 | Time: 19.51s\n",
      "Epoch  700 | Train Loss: 0.393990 | LR: 0.0100 | Momentum: 0.950 | Patience:  4/50 | Time: 19.51s\n",
      "Epoch  720 | Train Loss: 0.393988 | LR: 0.0100 | Momentum: 0.950 | Patience: 12/50 | Time: 20.03s\n",
      "Epoch  720 | Train Loss: 0.393988 | LR: 0.0100 | Momentum: 0.950 | Patience: 12/50 | Time: 20.03s\n",
      "Epoch  740 | Train Loss: 0.393987 | LR: 0.0100 | Momentum: 0.950 | Patience:  6/50 | Time: 20.56s\n",
      "Epoch  740 | Train Loss: 0.393987 | LR: 0.0100 | Momentum: 0.950 | Patience:  6/50 | Time: 20.56s\n",
      "Epoch  760 | Train Loss: 0.393985 | LR: 0.0100 | Momentum: 0.950 | Patience: 12/50 | Time: 21.16s\n",
      "Epoch  760 | Train Loss: 0.393985 | LR: 0.0100 | Momentum: 0.950 | Patience: 12/50 | Time: 21.16s\n",
      "Epoch  780 | Train Loss: 0.393984 | LR: 0.0100 | Momentum: 0.950 | Patience:  2/50 | Time: 21.72s\n",
      "Epoch  780 | Train Loss: 0.393984 | LR: 0.0100 | Momentum: 0.950 | Patience:  2/50 | Time: 21.72s\n",
      "Epoch  800 | Train Loss: 0.393982 | LR: 0.0100 | Momentum: 0.950 | Patience:  6/50 | Time: 22.24s\n",
      "Epoch  800 | Train Loss: 0.393982 | LR: 0.0100 | Momentum: 0.950 | Patience:  6/50 | Time: 22.24s\n",
      "Epoch  820 | Train Loss: 0.393981 | LR: 0.0100 | Momentum: 0.950 | Patience:  9/50 | Time: 22.78s\n",
      "Epoch  820 | Train Loss: 0.393981 | LR: 0.0100 | Momentum: 0.950 | Patience:  9/50 | Time: 22.78s\n",
      "Epoch  840 | Train Loss: 0.393980 | LR: 0.0100 | Momentum: 0.950 | Patience: 11/50 | Time: 23.31s\n",
      "Epoch  840 | Train Loss: 0.393980 | LR: 0.0100 | Momentum: 0.950 | Patience: 11/50 | Time: 23.31s\n",
      "Epoch  860 | Train Loss: 0.393979 | LR: 0.0100 | Momentum: 0.950 | Patience: 12/50 | Time: 23.85s\n",
      "Epoch  860 | Train Loss: 0.393979 | LR: 0.0100 | Momentum: 0.950 | Patience: 12/50 | Time: 23.85s\n",
      "Epoch  880 | Train Loss: 0.393978 | LR: 0.0100 | Momentum: 0.950 | Patience: 11/50 | Time: 24.39s\n",
      "Epoch  880 | Train Loss: 0.393978 | LR: 0.0100 | Momentum: 0.950 | Patience: 11/50 | Time: 24.39s\n",
      "Epoch  900 | Train Loss: 0.393977 | LR: 0.0100 | Momentum: 0.950 | Patience:  9/50 | Time: 24.92s\n",
      "Epoch  900 | Train Loss: 0.393977 | LR: 0.0100 | Momentum: 0.950 | Patience:  9/50 | Time: 24.92s\n",
      "Epoch  920 | Train Loss: 0.393977 | LR: 0.0100 | Momentum: 0.950 | Patience:  5/50 | Time: 25.44s\n",
      "Epoch  920 | Train Loss: 0.393977 | LR: 0.0100 | Momentum: 0.950 | Patience:  5/50 | Time: 25.44s\n",
      "Epoch  940 | Train Loss: 0.393976 | LR: 0.0100 | Momentum: 0.950 | Patience: 25/50 | Time: 26.00s\n",
      "Epoch  940 | Train Loss: 0.393976 | LR: 0.0100 | Momentum: 0.950 | Patience: 25/50 | Time: 26.00s\n",
      "Epoch  960 | Train Loss: 0.393975 | LR: 0.0100 | Momentum: 0.950 | Patience: 19/50 | Time: 26.51s\n",
      "Epoch  960 | Train Loss: 0.393975 | LR: 0.0100 | Momentum: 0.950 | Patience: 19/50 | Time: 26.51s\n",
      "Epoch  980 | Train Loss: 0.393974 | LR: 0.0100 | Momentum: 0.950 | Patience: 10/50 | Time: 27.01s\n",
      "Epoch  980 | Train Loss: 0.393974 | LR: 0.0100 | Momentum: 0.950 | Patience: 10/50 | Time: 27.01s\n",
      "Epoch 1000 | Train Loss: 0.393974 | LR: 0.0100 | Momentum: 0.950 | Patience: 30/50 | Time: 27.54s\n",
      "Epoch 1000 | Train Loss: 0.393974 | LR: 0.0100 | Momentum: 0.950 | Patience: 30/50 | Time: 27.54s\n",
      "Epoch 1020 | Train Loss: 0.393973 | LR: 0.0100 | Momentum: 0.950 | Patience: 19/50 | Time: 28.05s\n",
      "Epoch 1020 | Train Loss: 0.393973 | LR: 0.0100 | Momentum: 0.950 | Patience: 19/50 | Time: 28.05s\n",
      "Epoch 1040 | Train Loss: 0.393973 | LR: 0.0100 | Momentum: 0.950 | Patience:  4/50 | Time: 28.55s\n",
      "Epoch 1040 | Train Loss: 0.393973 | LR: 0.0100 | Momentum: 0.950 | Patience:  4/50 | Time: 28.55s\n",
      "Epoch 1060 | Train Loss: 0.393972 | LR: 0.0100 | Momentum: 0.950 | Patience: 24/50 | Time: 29.07s\n",
      "Epoch 1060 | Train Loss: 0.393972 | LR: 0.0100 | Momentum: 0.950 | Patience: 24/50 | Time: 29.07s\n",
      "Epoch 1080 | Train Loss: 0.393972 | LR: 0.0100 | Momentum: 0.950 | Patience:  5/50 | Time: 29.59s\n",
      "Epoch 1080 | Train Loss: 0.393972 | LR: 0.0100 | Momentum: 0.950 | Patience:  5/50 | Time: 29.59s\n",
      "Epoch 1100 | Train Loss: 0.393971 | LR: 0.0100 | Momentum: 0.950 | Patience: 25/50 | Time: 30.10s\n",
      "Epoch 1100 | Train Loss: 0.393971 | LR: 0.0100 | Momentum: 0.950 | Patience: 25/50 | Time: 30.10s\n",
      "Epoch 1120 | Train Loss: 0.393971 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 30.67s\n",
      "Epoch 1120 | Train Loss: 0.393971 | LR: 0.0100 | Momentum: 0.950 | Patience:  0/50 | Time: 30.67s\n",
      "Epoch 1140 | Train Loss: 0.393970 | LR: 0.0100 | Momentum: 0.950 | Patience: 20/50 | Time: 31.27s\n",
      "Epoch 1140 | Train Loss: 0.393970 | LR: 0.0100 | Momentum: 0.950 | Patience: 20/50 | Time: 31.27s\n",
      "Epoch 1160 | Train Loss: 0.393970 | LR: 0.0100 | Momentum: 0.950 | Patience: 40/50 | Time: 31.78s\n",
      "Epoch 1160 | Train Loss: 0.393970 | LR: 0.0100 | Momentum: 0.950 | Patience: 40/50 | Time: 31.78s\n",
      "\n",
      "Early stopping triggered at epoch 1170\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393970\n",
      "Total training time: 32.05s\n",
      "Average time per epoch: 0.0274s\n",
      "Starting AGD training with learning rate: 0.1, momentum: 0.9\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Early stopping triggered at epoch 1170\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393970\n",
      "Total training time: 32.05s\n",
      "Average time per epoch: 0.0274s\n",
      "Starting AGD training with learning rate: 0.1, momentum: 0.9\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.405816 | LR: 0.1000 | Momentum: 0.900 | Patience:  0/50 | Time: 0.51s\n",
      "Epoch   20 | Train Loss: 0.405816 | LR: 0.1000 | Momentum: 0.900 | Patience:  0/50 | Time: 0.51s\n",
      "Epoch   40 | Train Loss: 0.395290 | LR: 0.1000 | Momentum: 0.900 | Patience:  0/50 | Time: 1.02s\n",
      "Epoch   40 | Train Loss: 0.395290 | LR: 0.1000 | Momentum: 0.900 | Patience:  0/50 | Time: 1.02s\n",
      "Epoch   60 | Train Loss: 0.394133 | LR: 0.1000 | Momentum: 0.900 | Patience:  0/50 | Time: 1.53s\n",
      "Epoch   60 | Train Loss: 0.394133 | LR: 0.1000 | Momentum: 0.900 | Patience:  0/50 | Time: 1.53s\n",
      "Epoch   80 | Train Loss: 0.394052 | LR: 0.1000 | Momentum: 0.900 | Patience:  0/50 | Time: 2.05s\n",
      "Epoch   80 | Train Loss: 0.394052 | LR: 0.1000 | Momentum: 0.900 | Patience:  0/50 | Time: 2.05s\n",
      "Epoch  100 | Train Loss: 0.394018 | LR: 0.1000 | Momentum: 0.900 | Patience:  0/50 | Time: 2.56s\n",
      "Epoch  100 | Train Loss: 0.394018 | LR: 0.1000 | Momentum: 0.900 | Patience:  0/50 | Time: 2.56s\n",
      "Epoch  120 | Train Loss: 0.394000 | LR: 0.1000 | Momentum: 0.900 | Patience:  1/50 | Time: 3.11s\n",
      "Epoch  120 | Train Loss: 0.394000 | LR: 0.1000 | Momentum: 0.900 | Patience:  1/50 | Time: 3.11s\n",
      "Epoch  140 | Train Loss: 0.393988 | LR: 0.1000 | Momentum: 0.900 | Patience:  2/50 | Time: 3.69s\n",
      "Epoch  140 | Train Loss: 0.393988 | LR: 0.1000 | Momentum: 0.900 | Patience:  2/50 | Time: 3.69s\n",
      "Epoch  160 | Train Loss: 0.393981 | LR: 0.1000 | Momentum: 0.900 | Patience:  0/50 | Time: 4.22s\n",
      "Epoch  160 | Train Loss: 0.393981 | LR: 0.1000 | Momentum: 0.900 | Patience:  0/50 | Time: 4.22s\n",
      "Epoch  180 | Train Loss: 0.393976 | LR: 0.1000 | Momentum: 0.900 | Patience:  3/50 | Time: 4.75s\n",
      "Epoch  180 | Train Loss: 0.393976 | LR: 0.1000 | Momentum: 0.900 | Patience:  3/50 | Time: 4.75s\n",
      "Epoch  200 | Train Loss: 0.393972 | LR: 0.1000 | Momentum: 0.900 | Patience:  6/50 | Time: 5.26s\n",
      "Epoch  200 | Train Loss: 0.393972 | LR: 0.1000 | Momentum: 0.900 | Patience:  6/50 | Time: 5.26s\n",
      "Epoch  220 | Train Loss: 0.393970 | LR: 0.1000 | Momentum: 0.900 | Patience:  2/50 | Time: 5.80s\n",
      "Epoch  220 | Train Loss: 0.393970 | LR: 0.1000 | Momentum: 0.900 | Patience:  2/50 | Time: 5.80s\n",
      "Epoch  240 | Train Loss: 0.393968 | LR: 0.1000 | Momentum: 0.900 | Patience: 12/50 | Time: 6.32s\n",
      "Epoch  240 | Train Loss: 0.393968 | LR: 0.1000 | Momentum: 0.900 | Patience: 12/50 | Time: 6.32s\n",
      "Epoch  260 | Train Loss: 0.393967 | LR: 0.1000 | Momentum: 0.900 | Patience:  3/50 | Time: 6.84s\n",
      "Epoch  260 | Train Loss: 0.393967 | LR: 0.1000 | Momentum: 0.900 | Patience:  3/50 | Time: 6.84s\n",
      "Epoch  280 | Train Loss: 0.393966 | LR: 0.1000 | Momentum: 0.900 | Patience:  2/50 | Time: 7.35s\n",
      "Epoch  280 | Train Loss: 0.393966 | LR: 0.1000 | Momentum: 0.900 | Patience:  2/50 | Time: 7.35s\n",
      "Epoch  300 | Train Loss: 0.393965 | LR: 0.1000 | Momentum: 0.900 | Patience: 22/50 | Time: 7.85s\n",
      "Epoch  300 | Train Loss: 0.393965 | LR: 0.1000 | Momentum: 0.900 | Patience: 22/50 | Time: 7.85s\n",
      "Epoch  320 | Train Loss: 0.393965 | LR: 0.1000 | Momentum: 0.900 | Patience: 10/50 | Time: 8.40s\n",
      "Epoch  320 | Train Loss: 0.393965 | LR: 0.1000 | Momentum: 0.900 | Patience: 10/50 | Time: 8.40s\n",
      "Epoch  340 | Train Loss: 0.393964 | LR: 0.1000 | Momentum: 0.900 | Patience: 30/50 | Time: 8.90s\n",
      "Epoch  340 | Train Loss: 0.393964 | LR: 0.1000 | Momentum: 0.900 | Patience: 30/50 | Time: 8.90s\n",
      "Epoch  360 | Train Loss: 0.393964 | LR: 0.1000 | Momentum: 0.900 | Patience: 50/50 | Time: 9.43s\n",
      "\n",
      "Early stopping triggered at epoch 360\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393964\n",
      "Total training time: 9.43s\n",
      "Average time per epoch: 0.0262s\n",
      "Starting AGD training with learning rate: 0.1, momentum: 0.95\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "------------------------------------------------------------\n",
      "Epoch  360 | Train Loss: 0.393964 | LR: 0.1000 | Momentum: 0.900 | Patience: 50/50 | Time: 9.43s\n",
      "\n",
      "Early stopping triggered at epoch 360\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393964\n",
      "Total training time: 9.43s\n",
      "Average time per epoch: 0.0262s\n",
      "Starting AGD training with learning rate: 0.1, momentum: 0.95\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.427089 | LR: 0.1000 | Momentum: 0.950 | Patience: 12/50 | Time: 0.52s\n",
      "Epoch   20 | Train Loss: 0.427089 | LR: 0.1000 | Momentum: 0.950 | Patience: 12/50 | Time: 0.52s\n",
      "Epoch   40 | Train Loss: 0.405068 | LR: 0.1000 | Momentum: 0.950 | Patience:  0/50 | Time: 1.05s\n",
      "Epoch   40 | Train Loss: 0.405068 | LR: 0.1000 | Momentum: 0.950 | Patience:  0/50 | Time: 1.05s\n",
      "Epoch   60 | Train Loss: 0.395045 | LR: 0.1000 | Momentum: 0.950 | Patience:  0/50 | Time: 1.56s\n",
      "Epoch   60 | Train Loss: 0.395045 | LR: 0.1000 | Momentum: 0.950 | Patience:  0/50 | Time: 1.56s\n",
      "Epoch   80 | Train Loss: 0.394732 | LR: 0.1000 | Momentum: 0.950 | Patience:  0/50 | Time: 2.06s\n",
      "Epoch   80 | Train Loss: 0.394732 | LR: 0.1000 | Momentum: 0.950 | Patience:  0/50 | Time: 2.06s\n",
      "Epoch  100 | Train Loss: 0.394116 | LR: 0.1000 | Momentum: 0.950 | Patience:  6/50 | Time: 2.58s\n",
      "Epoch  100 | Train Loss: 0.394116 | LR: 0.1000 | Momentum: 0.950 | Patience:  6/50 | Time: 2.58s\n",
      "Epoch  120 | Train Loss: 0.394021 | LR: 0.1000 | Momentum: 0.950 | Patience:  0/50 | Time: 3.09s\n",
      "Epoch  120 | Train Loss: 0.394021 | LR: 0.1000 | Momentum: 0.950 | Patience:  0/50 | Time: 3.09s\n",
      "Epoch  140 | Train Loss: 0.393980 | LR: 0.1000 | Momentum: 0.950 | Patience: 11/50 | Time: 3.64s\n",
      "Epoch  140 | Train Loss: 0.393980 | LR: 0.1000 | Momentum: 0.950 | Patience: 11/50 | Time: 3.64s\n",
      "Epoch  160 | Train Loss: 0.393967 | LR: 0.1000 | Momentum: 0.950 | Patience:  1/50 | Time: 4.15s\n",
      "Epoch  160 | Train Loss: 0.393967 | LR: 0.1000 | Momentum: 0.950 | Patience:  1/50 | Time: 4.15s\n",
      "Epoch  180 | Train Loss: 0.393965 | LR: 0.1000 | Momentum: 0.950 | Patience: 16/50 | Time: 4.70s\n",
      "Epoch  180 | Train Loss: 0.393965 | LR: 0.1000 | Momentum: 0.950 | Patience: 16/50 | Time: 4.70s\n",
      "Epoch  200 | Train Loss: 0.393964 | LR: 0.1000 | Momentum: 0.950 | Patience:  9/50 | Time: 5.22s\n",
      "Epoch  200 | Train Loss: 0.393964 | LR: 0.1000 | Momentum: 0.950 | Patience:  9/50 | Time: 5.22s\n",
      "Epoch  220 | Train Loss: 0.393964 | LR: 0.1000 | Momentum: 0.950 | Patience: 29/50 | Time: 5.73s\n",
      "Epoch  220 | Train Loss: 0.393964 | LR: 0.1000 | Momentum: 0.950 | Patience: 29/50 | Time: 5.73s\n",
      "Epoch  240 | Train Loss: 0.393963 | LR: 0.1000 | Momentum: 0.950 | Patience: 49/50 | Time: 6.24s\n",
      "\n",
      "Early stopping triggered at epoch 241\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393963\n",
      "Total training time: 6.27s\n",
      "Average time per epoch: 0.0260s\n",
      "Epoch  240 | Train Loss: 0.393963 | LR: 0.1000 | Momentum: 0.950 | Patience: 49/50 | Time: 6.24s\n",
      "\n",
      "Early stopping triggered at epoch 241\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393963\n",
      "Total training time: 6.27s\n",
      "Average time per epoch: 0.0260s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AGD Fixed Learning Rate comparison saved!\n"
     ]
    }
   ],
   "source": [
    "# AGD Fixed Learning Rate - Loss Convergence Comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "agd_configs = [\n",
    "    {\"lr\": 0.01, \"momentum\": 0.9, \"color\": \"blue\", \"label\": \"LR 0.01, Mom 0.9\"},\n",
    "    {\"lr\": 0.01, \"momentum\": 0.95, \"color\": \"orange\", \"label\": \"LR 0.01, Mom 0.95\"},\n",
    "    {\"lr\": 0.1, \"momentum\": 0.9, \"color\": \"green\", \"label\": \"LR 0.1, Mom 0.9\"},\n",
    "    {\"lr\": 0.1, \"momentum\": 0.95, \"color\": \"red\", \"label\": \"LR 0.1, Mom 0.95\"},\n",
    "]\n",
    "\n",
    "for config in agd_configs:\n",
    "    model = WeightedLogisticRegression()\n",
    "    optimizer = AGDOptimizer(\n",
    "        learning_rate=config[\"lr\"],\n",
    "        momentum=config[\"momentum\"],\n",
    "        log_interval=20,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    result = optimizer.optimize(\n",
    "        model, X_train_scaled, y_train,\n",
    "        weights_train=weights_train\n",
    "    )\n",
    "    \n",
    "    plt.plot(result['epoch_history'], result['loss_history'], \n",
    "             color=config[\"color\"], linewidth=2, label=config[\"label\"])\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Training Loss', fontsize=12)\n",
    "plt.title('AGD Fixed Learning Rate - Loss Convergence', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(images_dir / \"agd_fixed_lr_convergence.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✅ AGD Fixed Learning Rate comparison saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1341b6cb",
   "metadata": {},
   "source": [
    "## AGD backtracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533f2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AGD with Backtracking Line Search\n",
      "Initial step size: 0.01, momentum: 0.9\n",
      "c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.444398 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 1.22s\n",
      "Epoch   20 | Train Loss: 0.444398 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 1.22s\n",
      "Epoch   40 | Train Loss: 0.417034 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 2.52s\n",
      "Epoch   40 | Train Loss: 0.417034 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 2.52s\n",
      "Epoch   60 | Train Loss: 0.408205 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 3.66s\n",
      "Epoch   60 | Train Loss: 0.408205 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 3.66s\n",
      "Epoch   80 | Train Loss: 0.402820 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 4.78s\n",
      "Epoch   80 | Train Loss: 0.402820 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 4.78s\n",
      "Epoch  100 | Train Loss: 0.399680 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 5.98s\n",
      "Epoch  100 | Train Loss: 0.399680 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 5.98s\n",
      "Epoch  120 | Train Loss: 0.397771 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 7.17s\n",
      "Epoch  120 | Train Loss: 0.397771 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 7.17s\n",
      "Epoch  140 | Train Loss: 0.396565 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 8.27s\n",
      "Epoch  140 | Train Loss: 0.396565 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 8.27s\n",
      "Epoch  160 | Train Loss: 0.395785 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 9.32s\n",
      "Epoch  160 | Train Loss: 0.395785 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 9.32s\n",
      "Epoch  180 | Train Loss: 0.395271 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 10.46s\n",
      "Epoch  180 | Train Loss: 0.395271 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 10.46s\n",
      "Epoch  200 | Train Loss: 0.394927 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 11.64s\n",
      "Epoch  200 | Train Loss: 0.394927 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 11.64s\n",
      "Epoch  220 | Train Loss: 0.394692 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 12.82s\n",
      "Epoch  220 | Train Loss: 0.394692 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 12.82s\n",
      "Epoch  240 | Train Loss: 0.394531 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 13.97s\n",
      "Epoch  240 | Train Loss: 0.394531 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 13.97s\n",
      "Epoch  260 | Train Loss: 0.394417 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 15.09s\n",
      "Epoch  260 | Train Loss: 0.394417 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 15.09s\n",
      "Epoch  280 | Train Loss: 0.394337 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 16.31s\n",
      "Epoch  280 | Train Loss: 0.394337 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 16.31s\n",
      "Epoch  300 | Train Loss: 0.394279 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 17.44s\n",
      "Epoch  300 | Train Loss: 0.394279 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 17.44s\n",
      "Epoch  320 | Train Loss: 0.394237 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 18.56s\n",
      "Epoch  320 | Train Loss: 0.394237 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 18.56s\n",
      "Epoch  340 | Train Loss: 0.394205 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 19.64s\n",
      "Epoch  340 | Train Loss: 0.394205 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 19.64s\n",
      "Epoch  360 | Train Loss: 0.394181 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 20.80s\n",
      "Epoch  360 | Train Loss: 0.394181 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 20.80s\n",
      "Epoch  380 | Train Loss: 0.394162 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 21.91s\n",
      "Epoch  380 | Train Loss: 0.394162 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 21.91s\n",
      "Epoch  400 | Train Loss: 0.394147 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 23.01s\n",
      "Epoch  400 | Train Loss: 0.394147 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 23.01s\n",
      "Epoch  420 | Train Loss: 0.394135 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 24.18s\n",
      "Epoch  420 | Train Loss: 0.394135 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 24.18s\n",
      "Epoch  440 | Train Loss: 0.394125 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 25.24s\n",
      "Epoch  440 | Train Loss: 0.394125 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 25.24s\n",
      "Epoch  460 | Train Loss: 0.394116 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 26.30s\n",
      "Epoch  460 | Train Loss: 0.394116 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 26.30s\n",
      "Epoch  480 | Train Loss: 0.394109 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 27.38s\n",
      "Epoch  480 | Train Loss: 0.394109 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 27.38s\n",
      "Epoch  500 | Train Loss: 0.394102 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 28.45s\n",
      "Epoch  500 | Train Loss: 0.394102 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 28.45s\n",
      "Epoch  520 | Train Loss: 0.394096 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 29.49s\n",
      "Epoch  520 | Train Loss: 0.394096 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 29.49s\n",
      "Epoch  540 | Train Loss: 0.394090 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 30.59s\n",
      "Epoch  540 | Train Loss: 0.394090 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 30.59s\n",
      "Epoch  560 | Train Loss: 0.394085 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 31.63s\n",
      "Epoch  560 | Train Loss: 0.394085 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 31.63s\n",
      "Epoch  580 | Train Loss: 0.394080 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 32.70s\n",
      "Epoch  580 | Train Loss: 0.394080 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 32.70s\n",
      "Epoch  600 | Train Loss: 0.394076 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 33.75s\n",
      "Epoch  600 | Train Loss: 0.394076 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 33.75s\n",
      "Epoch  620 | Train Loss: 0.394072 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 34.89s\n",
      "Epoch  620 | Train Loss: 0.394072 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 34.89s\n",
      "Epoch  640 | Train Loss: 0.394068 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 35.97s\n",
      "Epoch  640 | Train Loss: 0.394068 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 35.97s\n",
      "Epoch  660 | Train Loss: 0.394064 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 37.15s\n",
      "Epoch  660 | Train Loss: 0.394064 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 37.15s\n",
      "Epoch  680 | Train Loss: 0.394060 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 38.26s\n",
      "Epoch  680 | Train Loss: 0.394060 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 38.26s\n",
      "Epoch  700 | Train Loss: 0.394057 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 39.31s\n",
      "Epoch  700 | Train Loss: 0.394057 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 39.31s\n",
      "Epoch  720 | Train Loss: 0.394053 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 40.39s\n",
      "Epoch  720 | Train Loss: 0.394053 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 40.39s\n",
      "Epoch  740 | Train Loss: 0.394050 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 41.60s\n",
      "Epoch  740 | Train Loss: 0.394050 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 41.60s\n",
      "Epoch  760 | Train Loss: 0.394047 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 42.71s\n",
      "Epoch  760 | Train Loss: 0.394047 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 42.71s\n",
      "Epoch  780 | Train Loss: 0.394044 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 43.76s\n",
      "Epoch  780 | Train Loss: 0.394044 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 43.76s\n",
      "Epoch  800 | Train Loss: 0.394041 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 44.83s\n",
      "Epoch  800 | Train Loss: 0.394041 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 44.83s\n",
      "Epoch  820 | Train Loss: 0.394039 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 45.95s\n",
      "Epoch  820 | Train Loss: 0.394039 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 45.95s\n",
      "Epoch  840 | Train Loss: 0.394036 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 47.00s\n",
      "Epoch  840 | Train Loss: 0.394036 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 47.00s\n",
      "Epoch  860 | Train Loss: 0.394033 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 48.03s\n",
      "Epoch  860 | Train Loss: 0.394033 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 48.03s\n",
      "Epoch  880 | Train Loss: 0.394031 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 49.05s\n",
      "Epoch  880 | Train Loss: 0.394031 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 49.05s\n",
      "Epoch  900 | Train Loss: 0.394029 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 50.15s\n",
      "Epoch  900 | Train Loss: 0.394029 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 50.15s\n",
      "Epoch  920 | Train Loss: 0.394026 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 51.29s\n",
      "Epoch  920 | Train Loss: 0.394026 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 51.29s\n",
      "Epoch  940 | Train Loss: 0.394024 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 52.37s\n",
      "Epoch  940 | Train Loss: 0.394024 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 52.37s\n",
      "Epoch  960 | Train Loss: 0.394022 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 53.43s\n",
      "Epoch  960 | Train Loss: 0.394022 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 53.43s\n",
      "Epoch  980 | Train Loss: 0.394020 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 54.58s\n",
      "Epoch  980 | Train Loss: 0.394020 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 54.58s\n",
      "Epoch 1000 | Train Loss: 0.394018 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  7/50 | Time: 55.71s\n",
      "Epoch 1000 | Train Loss: 0.394018 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  7/50 | Time: 55.71s\n",
      "Epoch 1020 | Train Loss: 0.394017 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 56.79s\n",
      "Epoch 1020 | Train Loss: 0.394017 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 56.79s\n",
      "Epoch 1040 | Train Loss: 0.394015 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 57.91s\n",
      "Epoch 1040 | Train Loss: 0.394015 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 57.91s\n",
      "Epoch 1060 | Train Loss: 0.394013 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 59.13s\n",
      "Epoch 1060 | Train Loss: 0.394013 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 59.13s\n",
      "Epoch 1080 | Train Loss: 0.394011 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 60.31s\n",
      "Epoch 1080 | Train Loss: 0.394011 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 60.31s\n",
      "Epoch 1100 | Train Loss: 0.394010 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 11/50 | Time: 61.42s\n",
      "Epoch 1100 | Train Loss: 0.394010 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 11/50 | Time: 61.42s\n",
      "Epoch 1120 | Train Loss: 0.394008 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 62.52s\n",
      "Epoch 1120 | Train Loss: 0.394008 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 62.52s\n",
      "Epoch 1140 | Train Loss: 0.394007 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 63.67s\n",
      "Epoch 1140 | Train Loss: 0.394007 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 63.67s\n",
      "Epoch 1160 | Train Loss: 0.394005 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 64.81s\n",
      "Epoch 1160 | Train Loss: 0.394005 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 64.81s\n",
      "Epoch 1180 | Train Loss: 0.394004 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 65.94s\n",
      "Epoch 1180 | Train Loss: 0.394004 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  6/50 | Time: 65.94s\n",
      "Epoch 1200 | Train Loss: 0.394003 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 11/50 | Time: 67.17s\n",
      "Epoch 1200 | Train Loss: 0.394003 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 11/50 | Time: 67.17s\n",
      "Epoch 1220 | Train Loss: 0.394002 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 15/50 | Time: 68.56s\n",
      "Epoch 1220 | Train Loss: 0.394002 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 15/50 | Time: 68.56s\n",
      "Epoch 1240 | Train Loss: 0.394000 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 69.68s\n",
      "Epoch 1240 | Train Loss: 0.394000 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 69.68s\n",
      "Epoch 1260 | Train Loss: 0.393999 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 70.76s\n",
      "Epoch 1260 | Train Loss: 0.393999 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 70.76s\n",
      "Epoch 1280 | Train Loss: 0.393998 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  7/50 | Time: 71.84s\n",
      "Epoch 1280 | Train Loss: 0.393998 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  7/50 | Time: 71.84s\n",
      "Epoch 1300 | Train Loss: 0.393997 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 73.00s\n",
      "Epoch 1300 | Train Loss: 0.393997 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 73.00s\n",
      "Epoch 1320 | Train Loss: 0.393996 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 74.05s\n",
      "Epoch 1320 | Train Loss: 0.393996 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 74.05s\n",
      "Epoch 1340 | Train Loss: 0.393995 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 75.07s\n",
      "Epoch 1340 | Train Loss: 0.393995 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 75.07s\n",
      "Epoch 1360 | Train Loss: 0.393994 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 76.06s\n",
      "Epoch 1360 | Train Loss: 0.393994 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 76.06s\n",
      "Epoch 1380 | Train Loss: 0.393993 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 77.05s\n",
      "Epoch 1380 | Train Loss: 0.393993 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 77.05s\n",
      "Epoch 1400 | Train Loss: 0.393992 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  7/50 | Time: 78.14s\n",
      "Epoch 1400 | Train Loss: 0.393992 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  7/50 | Time: 78.14s\n",
      "Epoch 1420 | Train Loss: 0.393991 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 79.25s\n",
      "Epoch 1420 | Train Loss: 0.393991 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  4/50 | Time: 79.25s\n",
      "Epoch 1440 | Train Loss: 0.393990 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 80.30s\n",
      "Epoch 1440 | Train Loss: 0.393990 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 80.30s\n",
      "Epoch 1460 | Train Loss: 0.393989 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 20/50 | Time: 81.35s\n",
      "Epoch 1460 | Train Loss: 0.393989 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 20/50 | Time: 81.35s\n",
      "Epoch 1480 | Train Loss: 0.393989 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 15/50 | Time: 82.39s\n",
      "Epoch 1480 | Train Loss: 0.393989 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 15/50 | Time: 82.39s\n",
      "Epoch 1500 | Train Loss: 0.393988 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 83.58s\n",
      "Epoch 1500 | Train Loss: 0.393988 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  9/50 | Time: 83.58s\n",
      "Epoch 1520 | Train Loss: 0.393987 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 84.63s\n",
      "Epoch 1520 | Train Loss: 0.393987 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 84.63s\n",
      "Epoch 1540 | Train Loss: 0.393986 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 22/50 | Time: 85.69s\n",
      "Epoch 1540 | Train Loss: 0.393986 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 22/50 | Time: 85.69s\n",
      "Epoch 1560 | Train Loss: 0.393986 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 14/50 | Time: 86.76s\n",
      "Epoch 1560 | Train Loss: 0.393986 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 14/50 | Time: 86.76s\n",
      "Epoch 1580 | Train Loss: 0.393985 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 87.81s\n",
      "Epoch 1580 | Train Loss: 0.393985 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  5/50 | Time: 87.81s\n",
      "Epoch 1600 | Train Loss: 0.393984 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 25/50 | Time: 88.84s\n",
      "Epoch 1600 | Train Loss: 0.393984 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 25/50 | Time: 88.84s\n",
      "Epoch 1620 | Train Loss: 0.393984 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 14/50 | Time: 89.94s\n",
      "Epoch 1620 | Train Loss: 0.393984 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 14/50 | Time: 89.94s\n",
      "Epoch 1640 | Train Loss: 0.393983 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 90.98s\n",
      "Epoch 1640 | Train Loss: 0.393983 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 90.98s\n",
      "Epoch 1660 | Train Loss: 0.393982 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 21/50 | Time: 92.04s\n",
      "Epoch 1660 | Train Loss: 0.393982 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 21/50 | Time: 92.04s\n",
      "Epoch 1680 | Train Loss: 0.393982 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  7/50 | Time: 93.09s\n",
      "Epoch 1680 | Train Loss: 0.393982 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  7/50 | Time: 93.09s\n",
      "Epoch 1700 | Train Loss: 0.393981 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 27/50 | Time: 94.20s\n",
      "Epoch 1700 | Train Loss: 0.393981 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 27/50 | Time: 94.20s\n",
      "Epoch 1720 | Train Loss: 0.393981 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 11/50 | Time: 95.29s\n",
      "Epoch 1720 | Train Loss: 0.393981 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 11/50 | Time: 95.29s\n",
      "Epoch 1740 | Train Loss: 0.393980 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 31/50 | Time: 96.36s\n",
      "Epoch 1740 | Train Loss: 0.393980 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 31/50 | Time: 96.36s\n",
      "Epoch 1760 | Train Loss: 0.393980 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 12/50 | Time: 97.39s\n",
      "Epoch 1760 | Train Loss: 0.393980 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 12/50 | Time: 97.39s\n",
      "Epoch 1780 | Train Loss: 0.393979 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 32/50 | Time: 98.57s\n",
      "Epoch 1780 | Train Loss: 0.393979 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 32/50 | Time: 98.57s\n",
      "Epoch 1800 | Train Loss: 0.393979 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 11/50 | Time: 99.62s\n",
      "Epoch 1800 | Train Loss: 0.393979 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 11/50 | Time: 99.62s\n",
      "Epoch 1820 | Train Loss: 0.393978 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 31/50 | Time: 100.67s\n",
      "Epoch 1820 | Train Loss: 0.393978 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 31/50 | Time: 100.67s\n",
      "Epoch 1840 | Train Loss: 0.393978 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  7/50 | Time: 101.73s\n",
      "Epoch 1840 | Train Loss: 0.393978 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  7/50 | Time: 101.73s\n",
      "Epoch 1860 | Train Loss: 0.393977 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 27/50 | Time: 102.94s\n",
      "Epoch 1860 | Train Loss: 0.393977 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 27/50 | Time: 102.94s\n",
      "Epoch 1880 | Train Loss: 0.393977 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 47/50 | Time: 104.05s\n",
      "Epoch 1880 | Train Loss: 0.393977 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 47/50 | Time: 104.05s\n",
      "Epoch 1900 | Train Loss: 0.393977 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 19/50 | Time: 105.09s\n",
      "Epoch 1900 | Train Loss: 0.393977 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 19/50 | Time: 105.09s\n",
      "Epoch 1920 | Train Loss: 0.393976 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 39/50 | Time: 106.16s\n",
      "Epoch 1920 | Train Loss: 0.393976 | Step Size: 0.010000 | Avg Step: 0.010000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 39/50 | Time: 106.16s\n",
      "\n",
      "Early stopping triggered at epoch 1931\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393976\n",
      "Total training time: 106.84s\n",
      "Average time per epoch: 0.0553s\n",
      "Average step size: 0.010000\n",
      "Final step size: 0.010000\n",
      "Average backtrack iterations: 0.00\n",
      "Starting AGD with Backtracking Line Search\n",
      "Initial step size: 0.05, momentum: 0.9\n",
      "c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Early stopping triggered at epoch 1931\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393976\n",
      "Total training time: 106.84s\n",
      "Average time per epoch: 0.0553s\n",
      "Average step size: 0.010000\n",
      "Final step size: 0.010000\n",
      "Average backtrack iterations: 0.00\n",
      "Starting AGD with Backtracking Line Search\n",
      "Initial step size: 0.05, momentum: 0.9\n",
      "c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.417727 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 1.04s\n",
      "Epoch   20 | Train Loss: 0.417727 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 1.04s\n",
      "Epoch   40 | Train Loss: 0.395158 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 2.12s\n",
      "Epoch   40 | Train Loss: 0.395158 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 2.12s\n",
      "Epoch   60 | Train Loss: 0.394249 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 3.16s\n",
      "Epoch   60 | Train Loss: 0.394249 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 3.16s\n",
      "Epoch   80 | Train Loss: 0.394138 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 4.22s\n",
      "Epoch   80 | Train Loss: 0.394138 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 4.22s\n",
      "Epoch  100 | Train Loss: 0.394087 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 5.46s\n",
      "Epoch  100 | Train Loss: 0.394087 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 5.46s\n",
      "Epoch  120 | Train Loss: 0.394061 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 6.59s\n",
      "Epoch  120 | Train Loss: 0.394061 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 6.59s\n",
      "Epoch  140 | Train Loss: 0.394043 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 7.69s\n",
      "Epoch  140 | Train Loss: 0.394043 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 7.69s\n",
      "Epoch  160 | Train Loss: 0.394028 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 8.94s\n",
      "Epoch  160 | Train Loss: 0.394028 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 8.94s\n",
      "Epoch  180 | Train Loss: 0.394017 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 10.06s\n",
      "Epoch  180 | Train Loss: 0.394017 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 10.06s\n",
      "Epoch  200 | Train Loss: 0.394008 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 11.13s\n",
      "Epoch  200 | Train Loss: 0.394008 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 11.13s\n",
      "Epoch  220 | Train Loss: 0.394000 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 12.26s\n",
      "Epoch  220 | Train Loss: 0.394000 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 12.26s\n",
      "Epoch  240 | Train Loss: 0.393994 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 13.35s\n",
      "Epoch  240 | Train Loss: 0.393994 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 13.35s\n",
      "Epoch  260 | Train Loss: 0.393989 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 14.58s\n",
      "Epoch  260 | Train Loss: 0.393989 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 14.58s\n",
      "Epoch  280 | Train Loss: 0.393985 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 15.64s\n",
      "Epoch  280 | Train Loss: 0.393985 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 15.64s\n",
      "Epoch  300 | Train Loss: 0.393982 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 16.78s\n",
      "Epoch  300 | Train Loss: 0.393982 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 16.78s\n",
      "Epoch  320 | Train Loss: 0.393979 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  7/50 | Time: 18.00s\n",
      "Epoch  320 | Train Loss: 0.393979 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  7/50 | Time: 18.00s\n",
      "Epoch  340 | Train Loss: 0.393977 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 19.28s\n",
      "Epoch  340 | Train Loss: 0.393977 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  1/50 | Time: 19.28s\n",
      "Epoch  360 | Train Loss: 0.393975 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 20.40s\n",
      "Epoch  360 | Train Loss: 0.393975 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 20.40s\n",
      "Epoch  380 | Train Loss: 0.393973 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 21.54s\n",
      "Epoch  380 | Train Loss: 0.393973 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  8/50 | Time: 21.54s\n",
      "Epoch  400 | Train Loss: 0.393972 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 22.61s\n",
      "Epoch  400 | Train Loss: 0.393972 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 22.61s\n",
      "Epoch  420 | Train Loss: 0.393971 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 23.93s\n",
      "Epoch  420 | Train Loss: 0.393971 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 23.93s\n",
      "Epoch  440 | Train Loss: 0.393970 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 25.00s\n",
      "Epoch  440 | Train Loss: 0.393970 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 25.00s\n",
      "Epoch  460 | Train Loss: 0.393969 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 23/50 | Time: 26.10s\n",
      "Epoch  460 | Train Loss: 0.393969 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 23/50 | Time: 26.10s\n",
      "Epoch  480 | Train Loss: 0.393968 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 19/50 | Time: 27.30s\n",
      "Epoch  480 | Train Loss: 0.393968 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 19/50 | Time: 27.30s\n",
      "Epoch  500 | Train Loss: 0.393967 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 28.46s\n",
      "Epoch  500 | Train Loss: 0.393967 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 28.46s\n",
      "Epoch  520 | Train Loss: 0.393967 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 30/50 | Time: 29.65s\n",
      "Epoch  520 | Train Loss: 0.393967 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 30/50 | Time: 29.65s\n",
      "Epoch  540 | Train Loss: 0.393966 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 12/50 | Time: 30.76s\n",
      "Epoch  540 | Train Loss: 0.393966 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 12/50 | Time: 30.76s\n",
      "Epoch  560 | Train Loss: 0.393966 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 32/50 | Time: 31.83s\n",
      "Epoch  560 | Train Loss: 0.393966 | Step Size: 0.050000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 32/50 | Time: 31.83s\n",
      "\n",
      "Early stopping triggered at epoch 578\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393966\n",
      "Total training time: 32.84s\n",
      "Average time per epoch: 0.0568s\n",
      "Average step size: 0.050000\n",
      "Final step size: 0.050000\n",
      "Average backtrack iterations: 0.00\n",
      "Starting AGD with Backtracking Line Search\n",
      "Initial step size: 0.1, momentum: 0.9\n",
      "c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Early stopping triggered at epoch 578\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393966\n",
      "Total training time: 32.84s\n",
      "Average time per epoch: 0.0568s\n",
      "Average step size: 0.050000\n",
      "Final step size: 0.050000\n",
      "Average backtrack iterations: 0.00\n",
      "Starting AGD with Backtracking Line Search\n",
      "Initial step size: 0.1, momentum: 0.9\n",
      "c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.466736 | Step Size: 0.000000 | Avg Step: 0.040000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 12.0 | Patience: 11/50 | Time: 6.46s\n",
      "Epoch   20 | Train Loss: 0.466736 | Step Size: 0.000000 | Avg Step: 0.040000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 12.0 | Patience: 11/50 | Time: 6.46s\n",
      "Epoch   40 | Train Loss: 0.401117 | Step Size: 0.000000 | Avg Step: 0.075000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 5.0 | Patience:  3/50 | Time: 9.78s\n",
      "Epoch   40 | Train Loss: 0.401117 | Step Size: 0.000000 | Avg Step: 0.075000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 5.0 | Patience:  3/50 | Time: 9.78s\n",
      "Epoch   60 | Train Loss: 0.395183 | Step Size: 0.000000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 10.0 | Patience:  2/50 | Time: 14.81s\n",
      "Epoch   60 | Train Loss: 0.395183 | Step Size: 0.000000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 10.0 | Patience:  2/50 | Time: 14.81s\n",
      "Epoch   80 | Train Loss: 0.396066 | Step Size: 0.100000 | Avg Step: 0.045000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 11.0 | Patience: 22/50 | Time: 20.36s\n",
      "Epoch   80 | Train Loss: 0.396066 | Step Size: 0.100000 | Avg Step: 0.045000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 11.0 | Patience: 22/50 | Time: 20.36s\n",
      "Epoch  100 | Train Loss: 0.396745 | Step Size: 0.100000 | Avg Step: 0.040000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 12.0 | Patience: 16/50 | Time: 26.99s\n",
      "Epoch  100 | Train Loss: 0.396745 | Step Size: 0.100000 | Avg Step: 0.040000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 12.0 | Patience: 16/50 | Time: 26.99s\n",
      "Epoch  120 | Train Loss: 0.395986 | Step Size: 0.100000 | Avg Step: 0.035000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 13.0 | Patience: 13/50 | Time: 33.64s\n",
      "Epoch  120 | Train Loss: 0.395986 | Step Size: 0.100000 | Avg Step: 0.035000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 13.0 | Patience: 13/50 | Time: 33.64s\n",
      "Epoch  140 | Train Loss: 0.394620 | Step Size: 0.000000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 10.0 | Patience:  9/50 | Time: 38.87s\n",
      "Epoch  140 | Train Loss: 0.394620 | Step Size: 0.000000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 10.0 | Patience:  9/50 | Time: 38.87s\n",
      "Epoch  160 | Train Loss: 0.394158 | Step Size: 0.000000 | Avg Step: 0.055000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 9.0 | Patience:  5/50 | Time: 43.88s\n",
      "Epoch  160 | Train Loss: 0.394158 | Step Size: 0.000000 | Avg Step: 0.055000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 9.0 | Patience:  5/50 | Time: 43.88s\n",
      "Epoch  180 | Train Loss: 0.394022 | Step Size: 0.000000 | Avg Step: 0.055000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 9.0 | Patience:  1/50 | Time: 49.19s\n",
      "Epoch  180 | Train Loss: 0.394022 | Step Size: 0.000000 | Avg Step: 0.055000 | Momentum: 0.900 | Backtrack: 20 | Avg BT: 9.0 | Patience:  1/50 | Time: 49.19s\n",
      "Epoch  200 | Train Loss: 0.394027 | Step Size: 0.100000 | Avg Step: 0.045000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 11.0 | Patience: 21/50 | Time: 54.98s\n",
      "Epoch  200 | Train Loss: 0.394027 | Step Size: 0.100000 | Avg Step: 0.045000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 11.0 | Patience: 21/50 | Time: 54.98s\n",
      "Epoch  220 | Train Loss: 0.394040 | Step Size: 0.100000 | Avg Step: 0.040000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 12.0 | Patience: 17/50 | Time: 61.60s\n",
      "Epoch  220 | Train Loss: 0.394040 | Step Size: 0.100000 | Avg Step: 0.040000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 12.0 | Patience: 17/50 | Time: 61.60s\n",
      "Epoch  240 | Train Loss: 0.394022 | Step Size: 0.100000 | Avg Step: 0.045000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 11.0 | Patience: 14/50 | Time: 67.44s\n",
      "Epoch  240 | Train Loss: 0.394022 | Step Size: 0.100000 | Avg Step: 0.045000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 11.0 | Patience: 14/50 | Time: 67.44s\n",
      "Epoch  260 | Train Loss: 0.393999 | Step Size: 0.100000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 10.0 | Patience: 12/50 | Time: 72.81s\n",
      "Epoch  260 | Train Loss: 0.393999 | Step Size: 0.100000 | Avg Step: 0.050000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 10.0 | Patience: 12/50 | Time: 72.81s\n",
      "Epoch  280 | Train Loss: 0.393984 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 73.86s\n",
      "Epoch  280 | Train Loss: 0.393984 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 73.86s\n",
      "Epoch  300 | Train Loss: 0.393977 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 75.15s\n",
      "Epoch  300 | Train Loss: 0.393977 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 75.15s\n",
      "Epoch  320 | Train Loss: 0.393973 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 76.32s\n",
      "Epoch  320 | Train Loss: 0.393973 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  2/50 | Time: 76.32s\n",
      "Epoch  340 | Train Loss: 0.393970 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 77.34s\n",
      "Epoch  340 | Train Loss: 0.393970 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  0/50 | Time: 77.34s\n",
      "Epoch  360 | Train Loss: 0.393968 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 78.45s\n",
      "Epoch  360 | Train Loss: 0.393968 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 10/50 | Time: 78.45s\n",
      "Epoch  380 | Train Loss: 0.393967 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 79.67s\n",
      "Epoch  380 | Train Loss: 0.393967 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 79.67s\n",
      "Epoch  400 | Train Loss: 0.393966 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 80.85s\n",
      "Epoch  400 | Train Loss: 0.393966 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience:  3/50 | Time: 80.85s\n",
      "Epoch  420 | Train Loss: 0.393965 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 23/50 | Time: 82.00s\n",
      "Epoch  420 | Train Loss: 0.393965 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 23/50 | Time: 82.00s\n",
      "Epoch  440 | Train Loss: 0.393965 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 14/50 | Time: 83.08s\n",
      "Epoch  440 | Train Loss: 0.393965 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 14/50 | Time: 83.08s\n",
      "Epoch  460 | Train Loss: 0.393965 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 34/50 | Time: 84.19s\n",
      "Epoch  460 | Train Loss: 0.393965 | Step Size: 0.100000 | Avg Step: 0.100000 | Momentum: 0.900 | Backtrack:  0 | Avg BT: 0.0 | Patience: 34/50 | Time: 84.19s\n",
      "\n",
      "Early stopping triggered at epoch 476\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393964\n",
      "Total training time: 85.11s\n",
      "Average time per epoch: 0.1788s\n",
      "Average step size: 0.071639\n",
      "Final step size: 0.100000\n",
      "Average backtrack iterations: 5.67\n",
      "Starting AGD with Backtracking Line Search\n",
      "Initial step size: 0.1, momentum: 0.95\n",
      "c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Early stopping triggered at epoch 476\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.393964\n",
      "Total training time: 85.11s\n",
      "Average time per epoch: 0.1788s\n",
      "Average step size: 0.071639\n",
      "Final step size: 0.100000\n",
      "Average backtrack iterations: 5.67\n",
      "Starting AGD with Backtracking Line Search\n",
      "Initial step size: 0.1, momentum: 0.95\n",
      "c1: 0.0001, rho: 0.5\n",
      "Early stopping: patience=50, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch   20 | Train Loss: 0.558047 | Step Size: 0.000000 | Avg Step: 0.035000 | Momentum: 0.950 | Backtrack: 20 | Avg BT: 13.0 | Patience: 12/50 | Time: 6.45s\n",
      "Epoch   20 | Train Loss: 0.558047 | Step Size: 0.000000 | Avg Step: 0.035000 | Momentum: 0.950 | Backtrack: 20 | Avg BT: 13.0 | Patience: 12/50 | Time: 6.45s\n",
      "Epoch   40 | Train Loss: 0.713540 | Step Size: 0.100000 | Avg Step: 0.010000 | Momentum: 0.950 | Backtrack:  0 | Avg BT: 18.0 | Patience: 32/50 | Time: 15.68s\n",
      "Epoch   40 | Train Loss: 0.713540 | Step Size: 0.100000 | Avg Step: 0.010000 | Momentum: 0.950 | Backtrack:  0 | Avg BT: 18.0 | Patience: 32/50 | Time: 15.68s\n",
      "\n",
      "Early stopping triggered at epoch 58\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.450079\n",
      "Total training time: 17.98s\n",
      "Average time per epoch: 0.3099s\n",
      "Average step size: 0.041379\n",
      "Final step size: 0.000000\n",
      "Average backtrack iterations: 11.72\n",
      "\n",
      "Early stopping triggered at epoch 58\n",
      "No improvement in loss for 50 epochs (threshold: 1e-06)\n",
      "Final loss: 0.450079\n",
      "Total training time: 17.98s\n",
      "Average time per epoch: 0.3099s\n",
      "Average step size: 0.041379\n",
      "Final step size: 0.000000\n",
      "Average backtrack iterations: 11.72\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AGD Backtracking comparison saved!\n"
     ]
    }
   ],
   "source": [
    "# AGD Backtracking - Loss Convergence Comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "agd_bt_configs = [\n",
    "    {\"init_lr\": 0.01, \"momentum\": 0.9, \"color\": \"blue\", \"label\": \"Init 0.01, Mom 0.9\"},\n",
    "    {\"init_lr\": 0.05, \"momentum\": 0.9, \"color\": \"orange\", \"label\": \"Init 0.05, Mom 0.9\"},\n",
    "    {\"init_lr\": 0.1, \"momentum\": 0.9, \"color\": \"red\", \"label\": \"Init 0.1, Mom 0.9\"},\n",
    "    {\"init_lr\": 0.1, \"momentum\": 0.95, \"color\": \"green\", \"label\": \"Init 0.1, Mom 0.95\"},\n",
    "]\n",
    "\n",
    "for config in agd_bt_configs:\n",
    "    model = WeightedLogisticRegression()\n",
    "    optimizer = AGDBacktrackingOptimizer(\n",
    "        initial_learning_rate=config[\"init_lr\"],\n",
    "        momentum=config[\"momentum\"],\n",
    "        log_interval=20,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    result = optimizer.optimize(\n",
    "        model, X_train_scaled, y_train,\n",
    "        weights_train=weights_train\n",
    "    )\n",
    "    \n",
    "    plt.plot(result['epoch_history'], result['loss_history'], \n",
    "             color=config[\"color\"], linewidth=2, label=config[\"label\"])\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Training Loss', fontsize=12)\n",
    "plt.title('AGD Backtracking - Loss Convergence', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(images_dir / \"agd_backtracking_convergence.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✅ AGD Backtracking comparison saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb9eb7",
   "metadata": {},
   "source": [
    "## SGD iterations (mini-batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9411a3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SGD Iterations training\n",
      "Learning rate: 0.01, Batch size: 1\n",
      "Max iterations: 100000\n",
      "Early stopping: patience=500, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "Initial full dataset loss: 0.684106\n",
      "----------------------------------------------------------------------\n",
      "Iter    20 | Train Loss: 0.600300 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 86.5 iter/s | Time: 0.23s\n",
      "Iter    40 | Train Loss: 0.541488 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 95.2 iter/s | Time: 0.42s\n",
      "Iter    60 | Train Loss: 0.514474 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 96.0 iter/s | Time: 0.62s\n",
      "Iter    80 | Train Loss: 0.492335 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 95.8 iter/s | Time: 0.83s\n",
      "Iter   100 | Train Loss: 0.479825 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 97.0 iter/s | Time: 1.03s\n",
      "Iter   120 | Train Loss: 0.472780 | LR: 0.0100 | Batch:   1 | Patience:   1/500 | Rate: 97.8 iter/s | Time: 1.23s\n",
      "Iter   140 | Train Loss: 0.484778 | LR: 0.0100 | Batch:   1 | Patience:   2/500 | Rate: 90.1 iter/s | Time: 1.55s\n",
      "Iter   160 | Train Loss: 0.483368 | LR: 0.0100 | Batch:   1 | Patience:   6/500 | Rate: 91.6 iter/s | Time: 1.75s\n",
      "Iter   180 | Train Loss: 0.464747 | LR: 0.0100 | Batch:   1 | Patience:  10/500 | Rate: 92.6 iter/s | Time: 1.94s\n",
      "Iter   200 | Train Loss: 0.449816 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 93.4 iter/s | Time: 2.14s\n",
      "Iter   220 | Train Loss: 0.444395 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 94.2 iter/s | Time: 2.33s\n",
      "Iter   240 | Train Loss: 0.443546 | LR: 0.0100 | Batch:   1 | Patience:   1/500 | Rate: 94.8 iter/s | Time: 2.53s\n",
      "Iter   260 | Train Loss: 0.433611 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 93.8 iter/s | Time: 2.77s\n",
      "Iter   280 | Train Loss: 0.429502 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 94.1 iter/s | Time: 2.98s\n",
      "Iter   300 | Train Loss: 0.426555 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 94.5 iter/s | Time: 3.17s\n",
      "Iter   320 | Train Loss: 0.423894 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 95.0 iter/s | Time: 3.37s\n",
      "Iter   340 | Train Loss: 0.424974 | LR: 0.0100 | Batch:   1 | Patience:   2/500 | Rate: 95.2 iter/s | Time: 3.57s\n",
      "Iter   360 | Train Loss: 0.421330 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 95.7 iter/s | Time: 3.76s\n",
      "Iter   380 | Train Loss: 0.422604 | LR: 0.0100 | Batch:   1 | Patience:   3/500 | Rate: 95.9 iter/s | Time: 3.96s\n",
      "Iter   400 | Train Loss: 0.422203 | LR: 0.0100 | Batch:   1 | Patience:   7/500 | Rate: 95.6 iter/s | Time: 4.18s\n",
      "Iter   420 | Train Loss: 0.420575 | LR: 0.0100 | Batch:   1 | Patience:   1/500 | Rate: 95.9 iter/s | Time: 4.38s\n",
      "Iter   440 | Train Loss: 0.418793 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 96.3 iter/s | Time: 4.57s\n",
      "Iter   460 | Train Loss: 0.420990 | LR: 0.0100 | Batch:   1 | Patience:   3/500 | Rate: 96.4 iter/s | Time: 4.77s\n",
      "Iter   480 | Train Loss: 0.419233 | LR: 0.0100 | Batch:   1 | Patience:   7/500 | Rate: 96.7 iter/s | Time: 4.97s\n",
      "Iter   500 | Train Loss: 0.419593 | LR: 0.0100 | Batch:   1 | Patience:  11/500 | Rate: 96.7 iter/s | Time: 5.17s\n",
      "Iter   520 | Train Loss: 0.417855 | LR: 0.0100 | Batch:   1 | Patience:   1/500 | Rate: 97.0 iter/s | Time: 5.36s\n",
      "Iter   540 | Train Loss: 0.416570 | LR: 0.0100 | Batch:   1 | Patience:   1/500 | Rate: 97.2 iter/s | Time: 5.55s\n",
      "Iter   560 | Train Loss: 0.417530 | LR: 0.0100 | Batch:   1 | Patience:   3/500 | Rate: 97.5 iter/s | Time: 5.74s\n",
      "Iter   580 | Train Loss: 0.415612 | LR: 0.0100 | Batch:   1 | Patience:   2/500 | Rate: 97.7 iter/s | Time: 5.94s\n",
      "Iter   600 | Train Loss: 0.414094 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 97.8 iter/s | Time: 6.13s\n",
      "Iter   620 | Train Loss: 0.413543 | LR: 0.0100 | Batch:   1 | Patience:   3/500 | Rate: 97.8 iter/s | Time: 6.34s\n",
      "Iter   640 | Train Loss: 0.414202 | LR: 0.0100 | Batch:   1 | Patience:   3/500 | Rate: 96.9 iter/s | Time: 6.61s\n",
      "Iter   660 | Train Loss: 0.414260 | LR: 0.0100 | Batch:   1 | Patience:   7/500 | Rate: 96.6 iter/s | Time: 6.83s\n",
      "Iter   680 | Train Loss: 0.411840 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 96.6 iter/s | Time: 7.04s\n",
      "Iter   700 | Train Loss: 0.413463 | LR: 0.0100 | Batch:   1 | Patience:   3/500 | Rate: 96.6 iter/s | Time: 7.24s\n",
      "Iter   720 | Train Loss: 0.411050 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 96.5 iter/s | Time: 7.46s\n",
      "Iter   740 | Train Loss: 0.409432 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 96.5 iter/s | Time: 7.67s\n",
      "Iter   760 | Train Loss: 0.406933 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 96.6 iter/s | Time: 7.87s\n",
      "Iter   780 | Train Loss: 0.405804 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 96.5 iter/s | Time: 8.08s\n",
      "Iter   800 | Train Loss: 0.406625 | LR: 0.0100 | Batch:   1 | Patience:   3/500 | Rate: 95.7 iter/s | Time: 8.36s\n",
      "Iter   820 | Train Loss: 0.405127 | LR: 0.0100 | Batch:   1 | Patience:   1/500 | Rate: 95.0 iter/s | Time: 8.63s\n",
      "Iter   840 | Train Loss: 0.405238 | LR: 0.0100 | Batch:   1 | Patience:   1/500 | Rate: 95.0 iter/s | Time: 8.84s\n",
      "Iter   860 | Train Loss: 0.405497 | LR: 0.0100 | Batch:   1 | Patience:   5/500 | Rate: 95.1 iter/s | Time: 9.04s\n",
      "Iter   880 | Train Loss: 0.404711 | LR: 0.0100 | Batch:   1 | Patience:   9/500 | Rate: 95.1 iter/s | Time: 9.26s\n",
      "Iter   900 | Train Loss: 0.404862 | LR: 0.0100 | Batch:   1 | Patience:   1/500 | Rate: 95.0 iter/s | Time: 9.47s\n",
      "Iter   920 | Train Loss: 0.404169 | LR: 0.0100 | Batch:   1 | Patience:   1/500 | Rate: 94.8 iter/s | Time: 9.71s\n",
      "Iter   940 | Train Loss: 0.402832 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 94.9 iter/s | Time: 9.90s\n",
      "Iter   960 | Train Loss: 0.401688 | LR: 0.0100 | Batch:   1 | Patience:   1/500 | Rate: 95.0 iter/s | Time: 10.10s\n",
      "Iter   980 | Train Loss: 0.404129 | LR: 0.0100 | Batch:   1 | Patience:   3/500 | Rate: 94.9 iter/s | Time: 10.33s\n",
      "Iter  1000 | Train Loss: 0.404112 | LR: 0.0100 | Batch:   1 | Patience:   7/500 | Rate: 94.9 iter/s | Time: 10.54s\n",
      "Iter  1020 | Train Loss: 0.402498 | LR: 0.0100 | Batch:   1 | Patience:  11/500 | Rate: 94.9 iter/s | Time: 10.75s\n",
      "Iter  1040 | Train Loss: 0.402714 | LR: 0.0100 | Batch:   1 | Patience:  15/500 | Rate: 94.7 iter/s | Time: 10.98s\n",
      "Iter  1060 | Train Loss: 0.404915 | LR: 0.0100 | Batch:   1 | Patience:  19/500 | Rate: 94.6 iter/s | Time: 11.20s\n",
      "Iter  1080 | Train Loss: 0.406080 | LR: 0.0100 | Batch:   1 | Patience:  23/500 | Rate: 94.6 iter/s | Time: 11.41s\n",
      "Iter  1100 | Train Loss: 0.407434 | LR: 0.0100 | Batch:   1 | Patience:  27/500 | Rate: 94.6 iter/s | Time: 11.62s\n",
      "Iter  1120 | Train Loss: 0.407276 | LR: 0.0100 | Batch:   1 | Patience:  31/500 | Rate: 94.6 iter/s | Time: 11.84s\n",
      "Iter  1140 | Train Loss: 0.405510 | LR: 0.0100 | Batch:   1 | Patience:  35/500 | Rate: 94.6 iter/s | Time: 12.05s\n",
      "Iter  1160 | Train Loss: 0.404342 | LR: 0.0100 | Batch:   1 | Patience:  39/500 | Rate: 94.5 iter/s | Time: 12.27s\n",
      "Iter  1180 | Train Loss: 0.402840 | LR: 0.0100 | Batch:   1 | Patience:  43/500 | Rate: 94.5 iter/s | Time: 12.49s\n",
      "Iter  1200 | Train Loss: 0.401935 | LR: 0.0100 | Batch:   1 | Patience:  47/500 | Rate: 94.4 iter/s | Time: 12.71s\n",
      "Iter  1220 | Train Loss: 0.401076 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 94.4 iter/s | Time: 12.92s\n",
      "Iter  1240 | Train Loss: 0.400513 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 94.5 iter/s | Time: 13.12s\n",
      "Iter  1260 | Train Loss: 0.401844 | LR: 0.0100 | Batch:   1 | Patience:   3/500 | Rate: 94.6 iter/s | Time: 13.33s\n",
      "Iter  1280 | Train Loss: 0.402488 | LR: 0.0100 | Batch:   1 | Patience:   7/500 | Rate: 94.5 iter/s | Time: 13.54s\n",
      "Iter  1300 | Train Loss: 0.400705 | LR: 0.0100 | Batch:   1 | Patience:  11/500 | Rate: 94.6 iter/s | Time: 13.75s\n",
      "Iter  1320 | Train Loss: 0.400155 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 94.6 iter/s | Time: 13.95s\n",
      "Iter  1340 | Train Loss: 0.399886 | LR: 0.0100 | Batch:   1 | Patience:   1/500 | Rate: 94.6 iter/s | Time: 14.16s\n",
      "Iter  1360 | Train Loss: 0.399571 | LR: 0.0100 | Batch:   1 | Patience:   5/500 | Rate: 94.7 iter/s | Time: 14.37s\n",
      "Iter  1380 | Train Loss: 0.399457 | LR: 0.0100 | Batch:   1 | Patience:   2/500 | Rate: 94.7 iter/s | Time: 14.58s\n",
      "Iter  1400 | Train Loss: 0.399913 | LR: 0.0100 | Batch:   1 | Patience:   6/500 | Rate: 94.7 iter/s | Time: 14.78s\n",
      "Iter  1420 | Train Loss: 0.400554 | LR: 0.0100 | Batch:   1 | Patience:  10/500 | Rate: 94.7 iter/s | Time: 14.99s\n",
      "Iter  1440 | Train Loss: 0.401456 | LR: 0.0100 | Batch:   1 | Patience:  14/500 | Rate: 94.8 iter/s | Time: 15.19s\n",
      "Iter  1460 | Train Loss: 0.401659 | LR: 0.0100 | Batch:   1 | Patience:  18/500 | Rate: 94.8 iter/s | Time: 15.40s\n",
      "Iter  1480 | Train Loss: 0.401808 | LR: 0.0100 | Batch:   1 | Patience:  22/500 | Rate: 94.8 iter/s | Time: 15.61s\n",
      "Iter  1500 | Train Loss: 0.400993 | LR: 0.0100 | Batch:   1 | Patience:  26/500 | Rate: 94.9 iter/s | Time: 15.81s\n",
      "Iter  1520 | Train Loss: 0.401158 | LR: 0.0100 | Batch:   1 | Patience:  30/500 | Rate: 94.9 iter/s | Time: 16.02s\n",
      "Iter  1540 | Train Loss: 0.401142 | LR: 0.0100 | Batch:   1 | Patience:  34/500 | Rate: 94.9 iter/s | Time: 16.22s\n",
      "Iter  1560 | Train Loss: 0.401131 | LR: 0.0100 | Batch:   1 | Patience:  38/500 | Rate: 94.9 iter/s | Time: 16.44s\n",
      "Iter  1580 | Train Loss: 0.402776 | LR: 0.0100 | Batch:   1 | Patience:  42/500 | Rate: 95.0 iter/s | Time: 16.64s\n",
      "Iter  1600 | Train Loss: 0.401800 | LR: 0.0100 | Batch:   1 | Patience:  46/500 | Rate: 95.0 iter/s | Time: 16.84s\n",
      "Iter  1620 | Train Loss: 0.402097 | LR: 0.0100 | Batch:   1 | Patience:  50/500 | Rate: 95.0 iter/s | Time: 17.05s\n",
      "Iter  1640 | Train Loss: 0.402342 | LR: 0.0100 | Batch:   1 | Patience:  54/500 | Rate: 95.0 iter/s | Time: 17.26s\n",
      "Iter  1660 | Train Loss: 0.399927 | LR: 0.0100 | Batch:   1 | Patience:  58/500 | Rate: 95.0 iter/s | Time: 17.47s\n",
      "Iter  1680 | Train Loss: 0.400608 | LR: 0.0100 | Batch:   1 | Patience:  62/500 | Rate: 95.0 iter/s | Time: 17.68s\n",
      "Iter  1700 | Train Loss: 0.399529 | LR: 0.0100 | Batch:   1 | Patience:  66/500 | Rate: 94.8 iter/s | Time: 17.93s\n",
      "Iter  1720 | Train Loss: 0.399201 | LR: 0.0100 | Batch:   1 | Patience:  70/500 | Rate: 94.7 iter/s | Time: 18.16s\n",
      "Iter  1740 | Train Loss: 0.397968 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 94.8 iter/s | Time: 18.35s\n",
      "Iter  1760 | Train Loss: 0.397148 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 94.9 iter/s | Time: 18.55s\n",
      "Iter  1780 | Train Loss: 0.397851 | LR: 0.0100 | Batch:   1 | Patience:   3/500 | Rate: 95.0 iter/s | Time: 18.74s\n",
      "Iter  1800 | Train Loss: 0.397270 | LR: 0.0100 | Batch:   1 | Patience:   7/500 | Rate: 95.1 iter/s | Time: 18.93s\n",
      "Iter  1820 | Train Loss: 0.398051 | LR: 0.0100 | Batch:   1 | Patience:  11/500 | Rate: 95.2 iter/s | Time: 19.13s\n",
      "Iter  1840 | Train Loss: 0.398106 | LR: 0.0100 | Batch:   1 | Patience:  15/500 | Rate: 95.2 iter/s | Time: 19.32s\n",
      "Iter  1860 | Train Loss: 0.398912 | LR: 0.0100 | Batch:   1 | Patience:  19/500 | Rate: 95.3 iter/s | Time: 19.53s\n",
      "Iter  1880 | Train Loss: 0.399058 | LR: 0.0100 | Batch:   1 | Patience:  23/500 | Rate: 95.3 iter/s | Time: 19.72s\n",
      "Iter  1900 | Train Loss: 0.399723 | LR: 0.0100 | Batch:   1 | Patience:  27/500 | Rate: 95.4 iter/s | Time: 19.92s\n",
      "Iter  1920 | Train Loss: 0.399126 | LR: 0.0100 | Batch:   1 | Patience:  31/500 | Rate: 95.5 iter/s | Time: 20.11s\n",
      "Iter  1940 | Train Loss: 0.398072 | LR: 0.0100 | Batch:   1 | Patience:  35/500 | Rate: 95.6 iter/s | Time: 20.30s\n",
      "Iter  1960 | Train Loss: 0.398693 | LR: 0.0100 | Batch:   1 | Patience:  39/500 | Rate: 95.6 iter/s | Time: 20.50s\n",
      "Iter  1980 | Train Loss: 0.398475 | LR: 0.0100 | Batch:   1 | Patience:  43/500 | Rate: 95.7 iter/s | Time: 20.69s\n",
      "Iter  2000 | Train Loss: 0.398694 | LR: 0.0100 | Batch:   1 | Patience:  47/500 | Rate: 95.8 iter/s | Time: 20.89s\n",
      "Iter  2020 | Train Loss: 0.396885 | LR: 0.0100 | Batch:   1 | Patience:  51/500 | Rate: 95.8 iter/s | Time: 21.08s\n",
      "Iter  2040 | Train Loss: 0.396986 | LR: 0.0100 | Batch:   1 | Patience:   3/500 | Rate: 95.9 iter/s | Time: 21.28s\n",
      "Iter  2060 | Train Loss: 0.397411 | LR: 0.0100 | Batch:   1 | Patience:   7/500 | Rate: 95.9 iter/s | Time: 21.47s\n",
      "Iter  2080 | Train Loss: 0.398020 | LR: 0.0100 | Batch:   1 | Patience:  11/500 | Rate: 96.0 iter/s | Time: 21.66s\n",
      "Iter  2100 | Train Loss: 0.399158 | LR: 0.0100 | Batch:   1 | Patience:  15/500 | Rate: 96.1 iter/s | Time: 21.86s\n",
      "Iter  2120 | Train Loss: 0.399250 | LR: 0.0100 | Batch:   1 | Patience:  19/500 | Rate: 96.2 iter/s | Time: 22.05s\n",
      "Iter  2140 | Train Loss: 0.398384 | LR: 0.0100 | Batch:   1 | Patience:  23/500 | Rate: 96.2 iter/s | Time: 22.24s\n",
      "Iter  2160 | Train Loss: 0.399120 | LR: 0.0100 | Batch:   1 | Patience:  27/500 | Rate: 96.3 iter/s | Time: 22.44s\n",
      "Iter  2180 | Train Loss: 0.398438 | LR: 0.0100 | Batch:   1 | Patience:  31/500 | Rate: 96.3 iter/s | Time: 22.63s\n",
      "Iter  2200 | Train Loss: 0.398156 | LR: 0.0100 | Batch:   1 | Patience:  35/500 | Rate: 96.4 iter/s | Time: 22.82s\n",
      "Iter  2220 | Train Loss: 0.398708 | LR: 0.0100 | Batch:   1 | Patience:  39/500 | Rate: 96.4 iter/s | Time: 23.02s\n",
      "Iter  2240 | Train Loss: 0.398377 | LR: 0.0100 | Batch:   1 | Patience:  43/500 | Rate: 96.5 iter/s | Time: 23.21s\n",
      "Iter  2260 | Train Loss: 0.399086 | LR: 0.0100 | Batch:   1 | Patience:  47/500 | Rate: 96.6 iter/s | Time: 23.40s\n",
      "Iter  2280 | Train Loss: 0.398310 | LR: 0.0100 | Batch:   1 | Patience:  51/500 | Rate: 96.6 iter/s | Time: 23.60s\n",
      "Iter  2300 | Train Loss: 0.398223 | LR: 0.0100 | Batch:   1 | Patience:  55/500 | Rate: 96.7 iter/s | Time: 23.79s\n",
      "Iter  2320 | Train Loss: 0.397869 | LR: 0.0100 | Batch:   1 | Patience:  59/500 | Rate: 96.7 iter/s | Time: 23.98s\n",
      "Iter  2340 | Train Loss: 0.398094 | LR: 0.0100 | Batch:   1 | Patience:  63/500 | Rate: 96.8 iter/s | Time: 24.18s\n",
      "Iter  2360 | Train Loss: 0.398074 | LR: 0.0100 | Batch:   1 | Patience:  67/500 | Rate: 96.8 iter/s | Time: 24.37s\n",
      "Iter  2380 | Train Loss: 0.398702 | LR: 0.0100 | Batch:   1 | Patience:  71/500 | Rate: 96.9 iter/s | Time: 24.56s\n",
      "Iter  2400 | Train Loss: 0.399036 | LR: 0.0100 | Batch:   1 | Patience:  75/500 | Rate: 96.9 iter/s | Time: 24.76s\n",
      "Iter  2420 | Train Loss: 0.398609 | LR: 0.0100 | Batch:   1 | Patience:  79/500 | Rate: 97.0 iter/s | Time: 24.96s\n",
      "Iter  2440 | Train Loss: 0.398223 | LR: 0.0100 | Batch:   1 | Patience:  83/500 | Rate: 97.0 iter/s | Time: 25.15s\n",
      "Iter  2460 | Train Loss: 0.398628 | LR: 0.0100 | Batch:   1 | Patience:  87/500 | Rate: 97.0 iter/s | Time: 25.35s\n",
      "Iter  2480 | Train Loss: 0.397988 | LR: 0.0100 | Batch:   1 | Patience:  91/500 | Rate: 97.1 iter/s | Time: 25.55s\n",
      "Iter  2500 | Train Loss: 0.399032 | LR: 0.0100 | Batch:   1 | Patience:  95/500 | Rate: 97.1 iter/s | Time: 25.75s\n",
      "Iter  2520 | Train Loss: 0.398066 | LR: 0.0100 | Batch:   1 | Patience:  99/500 | Rate: 97.1 iter/s | Time: 25.95s\n",
      "Iter  2540 | Train Loss: 0.398107 | LR: 0.0100 | Batch:   1 | Patience: 103/500 | Rate: 97.1 iter/s | Time: 26.15s\n",
      "Iter  2560 | Train Loss: 0.398286 | LR: 0.0100 | Batch:   1 | Patience: 107/500 | Rate: 97.2 iter/s | Time: 26.35s\n",
      "Iter  2580 | Train Loss: 0.397721 | LR: 0.0100 | Batch:   1 | Patience: 111/500 | Rate: 97.2 iter/s | Time: 26.55s\n",
      "Iter  2600 | Train Loss: 0.396999 | LR: 0.0100 | Batch:   1 | Patience: 115/500 | Rate: 97.2 iter/s | Time: 26.75s\n",
      "Iter  2620 | Train Loss: 0.397805 | LR: 0.0100 | Batch:   1 | Patience:   1/500 | Rate: 97.2 iter/s | Time: 26.95s\n",
      "Iter  2640 | Train Loss: 0.397633 | LR: 0.0100 | Batch:   1 | Patience:   5/500 | Rate: 97.2 iter/s | Time: 27.15s\n",
      "Iter  2660 | Train Loss: 0.398196 | LR: 0.0100 | Batch:   1 | Patience:   9/500 | Rate: 97.2 iter/s | Time: 27.35s\n",
      "Iter  2680 | Train Loss: 0.397363 | LR: 0.0100 | Batch:   1 | Patience:  13/500 | Rate: 97.3 iter/s | Time: 27.55s\n",
      "Iter  2700 | Train Loss: 0.397599 | LR: 0.0100 | Batch:   1 | Patience:  17/500 | Rate: 97.3 iter/s | Time: 27.75s\n",
      "Iter  2720 | Train Loss: 0.396779 | LR: 0.0100 | Batch:   1 | Patience:  21/500 | Rate: 97.3 iter/s | Time: 27.95s\n",
      "Iter  2740 | Train Loss: 0.396537 | LR: 0.0100 | Batch:   1 | Patience:   1/500 | Rate: 97.3 iter/s | Time: 28.16s\n",
      "Iter  2760 | Train Loss: 0.396423 | LR: 0.0100 | Batch:   1 | Patience:   1/500 | Rate: 97.3 iter/s | Time: 28.35s\n",
      "Iter  2780 | Train Loss: 0.396014 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 97.4 iter/s | Time: 28.56s\n",
      "Iter  2800 | Train Loss: 0.395733 | LR: 0.0100 | Batch:   1 | Patience:   0/500 | Rate: 97.4 iter/s | Time: 28.76s\n",
      "Iter  2820 | Train Loss: 0.395989 | LR: 0.0100 | Batch:   1 | Patience:   4/500 | Rate: 97.4 iter/s | Time: 28.96s\n",
      "Iter  2840 | Train Loss: 0.396467 | LR: 0.0100 | Batch:   1 | Patience:   8/500 | Rate: 97.4 iter/s | Time: 29.16s\n",
      "Iter  2860 | Train Loss: 0.397021 | LR: 0.0100 | Batch:   1 | Patience:  12/500 | Rate: 97.4 iter/s | Time: 29.36s\n",
      "Iter  2880 | Train Loss: 0.396824 | LR: 0.0100 | Batch:   1 | Patience:  16/500 | Rate: 97.4 iter/s | Time: 29.56s\n",
      "Iter  2900 | Train Loss: 0.396836 | LR: 0.0100 | Batch:   1 | Patience:  20/500 | Rate: 97.4 iter/s | Time: 29.76s\n",
      "Iter  2920 | Train Loss: 0.396390 | LR: 0.0100 | Batch:   1 | Patience:  24/500 | Rate: 97.5 iter/s | Time: 29.96s\n",
      "Iter  2940 | Train Loss: 0.396459 | LR: 0.0100 | Batch:   1 | Patience:  28/500 | Rate: 97.5 iter/s | Time: 30.16s\n",
      "Iter  2960 | Train Loss: 0.396816 | LR: 0.0100 | Batch:   1 | Patience:  32/500 | Rate: 97.5 iter/s | Time: 30.36s\n",
      "Iter  2980 | Train Loss: 0.397767 | LR: 0.0100 | Batch:   1 | Patience:  36/500 | Rate: 97.5 iter/s | Time: 30.56s\n",
      "Iter  3000 | Train Loss: 0.397505 | LR: 0.0100 | Batch:   1 | Patience:  40/500 | Rate: 97.5 iter/s | Time: 30.76s\n",
      "Iter  3020 | Train Loss: 0.397892 | LR: 0.0100 | Batch:   1 | Patience:  44/500 | Rate: 97.5 iter/s | Time: 30.96s\n",
      "Iter  3040 | Train Loss: 0.397071 | LR: 0.0100 | Batch:   1 | Patience:  48/500 | Rate: 97.6 iter/s | Time: 31.16s\n",
      "Iter  3060 | Train Loss: 0.396921 | LR: 0.0100 | Batch:   1 | Patience:  52/500 | Rate: 97.6 iter/s | Time: 31.37s\n",
      "Iter  3080 | Train Loss: 0.396890 | LR: 0.0100 | Batch:   1 | Patience:  56/500 | Rate: 97.6 iter/s | Time: 31.56s\n",
      "Iter  3100 | Train Loss: 0.396539 | LR: 0.0100 | Batch:   1 | Patience:  60/500 | Rate: 97.6 iter/s | Time: 31.77s\n",
      "Iter  3120 | Train Loss: 0.397009 | LR: 0.0100 | Batch:   1 | Patience:  64/500 | Rate: 97.6 iter/s | Time: 31.96s\n",
      "Iter  3140 | Train Loss: 0.396487 | LR: 0.0100 | Batch:   1 | Patience:  68/500 | Rate: 97.6 iter/s | Time: 32.17s\n",
      "Iter  3160 | Train Loss: 0.396758 | LR: 0.0100 | Batch:   1 | Patience:  72/500 | Rate: 97.6 iter/s | Time: 32.36s\n",
      "Iter  3180 | Train Loss: 0.397980 | LR: 0.0100 | Batch:   1 | Patience:  76/500 | Rate: 97.6 iter/s | Time: 32.57s\n",
      "Iter  3200 | Train Loss: 0.398080 | LR: 0.0100 | Batch:   1 | Patience:  80/500 | Rate: 97.7 iter/s | Time: 32.77s\n",
      "Iter  3220 | Train Loss: 0.397858 | LR: 0.0100 | Batch:   1 | Patience:  84/500 | Rate: 97.7 iter/s | Time: 32.97s\n",
      "Iter  3240 | Train Loss: 0.397141 | LR: 0.0100 | Batch:   1 | Patience:  88/500 | Rate: 97.7 iter/s | Time: 33.17s\n",
      "Iter  3260 | Train Loss: 0.396887 | LR: 0.0100 | Batch:   1 | Patience:  92/500 | Rate: 97.7 iter/s | Time: 33.38s\n",
      "Iter  3280 | Train Loss: 0.396697 | LR: 0.0100 | Batch:   1 | Patience:  96/500 | Rate: 97.7 iter/s | Time: 33.58s\n",
      "Iter  3300 | Train Loss: 0.396841 | LR: 0.0100 | Batch:   1 | Patience: 100/500 | Rate: 97.7 iter/s | Time: 33.79s\n",
      "Iter  3320 | Train Loss: 0.397540 | LR: 0.0100 | Batch:   1 | Patience: 104/500 | Rate: 97.7 iter/s | Time: 33.99s\n",
      "Iter  3340 | Train Loss: 0.397835 | LR: 0.0100 | Batch:   1 | Patience: 108/500 | Rate: 97.7 iter/s | Time: 34.20s\n",
      "Iter  3360 | Train Loss: 0.397486 | LR: 0.0100 | Batch:   1 | Patience: 112/500 | Rate: 97.7 iter/s | Time: 34.40s\n",
      "Iter  3380 | Train Loss: 0.396784 | LR: 0.0100 | Batch:   1 | Patience: 116/500 | Rate: 97.7 iter/s | Time: 34.60s\n",
      "Iter  3400 | Train Loss: 0.397532 | LR: 0.0100 | Batch:   1 | Patience: 120/500 | Rate: 97.7 iter/s | Time: 34.80s\n",
      "Iter  3420 | Train Loss: 0.397524 | LR: 0.0100 | Batch:   1 | Patience: 124/500 | Rate: 97.7 iter/s | Time: 35.01s\n",
      "Iter  3440 | Train Loss: 0.397997 | LR: 0.0100 | Batch:   1 | Patience: 128/500 | Rate: 97.7 iter/s | Time: 35.20s\n",
      "Iter  3460 | Train Loss: 0.398200 | LR: 0.0100 | Batch:   1 | Patience: 132/500 | Rate: 97.7 iter/s | Time: 35.41s\n",
      "Iter  3480 | Train Loss: 0.397861 | LR: 0.0100 | Batch:   1 | Patience: 136/500 | Rate: 97.7 iter/s | Time: 35.61s\n",
      "Iter  3500 | Train Loss: 0.398133 | LR: 0.0100 | Batch:   1 | Patience: 140/500 | Rate: 97.7 iter/s | Time: 35.81s\n",
      "Iter  3520 | Train Loss: 0.398007 | LR: 0.0100 | Batch:   1 | Patience: 144/500 | Rate: 97.7 iter/s | Time: 36.02s\n",
      "Iter  3540 | Train Loss: 0.397946 | LR: 0.0100 | Batch:   1 | Patience: 148/500 | Rate: 97.7 iter/s | Time: 36.22s\n",
      "Iter  3560 | Train Loss: 0.398128 | LR: 0.0100 | Batch:   1 | Patience: 152/500 | Rate: 97.7 iter/s | Time: 36.42s\n",
      "Iter  3580 | Train Loss: 0.397002 | LR: 0.0100 | Batch:   1 | Patience: 156/500 | Rate: 97.4 iter/s | Time: 36.76s\n",
      "Iter  3600 | Train Loss: 0.397304 | LR: 0.0100 | Batch:   1 | Patience: 160/500 | Rate: 97.4 iter/s | Time: 36.96s\n",
      "Iter  3620 | Train Loss: 0.398243 | LR: 0.0100 | Batch:   1 | Patience: 164/500 | Rate: 97.4 iter/s | Time: 37.16s\n",
      "Iter  3640 | Train Loss: 0.398686 | LR: 0.0100 | Batch:   1 | Patience: 168/500 | Rate: 97.4 iter/s | Time: 37.36s\n",
      "Iter  3660 | Train Loss: 0.398605 | LR: 0.0100 | Batch:   1 | Patience: 172/500 | Rate: 97.4 iter/s | Time: 37.56s\n",
      "Iter  3680 | Train Loss: 0.398642 | LR: 0.0100 | Batch:   1 | Patience: 176/500 | Rate: 97.4 iter/s | Time: 37.76s\n",
      "Iter  3700 | Train Loss: 0.398350 | LR: 0.0100 | Batch:   1 | Patience: 180/500 | Rate: 97.4 iter/s | Time: 37.97s\n",
      "Iter  3720 | Train Loss: 0.399251 | LR: 0.0100 | Batch:   1 | Patience: 184/500 | Rate: 97.5 iter/s | Time: 38.17s\n",
      "Iter  3740 | Train Loss: 0.398645 | LR: 0.0100 | Batch:   1 | Patience: 188/500 | Rate: 97.5 iter/s | Time: 38.38s\n",
      "Iter  3760 | Train Loss: 0.399005 | LR: 0.0100 | Batch:   1 | Patience: 192/500 | Rate: 97.4 iter/s | Time: 38.58s\n",
      "Iter  3780 | Train Loss: 0.398653 | LR: 0.0100 | Batch:   1 | Patience: 196/500 | Rate: 97.4 iter/s | Time: 38.80s\n",
      "Iter  3800 | Train Loss: 0.399005 | LR: 0.0100 | Batch:   1 | Patience: 200/500 | Rate: 97.4 iter/s | Time: 39.01s\n",
      "Iter  3820 | Train Loss: 0.399337 | LR: 0.0100 | Batch:   1 | Patience: 204/500 | Rate: 97.4 iter/s | Time: 39.22s\n",
      "Iter  3840 | Train Loss: 0.399541 | LR: 0.0100 | Batch:   1 | Patience: 208/500 | Rate: 97.4 iter/s | Time: 39.43s\n",
      "Iter  3860 | Train Loss: 0.398156 | LR: 0.0100 | Batch:   1 | Patience: 212/500 | Rate: 97.4 iter/s | Time: 39.63s\n",
      "Iter  3880 | Train Loss: 0.398106 | LR: 0.0100 | Batch:   1 | Patience: 216/500 | Rate: 97.4 iter/s | Time: 39.84s\n",
      "Iter  3900 | Train Loss: 0.399117 | LR: 0.0100 | Batch:   1 | Patience: 220/500 | Rate: 97.4 iter/s | Time: 40.05s\n",
      "Iter  3920 | Train Loss: 0.398849 | LR: 0.0100 | Batch:   1 | Patience: 224/500 | Rate: 97.4 iter/s | Time: 40.26s\n",
      "Iter  3940 | Train Loss: 0.398953 | LR: 0.0100 | Batch:   1 | Patience: 228/500 | Rate: 97.4 iter/s | Time: 40.47s\n",
      "Iter  3960 | Train Loss: 0.398338 | LR: 0.0100 | Batch:   1 | Patience: 232/500 | Rate: 97.4 iter/s | Time: 40.67s\n",
      "Iter  3980 | Train Loss: 0.398838 | LR: 0.0100 | Batch:   1 | Patience: 236/500 | Rate: 97.4 iter/s | Time: 40.88s\n",
      "Iter  4000 | Train Loss: 0.398041 | LR: 0.0100 | Batch:   1 | Patience: 240/500 | Rate: 97.4 iter/s | Time: 41.08s\n",
      "Iter  4020 | Train Loss: 0.398438 | LR: 0.0100 | Batch:   1 | Patience: 244/500 | Rate: 97.4 iter/s | Time: 41.28s\n",
      "Iter  4040 | Train Loss: 0.400037 | LR: 0.0100 | Batch:   1 | Patience: 248/500 | Rate: 97.4 iter/s | Time: 41.48s\n",
      "Iter  4060 | Train Loss: 0.401171 | LR: 0.0100 | Batch:   1 | Patience: 252/500 | Rate: 97.4 iter/s | Time: 41.67s\n",
      "Iter  4080 | Train Loss: 0.402559 | LR: 0.0100 | Batch:   1 | Patience: 256/500 | Rate: 97.4 iter/s | Time: 41.87s\n",
      "Iter  4100 | Train Loss: 0.403976 | LR: 0.0100 | Batch:   1 | Patience: 260/500 | Rate: 97.5 iter/s | Time: 42.06s\n",
      "Iter  4120 | Train Loss: 0.404077 | LR: 0.0100 | Batch:   1 | Patience: 264/500 | Rate: 97.5 iter/s | Time: 42.26s\n",
      "Iter  4140 | Train Loss: 0.404919 | LR: 0.0100 | Batch:   1 | Patience: 268/500 | Rate: 97.5 iter/s | Time: 42.46s\n",
      "Iter  4160 | Train Loss: 0.401974 | LR: 0.0100 | Batch:   1 | Patience: 272/500 | Rate: 97.5 iter/s | Time: 42.67s\n",
      "Iter  4180 | Train Loss: 0.402816 | LR: 0.0100 | Batch:   1 | Patience: 276/500 | Rate: 97.5 iter/s | Time: 42.86s\n",
      "Iter  4200 | Train Loss: 0.404314 | LR: 0.0100 | Batch:   1 | Patience: 280/500 | Rate: 97.5 iter/s | Time: 43.07s\n",
      "Iter  4220 | Train Loss: 0.403265 | LR: 0.0100 | Batch:   1 | Patience: 284/500 | Rate: 97.5 iter/s | Time: 43.27s\n",
      "Iter  4240 | Train Loss: 0.404516 | LR: 0.0100 | Batch:   1 | Patience: 288/500 | Rate: 97.6 iter/s | Time: 43.46s\n",
      "Iter  4260 | Train Loss: 0.404695 | LR: 0.0100 | Batch:   1 | Patience: 292/500 | Rate: 97.6 iter/s | Time: 43.66s\n",
      "Iter  4280 | Train Loss: 0.402047 | LR: 0.0100 | Batch:   1 | Patience: 296/500 | Rate: 97.6 iter/s | Time: 43.85s\n",
      "Iter  4300 | Train Loss: 0.403017 | LR: 0.0100 | Batch:   1 | Patience: 300/500 | Rate: 97.6 iter/s | Time: 44.05s\n",
      "Iter  4320 | Train Loss: 0.403959 | LR: 0.0100 | Batch:   1 | Patience: 304/500 | Rate: 97.6 iter/s | Time: 44.24s\n",
      "Iter  4340 | Train Loss: 0.403574 | LR: 0.0100 | Batch:   1 | Patience: 308/500 | Rate: 97.7 iter/s | Time: 44.44s\n",
      "Iter  4360 | Train Loss: 0.402761 | LR: 0.0100 | Batch:   1 | Patience: 312/500 | Rate: 97.7 iter/s | Time: 44.64s\n",
      "Iter  4380 | Train Loss: 0.404690 | LR: 0.0100 | Batch:   1 | Patience: 316/500 | Rate: 97.7 iter/s | Time: 44.83s\n",
      "Iter  4400 | Train Loss: 0.403830 | LR: 0.0100 | Batch:   1 | Patience: 320/500 | Rate: 97.7 iter/s | Time: 45.03s\n",
      "Iter  4420 | Train Loss: 0.404352 | LR: 0.0100 | Batch:   1 | Patience: 324/500 | Rate: 97.7 iter/s | Time: 45.23s\n",
      "Iter  4440 | Train Loss: 0.400427 | LR: 0.0100 | Batch:   1 | Patience: 328/500 | Rate: 97.7 iter/s | Time: 45.43s\n",
      "Iter  4460 | Train Loss: 0.400670 | LR: 0.0100 | Batch:   1 | Patience: 332/500 | Rate: 97.8 iter/s | Time: 45.62s\n",
      "Iter  4480 | Train Loss: 0.400989 | LR: 0.0100 | Batch:   1 | Patience: 336/500 | Rate: 97.8 iter/s | Time: 45.82s\n",
      "Iter  4500 | Train Loss: 0.400152 | LR: 0.0100 | Batch:   1 | Patience: 340/500 | Rate: 97.8 iter/s | Time: 46.01s\n",
      "Iter  4520 | Train Loss: 0.398638 | LR: 0.0100 | Batch:   1 | Patience: 344/500 | Rate: 97.8 iter/s | Time: 46.21s\n",
      "Iter  4540 | Train Loss: 0.399317 | LR: 0.0100 | Batch:   1 | Patience: 348/500 | Rate: 97.8 iter/s | Time: 46.41s\n",
      "Iter  4560 | Train Loss: 0.399480 | LR: 0.0100 | Batch:   1 | Patience: 352/500 | Rate: 97.9 iter/s | Time: 46.60s\n",
      "Iter  4580 | Train Loss: 0.398845 | LR: 0.0100 | Batch:   1 | Patience: 356/500 | Rate: 97.9 iter/s | Time: 46.80s\n",
      "Iter  4600 | Train Loss: 0.397610 | LR: 0.0100 | Batch:   1 | Patience: 360/500 | Rate: 97.9 iter/s | Time: 46.99s\n",
      "Iter  4620 | Train Loss: 0.398150 | LR: 0.0100 | Batch:   1 | Patience: 364/500 | Rate: 97.9 iter/s | Time: 47.20s\n",
      "Iter  4640 | Train Loss: 0.397517 | LR: 0.0100 | Batch:   1 | Patience: 368/500 | Rate: 97.9 iter/s | Time: 47.39s\n",
      "Iter  4660 | Train Loss: 0.397882 | LR: 0.0100 | Batch:   1 | Patience: 372/500 | Rate: 97.9 iter/s | Time: 47.59s\n",
      "Iter  4680 | Train Loss: 0.396898 | LR: 0.0100 | Batch:   1 | Patience: 376/500 | Rate: 97.9 iter/s | Time: 47.78s\n",
      "Iter  4700 | Train Loss: 0.397311 | LR: 0.0100 | Batch:   1 | Patience: 380/500 | Rate: 98.0 iter/s | Time: 47.98s\n",
      "Iter  4720 | Train Loss: 0.398040 | LR: 0.0100 | Batch:   1 | Patience: 384/500 | Rate: 98.0 iter/s | Time: 48.18s\n",
      "Iter  4740 | Train Loss: 0.397205 | LR: 0.0100 | Batch:   1 | Patience: 388/500 | Rate: 98.0 iter/s | Time: 48.38s\n",
      "Iter  4760 | Train Loss: 0.397621 | LR: 0.0100 | Batch:   1 | Patience: 392/500 | Rate: 98.0 iter/s | Time: 48.57s\n",
      "Iter  4780 | Train Loss: 0.396931 | LR: 0.0100 | Batch:   1 | Patience: 396/500 | Rate: 98.0 iter/s | Time: 48.77s\n",
      "Iter  4800 | Train Loss: 0.397284 | LR: 0.0100 | Batch:   1 | Patience: 400/500 | Rate: 98.0 iter/s | Time: 48.96s\n",
      "Iter  4820 | Train Loss: 0.397071 | LR: 0.0100 | Batch:   1 | Patience: 404/500 | Rate: 98.0 iter/s | Time: 49.16s\n",
      "Iter  4840 | Train Loss: 0.396891 | LR: 0.0100 | Batch:   1 | Patience: 408/500 | Rate: 98.0 iter/s | Time: 49.36s\n",
      "Iter  4860 | Train Loss: 0.397114 | LR: 0.0100 | Batch:   1 | Patience: 412/500 | Rate: 98.1 iter/s | Time: 49.56s\n",
      "Iter  4880 | Train Loss: 0.397031 | LR: 0.0100 | Batch:   1 | Patience: 416/500 | Rate: 98.1 iter/s | Time: 49.75s\n",
      "Iter  4900 | Train Loss: 0.397250 | LR: 0.0100 | Batch:   1 | Patience: 420/500 | Rate: 98.1 iter/s | Time: 49.95s\n",
      "Iter  4920 | Train Loss: 0.397234 | LR: 0.0100 | Batch:   1 | Patience: 424/500 | Rate: 98.1 iter/s | Time: 50.14s\n",
      "Iter  4940 | Train Loss: 0.396887 | LR: 0.0100 | Batch:   1 | Patience: 428/500 | Rate: 98.1 iter/s | Time: 50.34s\n",
      "Iter  4960 | Train Loss: 0.397452 | LR: 0.0100 | Batch:   1 | Patience: 432/500 | Rate: 98.1 iter/s | Time: 50.54s\n",
      "Iter  4980 | Train Loss: 0.397942 | LR: 0.0100 | Batch:   1 | Patience: 436/500 | Rate: 98.1 iter/s | Time: 50.74s\n",
      "Iter  5000 | Train Loss: 0.398510 | LR: 0.0100 | Batch:   1 | Patience: 440/500 | Rate: 98.1 iter/s | Time: 50.94s\n",
      "Iter  5020 | Train Loss: 0.398436 | LR: 0.0100 | Batch:   1 | Patience: 444/500 | Rate: 98.2 iter/s | Time: 51.14s\n",
      "Iter  5040 | Train Loss: 0.398036 | LR: 0.0100 | Batch:   1 | Patience: 448/500 | Rate: 98.2 iter/s | Time: 51.34s\n",
      "Iter  5060 | Train Loss: 0.398147 | LR: 0.0100 | Batch:   1 | Patience: 452/500 | Rate: 98.2 iter/s | Time: 51.54s\n",
      "Iter  5080 | Train Loss: 0.397267 | LR: 0.0100 | Batch:   1 | Patience: 456/500 | Rate: 98.2 iter/s | Time: 51.74s\n",
      "Iter  5100 | Train Loss: 0.398636 | LR: 0.0100 | Batch:   1 | Patience: 460/500 | Rate: 98.2 iter/s | Time: 51.93s\n",
      "Iter  5120 | Train Loss: 0.399149 | LR: 0.0100 | Batch:   1 | Patience: 464/500 | Rate: 98.2 iter/s | Time: 52.13s\n",
      "Iter  5140 | Train Loss: 0.399444 | LR: 0.0100 | Batch:   1 | Patience: 468/500 | Rate: 98.2 iter/s | Time: 52.33s\n",
      "Iter  5160 | Train Loss: 0.399850 | LR: 0.0100 | Batch:   1 | Patience: 472/500 | Rate: 98.2 iter/s | Time: 52.53s\n",
      "Iter  5180 | Train Loss: 0.399916 | LR: 0.0100 | Batch:   1 | Patience: 476/500 | Rate: 98.2 iter/s | Time: 52.73s\n",
      "Iter  5200 | Train Loss: 0.399697 | LR: 0.0100 | Batch:   1 | Patience: 480/500 | Rate: 98.2 iter/s | Time: 52.94s\n",
      "Iter  5220 | Train Loss: 0.399133 | LR: 0.0100 | Batch:   1 | Patience: 484/500 | Rate: 98.2 iter/s | Time: 53.14s\n",
      "Iter  5240 | Train Loss: 0.400222 | LR: 0.0100 | Batch:   1 | Patience: 488/500 | Rate: 98.2 iter/s | Time: 53.36s\n",
      "Iter  5260 | Train Loss: 0.401319 | LR: 0.0100 | Batch:   1 | Patience: 492/500 | Rate: 98.2 iter/s | Time: 53.56s\n",
      "Iter  5280 | Train Loss: 0.400991 | LR: 0.0100 | Batch:   1 | Patience: 496/500 | Rate: 98.2 iter/s | Time: 53.76s\n",
      "\n",
      "Early stopping triggered at iteration 5296\n",
      "No improvement in loss for 500 checks (threshold: 1e-06)\n",
      "Final loss: 0.398980\n",
      "Total training time: 53.91s\n",
      "Average time per iteration: 0.0102s\n",
      "Total iterations: 5296\n",
      "Approximate epochs: 0.01\n",
      "Starting SGD Iterations training\n",
      "Learning rate: 0.05, Batch size: 1\n",
      "Max iterations: 100000\n",
      "Early stopping: patience=500, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "Initial full dataset loss: 0.698500\n",
      "----------------------------------------------------------------------\n",
      "Iter    20 | Train Loss: 0.532729 | LR: 0.0500 | Batch:   1 | Patience:   0/500 | Rate: 105.6 iter/s | Time: 0.19s\n",
      "Iter    40 | Train Loss: 0.451332 | LR: 0.0500 | Batch:   1 | Patience:   0/500 | Rate: 105.8 iter/s | Time: 0.38s\n",
      "Iter    60 | Train Loss: 0.457967 | LR: 0.0500 | Batch:   1 | Patience:   1/500 | Rate: 105.1 iter/s | Time: 0.57s\n",
      "Iter    80 | Train Loss: 0.455583 | LR: 0.0500 | Batch:   1 | Patience:   5/500 | Rate: 104.0 iter/s | Time: 0.77s\n",
      "Iter   100 | Train Loss: 0.437409 | LR: 0.0500 | Batch:   1 | Patience:   9/500 | Rate: 103.3 iter/s | Time: 0.97s\n",
      "Iter   120 | Train Loss: 0.427881 | LR: 0.0500 | Batch:   1 | Patience:   1/500 | Rate: 103.0 iter/s | Time: 1.16s\n",
      "Iter   140 | Train Loss: 0.445317 | LR: 0.0500 | Batch:   1 | Patience:   5/500 | Rate: 102.8 iter/s | Time: 1.36s\n",
      "Iter   160 | Train Loss: 0.422184 | LR: 0.0500 | Batch:   1 | Patience:   0/500 | Rate: 103.1 iter/s | Time: 1.55s\n",
      "Iter   180 | Train Loss: 0.418793 | LR: 0.0500 | Batch:   1 | Patience:   2/500 | Rate: 101.7 iter/s | Time: 1.77s\n",
      "Iter   200 | Train Loss: 0.427319 | LR: 0.0500 | Batch:   1 | Patience:   6/500 | Rate: 101.9 iter/s | Time: 1.96s\n",
      "Iter   220 | Train Loss: 0.417538 | LR: 0.0500 | Batch:   1 | Patience:  10/500 | Rate: 101.0 iter/s | Time: 2.18s\n",
      "Iter   240 | Train Loss: 0.424645 | LR: 0.0500 | Batch:   1 | Patience:  14/500 | Rate: 100.7 iter/s | Time: 2.38s\n",
      "Iter   260 | Train Loss: 0.428196 | LR: 0.0500 | Batch:   1 | Patience:  18/500 | Rate: 99.6 iter/s | Time: 2.61s\n",
      "Iter   280 | Train Loss: 0.426660 | LR: 0.0500 | Batch:   1 | Patience:  22/500 | Rate: 99.1 iter/s | Time: 2.82s\n",
      "Iter   300 | Train Loss: 0.423818 | LR: 0.0500 | Batch:   1 | Patience:  26/500 | Rate: 98.8 iter/s | Time: 3.04s\n",
      "Iter   320 | Train Loss: 0.432571 | LR: 0.0500 | Batch:   1 | Patience:  30/500 | Rate: 99.0 iter/s | Time: 3.23s\n",
      "Iter   340 | Train Loss: 0.432145 | LR: 0.0500 | Batch:   1 | Patience:  34/500 | Rate: 99.1 iter/s | Time: 3.43s\n",
      "Iter   360 | Train Loss: 0.428217 | LR: 0.0500 | Batch:   1 | Patience:  38/500 | Rate: 99.3 iter/s | Time: 3.63s\n",
      "Iter   380 | Train Loss: 0.425341 | LR: 0.0500 | Batch:   1 | Patience:  42/500 | Rate: 99.4 iter/s | Time: 3.82s\n",
      "Iter   400 | Train Loss: 0.428669 | LR: 0.0500 | Batch:   1 | Patience:  46/500 | Rate: 99.5 iter/s | Time: 4.02s\n",
      "Iter   420 | Train Loss: 0.426486 | LR: 0.0500 | Batch:   1 | Patience:  50/500 | Rate: 99.7 iter/s | Time: 4.21s\n",
      "Iter   440 | Train Loss: 0.425517 | LR: 0.0500 | Batch:   1 | Patience:  54/500 | Rate: 99.5 iter/s | Time: 4.42s\n",
      "Iter   460 | Train Loss: 0.424088 | LR: 0.0500 | Batch:   1 | Patience:  58/500 | Rate: 99.6 iter/s | Time: 4.62s\n",
      "Iter   480 | Train Loss: 0.427358 | LR: 0.0500 | Batch:   1 | Patience:  62/500 | Rate: 99.7 iter/s | Time: 4.81s\n",
      "Iter   500 | Train Loss: 0.423909 | LR: 0.0500 | Batch:   1 | Patience:  66/500 | Rate: 99.8 iter/s | Time: 5.01s\n",
      "Iter   520 | Train Loss: 0.430166 | LR: 0.0500 | Batch:   1 | Patience:  70/500 | Rate: 99.9 iter/s | Time: 5.20s\n",
      "Iter   540 | Train Loss: 0.431040 | LR: 0.0500 | Batch:   1 | Patience:  74/500 | Rate: 100.0 iter/s | Time: 5.40s\n",
      "Iter   560 | Train Loss: 0.429766 | LR: 0.0500 | Batch:   1 | Patience:  78/500 | Rate: 99.9 iter/s | Time: 5.61s\n",
      "Iter   580 | Train Loss: 0.433355 | LR: 0.0500 | Batch:   1 | Patience:  82/500 | Rate: 100.0 iter/s | Time: 5.80s\n",
      "Iter   600 | Train Loss: 0.439403 | LR: 0.0500 | Batch:   1 | Patience:  86/500 | Rate: 99.9 iter/s | Time: 6.01s\n",
      "Iter   620 | Train Loss: 0.420308 | LR: 0.0500 | Batch:   1 | Patience:  90/500 | Rate: 99.9 iter/s | Time: 6.21s\n",
      "Iter   640 | Train Loss: 0.420847 | LR: 0.0500 | Batch:   1 | Patience:   2/500 | Rate: 99.8 iter/s | Time: 6.41s\n",
      "Iter   660 | Train Loss: 0.416640 | LR: 0.0500 | Batch:   1 | Patience:   6/500 | Rate: 99.8 iter/s | Time: 6.61s\n",
      "Iter   680 | Train Loss: 0.427823 | LR: 0.0500 | Batch:   1 | Patience:   3/500 | Rate: 99.7 iter/s | Time: 6.82s\n",
      "Iter   700 | Train Loss: 0.435113 | LR: 0.0500 | Batch:   1 | Patience:   7/500 | Rate: 99.8 iter/s | Time: 7.02s\n",
      "Iter   720 | Train Loss: 0.433603 | LR: 0.0500 | Batch:   1 | Patience:  11/500 | Rate: 99.8 iter/s | Time: 7.21s\n",
      "Iter   740 | Train Loss: 0.424815 | LR: 0.0500 | Batch:   1 | Patience:  15/500 | Rate: 99.9 iter/s | Time: 7.41s\n",
      "Iter   760 | Train Loss: 0.431053 | LR: 0.0500 | Batch:   1 | Patience:  19/500 | Rate: 99.7 iter/s | Time: 7.63s\n",
      "Iter   780 | Train Loss: 0.462234 | LR: 0.0500 | Batch:   1 | Patience:  23/500 | Rate: 99.8 iter/s | Time: 7.82s\n",
      "Iter   800 | Train Loss: 0.444052 | LR: 0.0500 | Batch:   1 | Patience:  27/500 | Rate: 99.8 iter/s | Time: 8.02s\n",
      "Iter   820 | Train Loss: 0.423019 | LR: 0.0500 | Batch:   1 | Patience:  31/500 | Rate: 99.9 iter/s | Time: 8.21s\n",
      "Iter   840 | Train Loss: 0.422270 | LR: 0.0500 | Batch:   1 | Patience:  35/500 | Rate: 99.9 iter/s | Time: 8.41s\n",
      "Iter   860 | Train Loss: 0.430861 | LR: 0.0500 | Batch:   1 | Patience:  39/500 | Rate: 99.9 iter/s | Time: 8.60s\n",
      "Iter   880 | Train Loss: 0.433843 | LR: 0.0500 | Batch:   1 | Patience:  43/500 | Rate: 99.9 iter/s | Time: 8.81s\n",
      "Iter   900 | Train Loss: 0.437565 | LR: 0.0500 | Batch:   1 | Patience:  47/500 | Rate: 99.9 iter/s | Time: 9.01s\n",
      "Iter   920 | Train Loss: 0.427409 | LR: 0.0500 | Batch:   1 | Patience:  51/500 | Rate: 99.9 iter/s | Time: 9.21s\n",
      "Iter   940 | Train Loss: 0.421144 | LR: 0.0500 | Batch:   1 | Patience:  55/500 | Rate: 99.9 iter/s | Time: 9.41s\n",
      "Iter   960 | Train Loss: 0.420454 | LR: 0.0500 | Batch:   1 | Patience:  59/500 | Rate: 99.9 iter/s | Time: 9.61s\n",
      "Iter   980 | Train Loss: 0.427469 | LR: 0.0500 | Batch:   1 | Patience:  63/500 | Rate: 99.8 iter/s | Time: 9.82s\n",
      "Iter  1000 | Train Loss: 0.423317 | LR: 0.0500 | Batch:   1 | Patience:  67/500 | Rate: 99.8 iter/s | Time: 10.02s\n",
      "Iter  1020 | Train Loss: 0.426919 | LR: 0.0500 | Batch:   1 | Patience:  71/500 | Rate: 99.8 iter/s | Time: 10.22s\n",
      "Iter  1040 | Train Loss: 0.421085 | LR: 0.0500 | Batch:   1 | Patience:  75/500 | Rate: 99.9 iter/s | Time: 10.42s\n",
      "Iter  1060 | Train Loss: 0.421952 | LR: 0.0500 | Batch:   1 | Patience:  79/500 | Rate: 99.9 iter/s | Time: 10.61s\n",
      "Iter  1080 | Train Loss: 0.415663 | LR: 0.0500 | Batch:   1 | Patience:  83/500 | Rate: 99.9 iter/s | Time: 10.81s\n",
      "Iter  1100 | Train Loss: 0.409502 | LR: 0.0500 | Batch:   1 | Patience:   0/500 | Rate: 99.9 iter/s | Time: 11.01s\n",
      "Iter  1120 | Train Loss: 0.413652 | LR: 0.0500 | Batch:   1 | Patience:   3/500 | Rate: 99.9 iter/s | Time: 11.21s\n",
      "Iter  1140 | Train Loss: 0.413799 | LR: 0.0500 | Batch:   1 | Patience:   7/500 | Rate: 99.9 iter/s | Time: 11.41s\n",
      "Iter  1160 | Train Loss: 0.418024 | LR: 0.0500 | Batch:   1 | Patience:  11/500 | Rate: 99.9 iter/s | Time: 11.61s\n",
      "Iter  1180 | Train Loss: 0.424055 | LR: 0.0500 | Batch:   1 | Patience:  15/500 | Rate: 99.9 iter/s | Time: 11.81s\n",
      "Iter  1200 | Train Loss: 0.422725 | LR: 0.0500 | Batch:   1 | Patience:  19/500 | Rate: 99.9 iter/s | Time: 12.01s\n",
      "Iter  1220 | Train Loss: 0.421299 | LR: 0.0500 | Batch:   1 | Patience:  23/500 | Rate: 99.9 iter/s | Time: 12.21s\n",
      "Iter  1240 | Train Loss: 0.426898 | LR: 0.0500 | Batch:   1 | Patience:  27/500 | Rate: 99.9 iter/s | Time: 12.41s\n",
      "Iter  1260 | Train Loss: 0.420411 | LR: 0.0500 | Batch:   1 | Patience:  31/500 | Rate: 100.0 iter/s | Time: 12.61s\n",
      "Iter  1280 | Train Loss: 0.422220 | LR: 0.0500 | Batch:   1 | Patience:  35/500 | Rate: 99.8 iter/s | Time: 12.82s\n",
      "Iter  1300 | Train Loss: 0.428856 | LR: 0.0500 | Batch:   1 | Patience:  39/500 | Rate: 99.9 iter/s | Time: 13.02s\n",
      "Iter  1320 | Train Loss: 0.416362 | LR: 0.0500 | Batch:   1 | Patience:  43/500 | Rate: 99.9 iter/s | Time: 13.22s\n",
      "Iter  1340 | Train Loss: 0.409506 | LR: 0.0500 | Batch:   1 | Patience:  47/500 | Rate: 99.9 iter/s | Time: 13.42s\n",
      "Iter  1360 | Train Loss: 0.410945 | LR: 0.0500 | Batch:   1 | Patience:   2/500 | Rate: 99.9 iter/s | Time: 13.62s\n",
      "Iter  1380 | Train Loss: 0.410338 | LR: 0.0500 | Batch:   1 | Patience:   6/500 | Rate: 99.8 iter/s | Time: 13.82s\n",
      "Iter  1400 | Train Loss: 0.417097 | LR: 0.0500 | Batch:   1 | Patience:  10/500 | Rate: 99.8 iter/s | Time: 14.03s\n",
      "Iter  1420 | Train Loss: 0.418846 | LR: 0.0500 | Batch:   1 | Patience:  14/500 | Rate: 99.9 iter/s | Time: 14.22s\n",
      "Iter  1440 | Train Loss: 0.418860 | LR: 0.0500 | Batch:   1 | Patience:  18/500 | Rate: 99.9 iter/s | Time: 14.42s\n",
      "Iter  1460 | Train Loss: 0.415449 | LR: 0.0500 | Batch:   1 | Patience:  22/500 | Rate: 99.9 iter/s | Time: 14.61s\n",
      "Iter  1480 | Train Loss: 0.413696 | LR: 0.0500 | Batch:   1 | Patience:  26/500 | Rate: 99.9 iter/s | Time: 14.81s\n",
      "Iter  1500 | Train Loss: 0.414954 | LR: 0.0500 | Batch:   1 | Patience:  30/500 | Rate: 100.0 iter/s | Time: 15.00s\n",
      "Iter  1520 | Train Loss: 0.410421 | LR: 0.0500 | Batch:   1 | Patience:  34/500 | Rate: 100.0 iter/s | Time: 15.20s\n",
      "Iter  1540 | Train Loss: 0.409409 | LR: 0.0500 | Batch:   1 | Patience:  38/500 | Rate: 100.0 iter/s | Time: 15.39s\n",
      "Iter  1560 | Train Loss: 0.412264 | LR: 0.0500 | Batch:   1 | Patience:  42/500 | Rate: 100.1 iter/s | Time: 15.59s\n",
      "Iter  1580 | Train Loss: 0.412678 | LR: 0.0500 | Batch:   1 | Patience:  46/500 | Rate: 100.0 iter/s | Time: 15.79s\n",
      "Iter  1600 | Train Loss: 0.416972 | LR: 0.0500 | Batch:   1 | Patience:  50/500 | Rate: 100.1 iter/s | Time: 15.99s\n",
      "Iter  1620 | Train Loss: 0.415269 | LR: 0.0500 | Batch:   1 | Patience:  54/500 | Rate: 100.1 iter/s | Time: 16.19s\n",
      "Iter  1640 | Train Loss: 0.417006 | LR: 0.0500 | Batch:   1 | Patience:  58/500 | Rate: 100.1 iter/s | Time: 16.38s\n",
      "Iter  1660 | Train Loss: 0.419588 | LR: 0.0500 | Batch:   1 | Patience:  62/500 | Rate: 100.1 iter/s | Time: 16.58s\n",
      "Iter  1680 | Train Loss: 0.417687 | LR: 0.0500 | Batch:   1 | Patience:  66/500 | Rate: 100.2 iter/s | Time: 16.77s\n",
      "Iter  1700 | Train Loss: 0.424616 | LR: 0.0500 | Batch:   1 | Patience:  70/500 | Rate: 100.2 iter/s | Time: 16.97s\n",
      "Iter  1720 | Train Loss: 0.409470 | LR: 0.0500 | Batch:   1 | Patience:  74/500 | Rate: 100.2 iter/s | Time: 17.16s\n",
      "Iter  1740 | Train Loss: 0.414667 | LR: 0.0500 | Batch:   1 | Patience:  78/500 | Rate: 100.2 iter/s | Time: 17.37s\n",
      "Iter  1760 | Train Loss: 0.426016 | LR: 0.0500 | Batch:   1 | Patience:  82/500 | Rate: 100.2 iter/s | Time: 17.56s\n",
      "Iter  1780 | Train Loss: 0.418690 | LR: 0.0500 | Batch:   1 | Patience:  86/500 | Rate: 100.2 iter/s | Time: 17.76s\n",
      "Iter  1800 | Train Loss: 0.418193 | LR: 0.0500 | Batch:   1 | Patience:  90/500 | Rate: 100.3 iter/s | Time: 17.95s\n",
      "Iter  1820 | Train Loss: 0.421175 | LR: 0.0500 | Batch:   1 | Patience:  94/500 | Rate: 100.3 iter/s | Time: 18.15s\n",
      "Iter  1840 | Train Loss: 0.423920 | LR: 0.0500 | Batch:   1 | Patience:  98/500 | Rate: 100.3 iter/s | Time: 18.34s\n",
      "Iter  1860 | Train Loss: 0.418099 | LR: 0.0500 | Batch:   1 | Patience: 102/500 | Rate: 100.3 iter/s | Time: 18.54s\n",
      "Iter  1880 | Train Loss: 0.422033 | LR: 0.0500 | Batch:   1 | Patience: 106/500 | Rate: 100.3 iter/s | Time: 18.74s\n",
      "Iter  1900 | Train Loss: 0.428145 | LR: 0.0500 | Batch:   1 | Patience: 110/500 | Rate: 100.4 iter/s | Time: 18.93s\n",
      "Iter  1920 | Train Loss: 0.433929 | LR: 0.0500 | Batch:   1 | Patience: 114/500 | Rate: 100.4 iter/s | Time: 19.13s\n",
      "Iter  1940 | Train Loss: 0.424673 | LR: 0.0500 | Batch:   1 | Patience: 118/500 | Rate: 100.4 iter/s | Time: 19.32s\n",
      "Iter  1960 | Train Loss: 0.418411 | LR: 0.0500 | Batch:   1 | Patience: 122/500 | Rate: 100.4 iter/s | Time: 19.51s\n",
      "Iter  1980 | Train Loss: 0.414413 | LR: 0.0500 | Batch:   1 | Patience: 126/500 | Rate: 100.5 iter/s | Time: 19.71s\n",
      "Iter  2000 | Train Loss: 0.410382 | LR: 0.0500 | Batch:   1 | Patience: 130/500 | Rate: 100.4 iter/s | Time: 19.91s\n",
      "Iter  2020 | Train Loss: 0.440572 | LR: 0.0500 | Batch:   1 | Patience: 134/500 | Rate: 100.5 iter/s | Time: 20.11s\n",
      "Iter  2040 | Train Loss: 0.430215 | LR: 0.0500 | Batch:   1 | Patience: 138/500 | Rate: 100.5 iter/s | Time: 20.30s\n",
      "Iter  2060 | Train Loss: 0.438422 | LR: 0.0500 | Batch:   1 | Patience: 142/500 | Rate: 100.5 iter/s | Time: 20.49s\n",
      "Iter  2080 | Train Loss: 0.417855 | LR: 0.0500 | Batch:   1 | Patience: 146/500 | Rate: 100.5 iter/s | Time: 20.69s\n",
      "Iter  2100 | Train Loss: 0.410255 | LR: 0.0500 | Batch:   1 | Patience: 150/500 | Rate: 100.5 iter/s | Time: 20.89s\n",
      "Iter  2120 | Train Loss: 0.409040 | LR: 0.0500 | Batch:   1 | Patience: 154/500 | Rate: 100.6 iter/s | Time: 21.08s\n",
      "Iter  2140 | Train Loss: 0.412080 | LR: 0.0500 | Batch:   1 | Patience: 158/500 | Rate: 100.6 iter/s | Time: 21.27s\n",
      "Iter  2160 | Train Loss: 0.416723 | LR: 0.0500 | Batch:   1 | Patience: 162/500 | Rate: 100.6 iter/s | Time: 21.47s\n",
      "Iter  2180 | Train Loss: 0.433269 | LR: 0.0500 | Batch:   1 | Patience: 166/500 | Rate: 100.6 iter/s | Time: 21.66s\n",
      "Iter  2200 | Train Loss: 0.434985 | LR: 0.0500 | Batch:   1 | Patience: 170/500 | Rate: 100.6 iter/s | Time: 21.87s\n",
      "Iter  2220 | Train Loss: 0.427032 | LR: 0.0500 | Batch:   1 | Patience: 174/500 | Rate: 100.6 iter/s | Time: 22.07s\n",
      "Iter  2240 | Train Loss: 0.426884 | LR: 0.0500 | Batch:   1 | Patience: 178/500 | Rate: 100.6 iter/s | Time: 22.27s\n",
      "Iter  2260 | Train Loss: 0.418867 | LR: 0.0500 | Batch:   1 | Patience: 182/500 | Rate: 100.6 iter/s | Time: 22.46s\n",
      "Iter  2280 | Train Loss: 0.422366 | LR: 0.0500 | Batch:   1 | Patience: 186/500 | Rate: 100.6 iter/s | Time: 22.66s\n",
      "Iter  2300 | Train Loss: 0.419412 | LR: 0.0500 | Batch:   1 | Patience: 190/500 | Rate: 100.6 iter/s | Time: 22.86s\n",
      "Iter  2320 | Train Loss: 0.419599 | LR: 0.0500 | Batch:   1 | Patience: 194/500 | Rate: 100.6 iter/s | Time: 23.06s\n",
      "Iter  2340 | Train Loss: 0.418396 | LR: 0.0500 | Batch:   1 | Patience: 198/500 | Rate: 100.7 iter/s | Time: 23.25s\n",
      "Iter  2360 | Train Loss: 0.419171 | LR: 0.0500 | Batch:   1 | Patience: 202/500 | Rate: 100.7 iter/s | Time: 23.45s\n",
      "Iter  2380 | Train Loss: 0.419729 | LR: 0.0500 | Batch:   1 | Patience: 206/500 | Rate: 100.7 iter/s | Time: 23.64s\n",
      "Iter  2400 | Train Loss: 0.421598 | LR: 0.0500 | Batch:   1 | Patience: 210/500 | Rate: 100.7 iter/s | Time: 23.84s\n",
      "Iter  2420 | Train Loss: 0.417133 | LR: 0.0500 | Batch:   1 | Patience: 214/500 | Rate: 100.6 iter/s | Time: 24.05s\n",
      "Iter  2440 | Train Loss: 0.417317 | LR: 0.0500 | Batch:   1 | Patience: 218/500 | Rate: 100.6 iter/s | Time: 24.25s\n",
      "Iter  2460 | Train Loss: 0.412407 | LR: 0.0500 | Batch:   1 | Patience: 222/500 | Rate: 100.6 iter/s | Time: 24.45s\n",
      "Iter  2480 | Train Loss: 0.416894 | LR: 0.0500 | Batch:   1 | Patience: 226/500 | Rate: 100.5 iter/s | Time: 24.67s\n",
      "Iter  2500 | Train Loss: 0.409322 | LR: 0.0500 | Batch:   1 | Patience: 230/500 | Rate: 100.5 iter/s | Time: 24.87s\n",
      "Iter  2520 | Train Loss: 0.425324 | LR: 0.0500 | Batch:   1 | Patience: 234/500 | Rate: 100.5 iter/s | Time: 25.07s\n",
      "Iter  2540 | Train Loss: 0.421383 | LR: 0.0500 | Batch:   1 | Patience: 238/500 | Rate: 100.5 iter/s | Time: 25.27s\n",
      "Iter  2560 | Train Loss: 0.425012 | LR: 0.0500 | Batch:   1 | Patience: 242/500 | Rate: 100.5 iter/s | Time: 25.46s\n",
      "Iter  2580 | Train Loss: 0.423729 | LR: 0.0500 | Batch:   1 | Patience: 246/500 | Rate: 100.6 iter/s | Time: 25.66s\n",
      "Iter  2600 | Train Loss: 0.423051 | LR: 0.0500 | Batch:   1 | Patience: 250/500 | Rate: 100.6 iter/s | Time: 25.85s\n",
      "Iter  2620 | Train Loss: 0.425605 | LR: 0.0500 | Batch:   1 | Patience: 254/500 | Rate: 100.6 iter/s | Time: 26.05s\n",
      "Iter  2640 | Train Loss: 0.428621 | LR: 0.0500 | Batch:   1 | Patience: 258/500 | Rate: 100.6 iter/s | Time: 26.25s\n",
      "Iter  2660 | Train Loss: 0.434516 | LR: 0.0500 | Batch:   1 | Patience: 262/500 | Rate: 100.6 iter/s | Time: 26.44s\n",
      "Iter  2680 | Train Loss: 0.447519 | LR: 0.0500 | Batch:   1 | Patience: 266/500 | Rate: 100.6 iter/s | Time: 26.63s\n",
      "Iter  2700 | Train Loss: 0.440990 | LR: 0.0500 | Batch:   1 | Patience: 270/500 | Rate: 100.6 iter/s | Time: 26.83s\n",
      "Iter  2720 | Train Loss: 0.428501 | LR: 0.0500 | Batch:   1 | Patience: 274/500 | Rate: 100.7 iter/s | Time: 27.02s\n",
      "Iter  2740 | Train Loss: 0.452662 | LR: 0.0500 | Batch:   1 | Patience: 278/500 | Rate: 100.7 iter/s | Time: 27.21s\n",
      "Iter  2760 | Train Loss: 0.430077 | LR: 0.0500 | Batch:   1 | Patience: 282/500 | Rate: 100.7 iter/s | Time: 27.41s\n",
      "Iter  2780 | Train Loss: 0.420538 | LR: 0.0500 | Batch:   1 | Patience: 286/500 | Rate: 100.7 iter/s | Time: 27.60s\n",
      "Iter  2800 | Train Loss: 0.418860 | LR: 0.0500 | Batch:   1 | Patience: 290/500 | Rate: 100.8 iter/s | Time: 27.79s\n",
      "Iter  2820 | Train Loss: 0.416938 | LR: 0.0500 | Batch:   1 | Patience: 294/500 | Rate: 100.8 iter/s | Time: 27.98s\n",
      "Iter  2840 | Train Loss: 0.417595 | LR: 0.0500 | Batch:   1 | Patience: 298/500 | Rate: 100.8 iter/s | Time: 28.18s\n",
      "Iter  2860 | Train Loss: 0.414408 | LR: 0.0500 | Batch:   1 | Patience: 302/500 | Rate: 100.8 iter/s | Time: 28.38s\n",
      "Iter  2880 | Train Loss: 0.421734 | LR: 0.0500 | Batch:   1 | Patience: 306/500 | Rate: 100.8 iter/s | Time: 28.57s\n",
      "Iter  2900 | Train Loss: 0.422153 | LR: 0.0500 | Batch:   1 | Patience: 310/500 | Rate: 100.8 iter/s | Time: 28.77s\n",
      "Iter  2920 | Train Loss: 0.434235 | LR: 0.0500 | Batch:   1 | Patience: 314/500 | Rate: 100.8 iter/s | Time: 28.97s\n",
      "Iter  2940 | Train Loss: 0.424787 | LR: 0.0500 | Batch:   1 | Patience: 318/500 | Rate: 100.8 iter/s | Time: 29.17s\n",
      "Iter  2960 | Train Loss: 0.425867 | LR: 0.0500 | Batch:   1 | Patience: 322/500 | Rate: 100.8 iter/s | Time: 29.36s\n",
      "Iter  2980 | Train Loss: 0.433226 | LR: 0.0500 | Batch:   1 | Patience: 326/500 | Rate: 100.8 iter/s | Time: 29.56s\n",
      "Iter  3000 | Train Loss: 0.433307 | LR: 0.0500 | Batch:   1 | Patience: 330/500 | Rate: 100.8 iter/s | Time: 29.77s\n",
      "Iter  3020 | Train Loss: 0.440398 | LR: 0.0500 | Batch:   1 | Patience: 334/500 | Rate: 100.7 iter/s | Time: 29.98s\n",
      "Iter  3040 | Train Loss: 0.420654 | LR: 0.0500 | Batch:   1 | Patience: 338/500 | Rate: 100.7 iter/s | Time: 30.20s\n",
      "Iter  3060 | Train Loss: 0.413026 | LR: 0.0500 | Batch:   1 | Patience: 342/500 | Rate: 100.7 iter/s | Time: 30.40s\n",
      "Iter  3080 | Train Loss: 0.414762 | LR: 0.0500 | Batch:   1 | Patience: 346/500 | Rate: 100.7 iter/s | Time: 30.60s\n",
      "Iter  3100 | Train Loss: 0.415643 | LR: 0.0500 | Batch:   1 | Patience: 350/500 | Rate: 100.6 iter/s | Time: 30.80s\n",
      "Iter  3120 | Train Loss: 0.414823 | LR: 0.0500 | Batch:   1 | Patience: 354/500 | Rate: 100.6 iter/s | Time: 31.01s\n",
      "Iter  3140 | Train Loss: 0.412374 | LR: 0.0500 | Batch:   1 | Patience: 358/500 | Rate: 100.6 iter/s | Time: 31.21s\n",
      "Iter  3160 | Train Loss: 0.414746 | LR: 0.0500 | Batch:   1 | Patience: 362/500 | Rate: 100.6 iter/s | Time: 31.42s\n",
      "Iter  3180 | Train Loss: 0.415511 | LR: 0.0500 | Batch:   1 | Patience: 366/500 | Rate: 100.6 iter/s | Time: 31.62s\n",
      "Iter  3200 | Train Loss: 0.419671 | LR: 0.0500 | Batch:   1 | Patience: 370/500 | Rate: 100.6 iter/s | Time: 31.82s\n",
      "Iter  3220 | Train Loss: 0.417387 | LR: 0.0500 | Batch:   1 | Patience: 374/500 | Rate: 100.6 iter/s | Time: 32.02s\n",
      "Iter  3240 | Train Loss: 0.430153 | LR: 0.0500 | Batch:   1 | Patience: 378/500 | Rate: 100.6 iter/s | Time: 32.22s\n",
      "Iter  3260 | Train Loss: 0.421841 | LR: 0.0500 | Batch:   1 | Patience: 382/500 | Rate: 100.6 iter/s | Time: 32.42s\n",
      "Iter  3280 | Train Loss: 0.413718 | LR: 0.0500 | Batch:   1 | Patience: 386/500 | Rate: 100.6 iter/s | Time: 32.61s\n",
      "Iter  3300 | Train Loss: 0.416396 | LR: 0.0500 | Batch:   1 | Patience: 390/500 | Rate: 100.6 iter/s | Time: 32.81s\n",
      "Iter  3320 | Train Loss: 0.419480 | LR: 0.0500 | Batch:   1 | Patience: 394/500 | Rate: 100.6 iter/s | Time: 33.01s\n",
      "Iter  3340 | Train Loss: 0.420512 | LR: 0.0500 | Batch:   1 | Patience: 398/500 | Rate: 100.6 iter/s | Time: 33.21s\n",
      "Iter  3360 | Train Loss: 0.435005 | LR: 0.0500 | Batch:   1 | Patience: 402/500 | Rate: 100.5 iter/s | Time: 33.42s\n",
      "Iter  3380 | Train Loss: 0.426613 | LR: 0.0500 | Batch:   1 | Patience: 406/500 | Rate: 100.5 iter/s | Time: 33.65s\n",
      "Iter  3400 | Train Loss: 0.426993 | LR: 0.0500 | Batch:   1 | Patience: 410/500 | Rate: 100.4 iter/s | Time: 33.85s\n",
      "Iter  3420 | Train Loss: 0.414089 | LR: 0.0500 | Batch:   1 | Patience: 414/500 | Rate: 100.4 iter/s | Time: 34.07s\n",
      "Iter  3440 | Train Loss: 0.418102 | LR: 0.0500 | Batch:   1 | Patience: 418/500 | Rate: 100.3 iter/s | Time: 34.30s\n",
      "Iter  3460 | Train Loss: 0.424651 | LR: 0.0500 | Batch:   1 | Patience: 422/500 | Rate: 100.3 iter/s | Time: 34.50s\n",
      "Iter  3480 | Train Loss: 0.419561 | LR: 0.0500 | Batch:   1 | Patience: 426/500 | Rate: 100.3 iter/s | Time: 34.70s\n",
      "Iter  3500 | Train Loss: 0.414709 | LR: 0.0500 | Batch:   1 | Patience: 430/500 | Rate: 100.2 iter/s | Time: 34.92s\n",
      "Iter  3520 | Train Loss: 0.418740 | LR: 0.0500 | Batch:   1 | Patience: 434/500 | Rate: 100.2 iter/s | Time: 35.12s\n",
      "Iter  3540 | Train Loss: 0.419645 | LR: 0.0500 | Batch:   1 | Patience: 438/500 | Rate: 100.2 iter/s | Time: 35.32s\n",
      "Iter  3560 | Train Loss: 0.415891 | LR: 0.0500 | Batch:   1 | Patience: 442/500 | Rate: 100.2 iter/s | Time: 35.54s\n",
      "Iter  3580 | Train Loss: 0.412052 | LR: 0.0500 | Batch:   1 | Patience: 446/500 | Rate: 100.1 iter/s | Time: 35.78s\n",
      "Iter  3600 | Train Loss: 0.429928 | LR: 0.0500 | Batch:   1 | Patience: 450/500 | Rate: 100.0 iter/s | Time: 35.98s\n",
      "Iter  3620 | Train Loss: 0.425262 | LR: 0.0500 | Batch:   1 | Patience: 454/500 | Rate: 99.8 iter/s | Time: 36.26s\n",
      "Iter  3640 | Train Loss: 0.414592 | LR: 0.0500 | Batch:   1 | Patience: 458/500 | Rate: 99.9 iter/s | Time: 36.45s\n",
      "Iter  3660 | Train Loss: 0.411445 | LR: 0.0500 | Batch:   1 | Patience: 462/500 | Rate: 99.9 iter/s | Time: 36.65s\n",
      "Iter  3680 | Train Loss: 0.412236 | LR: 0.0500 | Batch:   1 | Patience: 466/500 | Rate: 99.9 iter/s | Time: 36.85s\n",
      "Iter  3700 | Train Loss: 0.419072 | LR: 0.0500 | Batch:   1 | Patience: 470/500 | Rate: 99.8 iter/s | Time: 37.06s\n",
      "Iter  3720 | Train Loss: 0.426566 | LR: 0.0500 | Batch:   1 | Patience: 474/500 | Rate: 99.9 iter/s | Time: 37.25s\n",
      "Iter  3740 | Train Loss: 0.425813 | LR: 0.0500 | Batch:   1 | Patience: 478/500 | Rate: 99.9 iter/s | Time: 37.45s\n",
      "Iter  3760 | Train Loss: 0.429028 | LR: 0.0500 | Batch:   1 | Patience: 482/500 | Rate: 99.9 iter/s | Time: 37.65s\n",
      "Iter  3780 | Train Loss: 0.438695 | LR: 0.0500 | Batch:   1 | Patience: 486/500 | Rate: 99.9 iter/s | Time: 37.85s\n",
      "Iter  3800 | Train Loss: 0.424980 | LR: 0.0500 | Batch:   1 | Patience: 490/500 | Rate: 99.8 iter/s | Time: 38.06s\n",
      "Iter  3820 | Train Loss: 0.423729 | LR: 0.0500 | Batch:   1 | Patience: 494/500 | Rate: 99.8 iter/s | Time: 38.27s\n",
      "Iter  3840 | Train Loss: 0.417568 | LR: 0.0500 | Batch:   1 | Patience: 498/500 | Rate: 99.8 iter/s | Time: 38.47s\n",
      "\n",
      "Early stopping triggered at iteration 3846\n",
      "No improvement in loss for 500 checks (threshold: 1e-06)\n",
      "Final loss: 0.417363\n",
      "Total training time: 38.54s\n",
      "Average time per iteration: 0.0100s\n",
      "Total iterations: 3846\n",
      "Approximate epochs: 0.01\n",
      "Starting SGD Iterations training\n",
      "Learning rate: 0.01, Batch size: 32\n",
      "Max iterations: 100000\n",
      "Early stopping: patience=500, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "Initial full dataset loss: 0.705891\n",
      "----------------------------------------------------------------------\n",
      "Iter    20 | Train Loss: 0.597429 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 105.9 iter/s | Time: 0.19s\n",
      "Iter    40 | Train Loss: 0.545700 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 103.1 iter/s | Time: 0.39s\n",
      "Iter    60 | Train Loss: 0.509102 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 101.3 iter/s | Time: 0.59s\n",
      "Iter    80 | Train Loss: 0.488222 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 102.1 iter/s | Time: 0.78s\n",
      "Iter   100 | Train Loss: 0.474679 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 102.2 iter/s | Time: 0.98s\n",
      "Iter   120 | Train Loss: 0.463948 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.9 iter/s | Time: 1.19s\n",
      "Iter   140 | Train Loss: 0.455386 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.4 iter/s | Time: 1.41s\n",
      "Iter   160 | Train Loss: 0.448240 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 98.6 iter/s | Time: 1.62s\n",
      "Iter   180 | Train Loss: 0.442192 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 97.9 iter/s | Time: 1.84s\n",
      "Iter   200 | Train Loss: 0.438421 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 97.5 iter/s | Time: 2.05s\n",
      "Iter   220 | Train Loss: 0.434659 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 97.2 iter/s | Time: 2.26s\n",
      "Iter   240 | Train Loss: 0.431850 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 96.7 iter/s | Time: 2.48s\n",
      "Iter   260 | Train Loss: 0.429117 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 96.4 iter/s | Time: 2.70s\n",
      "Iter   280 | Train Loss: 0.426751 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 96.1 iter/s | Time: 2.91s\n",
      "Iter   300 | Train Loss: 0.425218 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 96.3 iter/s | Time: 3.11s\n",
      "Iter   320 | Train Loss: 0.422565 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 96.6 iter/s | Time: 3.31s\n",
      "Iter   340 | Train Loss: 0.420425 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 96.7 iter/s | Time: 3.52s\n",
      "Iter   360 | Train Loss: 0.419016 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 97.0 iter/s | Time: 3.71s\n",
      "Iter   380 | Train Loss: 0.418085 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 97.1 iter/s | Time: 3.91s\n",
      "Iter   400 | Train Loss: 0.416968 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 97.4 iter/s | Time: 4.11s\n",
      "Iter   420 | Train Loss: 0.415943 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 95.7 iter/s | Time: 4.39s\n",
      "Iter   440 | Train Loss: 0.414905 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 96.0 iter/s | Time: 4.58s\n",
      "Iter   460 | Train Loss: 0.414282 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 96.4 iter/s | Time: 4.77s\n",
      "Iter   480 | Train Loss: 0.413147 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 96.8 iter/s | Time: 4.96s\n",
      "Iter   500 | Train Loss: 0.412624 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 97.1 iter/s | Time: 5.15s\n",
      "Iter   520 | Train Loss: 0.412041 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 97.5 iter/s | Time: 5.34s\n",
      "Iter   540 | Train Loss: 0.411229 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 97.7 iter/s | Time: 5.53s\n",
      "Iter   560 | Train Loss: 0.410373 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 97.8 iter/s | Time: 5.72s\n",
      "Iter   580 | Train Loss: 0.409698 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 98.1 iter/s | Time: 5.91s\n",
      "Iter   600 | Train Loss: 0.408924 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 98.3 iter/s | Time: 6.11s\n",
      "Iter   620 | Train Loss: 0.408237 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 98.4 iter/s | Time: 6.30s\n",
      "Iter   640 | Train Loss: 0.407615 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 98.6 iter/s | Time: 6.49s\n",
      "Iter   660 | Train Loss: 0.407021 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 98.7 iter/s | Time: 6.69s\n",
      "Iter   680 | Train Loss: 0.406357 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 98.9 iter/s | Time: 6.88s\n",
      "Iter   700 | Train Loss: 0.405862 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.0 iter/s | Time: 7.07s\n",
      "Iter   720 | Train Loss: 0.405617 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 99.2 iter/s | Time: 7.26s\n",
      "Iter   740 | Train Loss: 0.404978 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.4 iter/s | Time: 7.45s\n",
      "Iter   760 | Train Loss: 0.404676 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.5 iter/s | Time: 7.64s\n",
      "Iter   780 | Train Loss: 0.404228 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.6 iter/s | Time: 7.83s\n",
      "Iter   800 | Train Loss: 0.403759 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.6 iter/s | Time: 8.03s\n",
      "Iter   820 | Train Loss: 0.403325 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.7 iter/s | Time: 8.22s\n",
      "Iter   840 | Train Loss: 0.403097 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.9 iter/s | Time: 8.41s\n",
      "Iter   860 | Train Loss: 0.402787 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.0 iter/s | Time: 8.60s\n",
      "Iter   880 | Train Loss: 0.402618 | LR: 0.0100 | Batch:  32 | Patience:   1/500 | Rate: 100.1 iter/s | Time: 8.79s\n",
      "Iter   900 | Train Loss: 0.402187 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.2 iter/s | Time: 8.98s\n",
      "Iter   920 | Train Loss: 0.401832 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.3 iter/s | Time: 9.17s\n",
      "Iter   940 | Train Loss: 0.401498 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.4 iter/s | Time: 9.36s\n",
      "Iter   960 | Train Loss: 0.401008 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.5 iter/s | Time: 9.55s\n",
      "Iter   980 | Train Loss: 0.400755 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.6 iter/s | Time: 9.74s\n",
      "Iter  1000 | Train Loss: 0.400444 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.6 iter/s | Time: 9.94s\n",
      "Iter  1020 | Train Loss: 0.400153 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.7 iter/s | Time: 10.13s\n",
      "Iter  1040 | Train Loss: 0.399982 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.8 iter/s | Time: 10.32s\n",
      "Iter  1060 | Train Loss: 0.399725 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.7 iter/s | Time: 10.53s\n",
      "Iter  1080 | Train Loss: 0.399491 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.7 iter/s | Time: 10.72s\n",
      "Iter  1100 | Train Loss: 0.399371 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.8 iter/s | Time: 10.91s\n",
      "Iter  1120 | Train Loss: 0.399281 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 100.9 iter/s | Time: 11.10s\n",
      "Iter  1140 | Train Loss: 0.398906 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 101.0 iter/s | Time: 11.29s\n",
      "Iter  1160 | Train Loss: 0.398648 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.8 iter/s | Time: 11.51s\n",
      "Iter  1180 | Train Loss: 0.398451 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.8 iter/s | Time: 11.71s\n",
      "Iter  1200 | Train Loss: 0.398408 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 100.9 iter/s | Time: 11.90s\n",
      "Iter  1220 | Train Loss: 0.398371 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 100.9 iter/s | Time: 12.09s\n",
      "Iter  1240 | Train Loss: 0.398345 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 101.0 iter/s | Time: 12.28s\n",
      "Iter  1260 | Train Loss: 0.398203 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 101.0 iter/s | Time: 12.47s\n",
      "Iter  1280 | Train Loss: 0.398113 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 101.0 iter/s | Time: 12.67s\n",
      "Iter  1300 | Train Loss: 0.397920 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 101.1 iter/s | Time: 12.86s\n",
      "Iter  1320 | Train Loss: 0.397800 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 101.2 iter/s | Time: 13.05s\n",
      "Iter  1340 | Train Loss: 0.397650 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 101.2 iter/s | Time: 13.24s\n",
      "Iter  1360 | Train Loss: 0.397547 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 101.3 iter/s | Time: 13.43s\n",
      "Iter  1380 | Train Loss: 0.397391 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 101.3 iter/s | Time: 13.62s\n",
      "Iter  1400 | Train Loss: 0.397270 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 101.3 iter/s | Time: 13.82s\n",
      "Iter  1420 | Train Loss: 0.397262 | LR: 0.0100 | Batch:  32 | Patience:   1/500 | Rate: 101.3 iter/s | Time: 14.02s\n",
      "Iter  1440 | Train Loss: 0.397213 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 101.2 iter/s | Time: 14.23s\n",
      "Iter  1460 | Train Loss: 0.397086 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 101.0 iter/s | Time: 14.45s\n",
      "Iter  1480 | Train Loss: 0.397026 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.8 iter/s | Time: 14.68s\n",
      "Iter  1500 | Train Loss: 0.397087 | LR: 0.0100 | Batch:  32 | Patience:   4/500 | Rate: 100.6 iter/s | Time: 14.90s\n",
      "Iter  1520 | Train Loss: 0.396920 | LR: 0.0100 | Batch:  32 | Patience:   8/500 | Rate: 100.6 iter/s | Time: 15.11s\n",
      "Iter  1540 | Train Loss: 0.396785 | LR: 0.0100 | Batch:  32 | Patience:   1/500 | Rate: 100.7 iter/s | Time: 15.30s\n",
      "Iter  1560 | Train Loss: 0.396710 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 100.6 iter/s | Time: 15.50s\n",
      "Iter  1580 | Train Loss: 0.396543 | LR: 0.0100 | Batch:  32 | Patience:   1/500 | Rate: 100.6 iter/s | Time: 15.71s\n",
      "Iter  1600 | Train Loss: 0.396451 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.5 iter/s | Time: 15.92s\n",
      "Iter  1620 | Train Loss: 0.396424 | LR: 0.0100 | Batch:  32 | Patience:   1/500 | Rate: 100.5 iter/s | Time: 16.13s\n",
      "Iter  1640 | Train Loss: 0.396371 | LR: 0.0100 | Batch:  32 | Patience:   5/500 | Rate: 100.4 iter/s | Time: 16.33s\n",
      "Iter  1660 | Train Loss: 0.396393 | LR: 0.0100 | Batch:  32 | Patience:   9/500 | Rate: 100.4 iter/s | Time: 16.54s\n",
      "Iter  1680 | Train Loss: 0.396233 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.3 iter/s | Time: 16.74s\n",
      "Iter  1700 | Train Loss: 0.396126 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 100.3 iter/s | Time: 16.95s\n",
      "Iter  1720 | Train Loss: 0.396046 | LR: 0.0100 | Batch:  32 | Patience:   1/500 | Rate: 100.3 iter/s | Time: 17.15s\n",
      "Iter  1740 | Train Loss: 0.395983 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.3 iter/s | Time: 17.36s\n",
      "Iter  1760 | Train Loss: 0.395878 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.2 iter/s | Time: 17.56s\n",
      "Iter  1780 | Train Loss: 0.395883 | LR: 0.0100 | Batch:  32 | Patience:   3/500 | Rate: 100.2 iter/s | Time: 17.76s\n",
      "Iter  1800 | Train Loss: 0.395934 | LR: 0.0100 | Batch:  32 | Patience:   7/500 | Rate: 100.1 iter/s | Time: 17.97s\n",
      "Iter  1820 | Train Loss: 0.395888 | LR: 0.0100 | Batch:  32 | Patience:  11/500 | Rate: 100.1 iter/s | Time: 18.18s\n",
      "Iter  1840 | Train Loss: 0.395809 | LR: 0.0100 | Batch:  32 | Patience:   1/500 | Rate: 100.1 iter/s | Time: 18.38s\n",
      "Iter  1860 | Train Loss: 0.395719 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.1 iter/s | Time: 18.58s\n",
      "Iter  1880 | Train Loss: 0.395681 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.1 iter/s | Time: 18.79s\n",
      "Iter  1900 | Train Loss: 0.395616 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.1 iter/s | Time: 18.99s\n",
      "Iter  1920 | Train Loss: 0.395617 | LR: 0.0100 | Batch:  32 | Patience:   4/500 | Rate: 100.0 iter/s | Time: 19.19s\n",
      "Iter  1940 | Train Loss: 0.395535 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.0 iter/s | Time: 19.39s\n",
      "Iter  1960 | Train Loss: 0.395523 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 100.0 iter/s | Time: 19.61s\n",
      "Iter  1980 | Train Loss: 0.395516 | LR: 0.0100 | Batch:  32 | Patience:   4/500 | Rate: 99.9 iter/s | Time: 19.81s\n",
      "Iter  2000 | Train Loss: 0.395428 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.9 iter/s | Time: 20.02s\n",
      "Iter  2020 | Train Loss: 0.395399 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.9 iter/s | Time: 20.22s\n",
      "Iter  2040 | Train Loss: 0.395337 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.9 iter/s | Time: 20.42s\n",
      "Iter  2060 | Train Loss: 0.395298 | LR: 0.0100 | Batch:  32 | Patience:   1/500 | Rate: 99.9 iter/s | Time: 20.63s\n",
      "Iter  2080 | Train Loss: 0.395291 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 99.8 iter/s | Time: 20.83s\n",
      "Iter  2100 | Train Loss: 0.395224 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.8 iter/s | Time: 21.04s\n",
      "Iter  2120 | Train Loss: 0.395196 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 99.8 iter/s | Time: 21.24s\n",
      "Iter  2140 | Train Loss: 0.395148 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.8 iter/s | Time: 21.44s\n",
      "Iter  2160 | Train Loss: 0.395147 | LR: 0.0100 | Batch:  32 | Patience:   3/500 | Rate: 99.8 iter/s | Time: 21.65s\n",
      "Iter  2180 | Train Loss: 0.395127 | LR: 0.0100 | Batch:  32 | Patience:   1/500 | Rate: 99.7 iter/s | Time: 21.87s\n",
      "Iter  2200 | Train Loss: 0.395205 | LR: 0.0100 | Batch:  32 | Patience:   3/500 | Rate: 99.6 iter/s | Time: 22.08s\n",
      "Iter  2220 | Train Loss: 0.395057 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.6 iter/s | Time: 22.29s\n",
      "Iter  2240 | Train Loss: 0.395099 | LR: 0.0100 | Batch:  32 | Patience:   3/500 | Rate: 99.5 iter/s | Time: 22.50s\n",
      "Iter  2260 | Train Loss: 0.394982 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.5 iter/s | Time: 22.71s\n",
      "Iter  2280 | Train Loss: 0.394895 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.5 iter/s | Time: 22.93s\n",
      "Iter  2300 | Train Loss: 0.394892 | LR: 0.0100 | Batch:  32 | Patience:   4/500 | Rate: 99.5 iter/s | Time: 23.13s\n",
      "Iter  2320 | Train Loss: 0.394837 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.5 iter/s | Time: 23.33s\n",
      "Iter  2340 | Train Loss: 0.394770 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.5 iter/s | Time: 23.52s\n",
      "Iter  2360 | Train Loss: 0.394792 | LR: 0.0100 | Batch:  32 | Patience:   4/500 | Rate: 99.5 iter/s | Time: 23.73s\n",
      "Iter  2380 | Train Loss: 0.394786 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 99.5 iter/s | Time: 23.93s\n",
      "Iter  2400 | Train Loss: 0.394732 | LR: 0.0100 | Batch:  32 | Patience:   1/500 | Rate: 99.5 iter/s | Time: 24.13s\n",
      "Iter  2420 | Train Loss: 0.394749 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.4 iter/s | Time: 24.34s\n",
      "Iter  2440 | Train Loss: 0.394742 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.4 iter/s | Time: 24.54s\n",
      "Iter  2460 | Train Loss: 0.394670 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.4 iter/s | Time: 24.74s\n",
      "Iter  2480 | Train Loss: 0.394661 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.4 iter/s | Time: 24.94s\n",
      "Iter  2500 | Train Loss: 0.394644 | LR: 0.0100 | Batch:  32 | Patience:   1/500 | Rate: 99.4 iter/s | Time: 25.14s\n",
      "Iter  2520 | Train Loss: 0.394705 | LR: 0.0100 | Batch:  32 | Patience:   5/500 | Rate: 99.5 iter/s | Time: 25.34s\n",
      "Iter  2540 | Train Loss: 0.394811 | LR: 0.0100 | Batch:  32 | Patience:   9/500 | Rate: 99.5 iter/s | Time: 25.54s\n",
      "Iter  2560 | Train Loss: 0.394772 | LR: 0.0100 | Batch:  32 | Patience:  13/500 | Rate: 99.5 iter/s | Time: 25.73s\n",
      "Iter  2580 | Train Loss: 0.394758 | LR: 0.0100 | Batch:  32 | Patience:  17/500 | Rate: 99.5 iter/s | Time: 25.92s\n",
      "Iter  2600 | Train Loss: 0.394768 | LR: 0.0100 | Batch:  32 | Patience:  21/500 | Rate: 99.6 iter/s | Time: 26.12s\n",
      "Iter  2620 | Train Loss: 0.394702 | LR: 0.0100 | Batch:  32 | Patience:  25/500 | Rate: 99.5 iter/s | Time: 26.32s\n",
      "Iter  2640 | Train Loss: 0.394681 | LR: 0.0100 | Batch:  32 | Patience:  29/500 | Rate: 99.6 iter/s | Time: 26.51s\n",
      "Iter  2660 | Train Loss: 0.394719 | LR: 0.0100 | Batch:  32 | Patience:  33/500 | Rate: 99.6 iter/s | Time: 26.71s\n",
      "Iter  2680 | Train Loss: 0.394689 | LR: 0.0100 | Batch:  32 | Patience:  37/500 | Rate: 99.6 iter/s | Time: 26.91s\n",
      "Iter  2700 | Train Loss: 0.394642 | LR: 0.0100 | Batch:  32 | Patience:  41/500 | Rate: 99.6 iter/s | Time: 27.10s\n",
      "Iter  2720 | Train Loss: 0.394609 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.7 iter/s | Time: 27.29s\n",
      "Iter  2740 | Train Loss: 0.394633 | LR: 0.0100 | Batch:  32 | Patience:   3/500 | Rate: 99.7 iter/s | Time: 27.49s\n",
      "Iter  2760 | Train Loss: 0.394577 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 99.7 iter/s | Time: 27.68s\n",
      "Iter  2780 | Train Loss: 0.394545 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.7 iter/s | Time: 27.89s\n",
      "Iter  2800 | Train Loss: 0.394550 | LR: 0.0100 | Batch:  32 | Patience:   1/500 | Rate: 99.7 iter/s | Time: 28.09s\n",
      "Iter  2820 | Train Loss: 0.394532 | LR: 0.0100 | Batch:  32 | Patience:   5/500 | Rate: 99.7 iter/s | Time: 28.29s\n",
      "Iter  2840 | Train Loss: 0.394536 | LR: 0.0100 | Batch:  32 | Patience:   3/500 | Rate: 99.7 iter/s | Time: 28.49s\n",
      "Iter  2860 | Train Loss: 0.394538 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.7 iter/s | Time: 28.69s\n",
      "Iter  2880 | Train Loss: 0.394539 | LR: 0.0100 | Batch:  32 | Patience:   4/500 | Rate: 99.7 iter/s | Time: 28.89s\n",
      "Iter  2900 | Train Loss: 0.394542 | LR: 0.0100 | Batch:  32 | Patience:   8/500 | Rate: 99.7 iter/s | Time: 29.09s\n",
      "Iter  2920 | Train Loss: 0.394500 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.7 iter/s | Time: 29.29s\n",
      "Iter  2940 | Train Loss: 0.394488 | LR: 0.0100 | Batch:  32 | Patience:   1/500 | Rate: 99.7 iter/s | Time: 29.49s\n",
      "Iter  2960 | Train Loss: 0.394444 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.7 iter/s | Time: 29.69s\n",
      "Iter  2980 | Train Loss: 0.394445 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 99.7 iter/s | Time: 29.89s\n",
      "Iter  3000 | Train Loss: 0.394503 | LR: 0.0100 | Batch:  32 | Patience:   6/500 | Rate: 99.7 iter/s | Time: 30.09s\n",
      "Iter  3020 | Train Loss: 0.394498 | LR: 0.0100 | Batch:  32 | Patience:  10/500 | Rate: 99.7 iter/s | Time: 30.29s\n",
      "Iter  3040 | Train Loss: 0.394452 | LR: 0.0100 | Batch:  32 | Patience:  14/500 | Rate: 99.7 iter/s | Time: 30.50s\n",
      "Iter  3060 | Train Loss: 0.394449 | LR: 0.0100 | Batch:  32 | Patience:  18/500 | Rate: 99.7 iter/s | Time: 30.70s\n",
      "Iter  3080 | Train Loss: 0.394435 | LR: 0.0100 | Batch:  32 | Patience:  22/500 | Rate: 99.7 iter/s | Time: 30.90s\n",
      "Iter  3100 | Train Loss: 0.394420 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.7 iter/s | Time: 31.10s\n",
      "Iter  3120 | Train Loss: 0.394405 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.7 iter/s | Time: 31.30s\n",
      "Iter  3140 | Train Loss: 0.394383 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.7 iter/s | Time: 31.50s\n",
      "Iter  3160 | Train Loss: 0.394367 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 99.7 iter/s | Time: 31.70s\n",
      "Iter  3180 | Train Loss: 0.394368 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.7 iter/s | Time: 31.90s\n",
      "Iter  3200 | Train Loss: 0.394408 | LR: 0.0100 | Batch:  32 | Patience:   4/500 | Rate: 99.7 iter/s | Time: 32.10s\n",
      "Iter  3220 | Train Loss: 0.394419 | LR: 0.0100 | Batch:  32 | Patience:   8/500 | Rate: 99.7 iter/s | Time: 32.30s\n",
      "Iter  3240 | Train Loss: 0.394367 | LR: 0.0100 | Batch:  32 | Patience:  12/500 | Rate: 99.7 iter/s | Time: 32.50s\n",
      "Iter  3260 | Train Loss: 0.394367 | LR: 0.0100 | Batch:  32 | Patience:  16/500 | Rate: 99.7 iter/s | Time: 32.69s\n",
      "Iter  3280 | Train Loss: 0.394360 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.7 iter/s | Time: 32.90s\n",
      "Iter  3300 | Train Loss: 0.394337 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.7 iter/s | Time: 33.09s\n",
      "Iter  3320 | Train Loss: 0.394361 | LR: 0.0100 | Batch:  32 | Patience:   3/500 | Rate: 99.7 iter/s | Time: 33.29s\n",
      "Iter  3340 | Train Loss: 0.394347 | LR: 0.0100 | Batch:  32 | Patience:   7/500 | Rate: 99.8 iter/s | Time: 33.48s\n",
      "Iter  3360 | Train Loss: 0.394336 | LR: 0.0100 | Batch:  32 | Patience:   1/500 | Rate: 99.8 iter/s | Time: 33.68s\n",
      "Iter  3380 | Train Loss: 0.394359 | LR: 0.0100 | Batch:  32 | Patience:   5/500 | Rate: 99.8 iter/s | Time: 33.87s\n",
      "Iter  3400 | Train Loss: 0.394410 | LR: 0.0100 | Batch:  32 | Patience:   9/500 | Rate: 99.8 iter/s | Time: 34.07s\n",
      "Iter  3420 | Train Loss: 0.394463 | LR: 0.0100 | Batch:  32 | Patience:  13/500 | Rate: 99.8 iter/s | Time: 34.26s\n",
      "Iter  3440 | Train Loss: 0.394457 | LR: 0.0100 | Batch:  32 | Patience:  17/500 | Rate: 99.8 iter/s | Time: 34.46s\n",
      "Iter  3460 | Train Loss: 0.394525 | LR: 0.0100 | Batch:  32 | Patience:  21/500 | Rate: 99.8 iter/s | Time: 34.67s\n",
      "Iter  3480 | Train Loss: 0.394470 | LR: 0.0100 | Batch:  32 | Patience:  25/500 | Rate: 99.8 iter/s | Time: 34.86s\n",
      "Iter  3500 | Train Loss: 0.394464 | LR: 0.0100 | Batch:  32 | Patience:  29/500 | Rate: 99.8 iter/s | Time: 35.06s\n",
      "Iter  3520 | Train Loss: 0.394478 | LR: 0.0100 | Batch:  32 | Patience:  33/500 | Rate: 99.9 iter/s | Time: 35.25s\n",
      "Iter  3540 | Train Loss: 0.394407 | LR: 0.0100 | Batch:  32 | Patience:  37/500 | Rate: 99.9 iter/s | Time: 35.44s\n",
      "Iter  3560 | Train Loss: 0.394429 | LR: 0.0100 | Batch:  32 | Patience:  41/500 | Rate: 99.9 iter/s | Time: 35.64s\n",
      "Iter  3580 | Train Loss: 0.394376 | LR: 0.0100 | Batch:  32 | Patience:  45/500 | Rate: 99.9 iter/s | Time: 35.84s\n",
      "Iter  3600 | Train Loss: 0.394374 | LR: 0.0100 | Batch:  32 | Patience:  49/500 | Rate: 99.9 iter/s | Time: 36.04s\n",
      "Iter  3620 | Train Loss: 0.394324 | LR: 0.0100 | Batch:  32 | Patience:  53/500 | Rate: 99.9 iter/s | Time: 36.24s\n",
      "Iter  3640 | Train Loss: 0.394311 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.9 iter/s | Time: 36.44s\n",
      "Iter  3660 | Train Loss: 0.394288 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.9 iter/s | Time: 36.63s\n",
      "Iter  3680 | Train Loss: 0.394301 | LR: 0.0100 | Batch:  32 | Patience:   4/500 | Rate: 99.9 iter/s | Time: 36.84s\n",
      "Iter  3700 | Train Loss: 0.394233 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.9 iter/s | Time: 37.04s\n",
      "Iter  3720 | Train Loss: 0.394237 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 99.9 iter/s | Time: 37.24s\n",
      "Iter  3740 | Train Loss: 0.394249 | LR: 0.0100 | Batch:  32 | Patience:   6/500 | Rate: 99.9 iter/s | Time: 37.44s\n",
      "Iter  3760 | Train Loss: 0.394240 | LR: 0.0100 | Batch:  32 | Patience:   1/500 | Rate: 99.9 iter/s | Time: 37.65s\n",
      "Iter  3780 | Train Loss: 0.394253 | LR: 0.0100 | Batch:  32 | Patience:   5/500 | Rate: 99.9 iter/s | Time: 37.85s\n",
      "Iter  3800 | Train Loss: 0.394277 | LR: 0.0100 | Batch:  32 | Patience:   9/500 | Rate: 99.9 iter/s | Time: 38.06s\n",
      "Iter  3820 | Train Loss: 0.394274 | LR: 0.0100 | Batch:  32 | Patience:  13/500 | Rate: 99.8 iter/s | Time: 38.26s\n",
      "Iter  3840 | Train Loss: 0.394259 | LR: 0.0100 | Batch:  32 | Patience:  17/500 | Rate: 99.8 iter/s | Time: 38.48s\n",
      "Iter  3860 | Train Loss: 0.394269 | LR: 0.0100 | Batch:  32 | Patience:  21/500 | Rate: 99.7 iter/s | Time: 38.72s\n",
      "Iter  3880 | Train Loss: 0.394282 | LR: 0.0100 | Batch:  32 | Patience:  25/500 | Rate: 99.7 iter/s | Time: 38.92s\n",
      "Iter  3900 | Train Loss: 0.394273 | LR: 0.0100 | Batch:  32 | Patience:  29/500 | Rate: 99.6 iter/s | Time: 39.16s\n",
      "Iter  3920 | Train Loss: 0.394270 | LR: 0.0100 | Batch:  32 | Patience:  33/500 | Rate: 99.6 iter/s | Time: 39.36s\n",
      "Iter  3940 | Train Loss: 0.394277 | LR: 0.0100 | Batch:  32 | Patience:  37/500 | Rate: 99.6 iter/s | Time: 39.57s\n",
      "Iter  3960 | Train Loss: 0.394296 | LR: 0.0100 | Batch:  32 | Patience:  41/500 | Rate: 99.6 iter/s | Time: 39.77s\n",
      "Iter  3980 | Train Loss: 0.394291 | LR: 0.0100 | Batch:  32 | Patience:  45/500 | Rate: 99.6 iter/s | Time: 39.97s\n",
      "Iter  4000 | Train Loss: 0.394321 | LR: 0.0100 | Batch:  32 | Patience:  49/500 | Rate: 99.6 iter/s | Time: 40.17s\n",
      "Iter  4020 | Train Loss: 0.394311 | LR: 0.0100 | Batch:  32 | Patience:  53/500 | Rate: 99.5 iter/s | Time: 40.38s\n",
      "Iter  4040 | Train Loss: 0.394283 | LR: 0.0100 | Batch:  32 | Patience:  57/500 | Rate: 99.6 iter/s | Time: 40.58s\n",
      "Iter  4060 | Train Loss: 0.394294 | LR: 0.0100 | Batch:  32 | Patience:  61/500 | Rate: 99.4 iter/s | Time: 40.84s\n",
      "Iter  4080 | Train Loss: 0.394329 | LR: 0.0100 | Batch:  32 | Patience:  65/500 | Rate: 99.4 iter/s | Time: 41.05s\n",
      "Iter  4100 | Train Loss: 0.394318 | LR: 0.0100 | Batch:  32 | Patience:  69/500 | Rate: 99.4 iter/s | Time: 41.24s\n",
      "Iter  4120 | Train Loss: 0.394336 | LR: 0.0100 | Batch:  32 | Patience:  73/500 | Rate: 99.4 iter/s | Time: 41.44s\n",
      "Iter  4140 | Train Loss: 0.394300 | LR: 0.0100 | Batch:  32 | Patience:  77/500 | Rate: 99.5 iter/s | Time: 41.63s\n",
      "Iter  4160 | Train Loss: 0.394304 | LR: 0.0100 | Batch:  32 | Patience:  81/500 | Rate: 99.4 iter/s | Time: 41.85s\n",
      "Iter  4180 | Train Loss: 0.394276 | LR: 0.0100 | Batch:  32 | Patience:  85/500 | Rate: 99.4 iter/s | Time: 42.04s\n",
      "Iter  4200 | Train Loss: 0.394268 | LR: 0.0100 | Batch:  32 | Patience:  89/500 | Rate: 99.4 iter/s | Time: 42.23s\n",
      "Iter  4220 | Train Loss: 0.394298 | LR: 0.0100 | Batch:  32 | Patience:  93/500 | Rate: 99.5 iter/s | Time: 42.43s\n",
      "Iter  4240 | Train Loss: 0.394260 | LR: 0.0100 | Batch:  32 | Patience:  97/500 | Rate: 99.5 iter/s | Time: 42.62s\n",
      "Iter  4260 | Train Loss: 0.394226 | LR: 0.0100 | Batch:  32 | Patience: 101/500 | Rate: 99.5 iter/s | Time: 42.81s\n",
      "Iter  4280 | Train Loss: 0.394226 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.5 iter/s | Time: 43.01s\n",
      "Iter  4300 | Train Loss: 0.394233 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 99.5 iter/s | Time: 43.21s\n",
      "Iter  4320 | Train Loss: 0.394217 | LR: 0.0100 | Batch:  32 | Patience:   6/500 | Rate: 99.5 iter/s | Time: 43.41s\n",
      "Iter  4340 | Train Loss: 0.394225 | LR: 0.0100 | Batch:  32 | Patience:  10/500 | Rate: 99.5 iter/s | Time: 43.60s\n",
      "Iter  4360 | Train Loss: 0.394252 | LR: 0.0100 | Batch:  32 | Patience:  14/500 | Rate: 99.5 iter/s | Time: 43.80s\n",
      "Iter  4380 | Train Loss: 0.394274 | LR: 0.0100 | Batch:  32 | Patience:  18/500 | Rate: 99.5 iter/s | Time: 44.00s\n",
      "Iter  4400 | Train Loss: 0.394313 | LR: 0.0100 | Batch:  32 | Patience:  22/500 | Rate: 99.6 iter/s | Time: 44.19s\n",
      "Iter  4420 | Train Loss: 0.394311 | LR: 0.0100 | Batch:  32 | Patience:  26/500 | Rate: 99.6 iter/s | Time: 44.39s\n",
      "Iter  4440 | Train Loss: 0.394333 | LR: 0.0100 | Batch:  32 | Patience:  30/500 | Rate: 99.6 iter/s | Time: 44.59s\n",
      "Iter  4460 | Train Loss: 0.394329 | LR: 0.0100 | Batch:  32 | Patience:  34/500 | Rate: 99.6 iter/s | Time: 44.80s\n",
      "Iter  4480 | Train Loss: 0.394330 | LR: 0.0100 | Batch:  32 | Patience:  38/500 | Rate: 99.6 iter/s | Time: 44.99s\n",
      "Iter  4500 | Train Loss: 0.394338 | LR: 0.0100 | Batch:  32 | Patience:  42/500 | Rate: 99.6 iter/s | Time: 45.19s\n",
      "Iter  4520 | Train Loss: 0.394331 | LR: 0.0100 | Batch:  32 | Patience:  46/500 | Rate: 99.5 iter/s | Time: 45.45s\n",
      "Iter  4540 | Train Loss: 0.394320 | LR: 0.0100 | Batch:  32 | Patience:  50/500 | Rate: 99.5 iter/s | Time: 45.65s\n",
      "Iter  4560 | Train Loss: 0.394328 | LR: 0.0100 | Batch:  32 | Patience:  54/500 | Rate: 99.5 iter/s | Time: 45.85s\n",
      "Iter  4580 | Train Loss: 0.394317 | LR: 0.0100 | Batch:  32 | Patience:  58/500 | Rate: 99.5 iter/s | Time: 46.05s\n",
      "Iter  4600 | Train Loss: 0.394359 | LR: 0.0100 | Batch:  32 | Patience:  62/500 | Rate: 99.5 iter/s | Time: 46.24s\n",
      "Iter  4620 | Train Loss: 0.394340 | LR: 0.0100 | Batch:  32 | Patience:  66/500 | Rate: 99.5 iter/s | Time: 46.44s\n",
      "Iter  4640 | Train Loss: 0.394322 | LR: 0.0100 | Batch:  32 | Patience:  70/500 | Rate: 99.5 iter/s | Time: 46.63s\n",
      "Iter  4660 | Train Loss: 0.394299 | LR: 0.0100 | Batch:  32 | Patience:  74/500 | Rate: 99.5 iter/s | Time: 46.85s\n",
      "Iter  4680 | Train Loss: 0.394299 | LR: 0.0100 | Batch:  32 | Patience:  78/500 | Rate: 99.4 iter/s | Time: 47.08s\n",
      "Iter  4700 | Train Loss: 0.394270 | LR: 0.0100 | Batch:  32 | Patience:  82/500 | Rate: 99.4 iter/s | Time: 47.27s\n",
      "Iter  4720 | Train Loss: 0.394268 | LR: 0.0100 | Batch:  32 | Patience:  86/500 | Rate: 99.4 iter/s | Time: 47.47s\n",
      "Iter  4740 | Train Loss: 0.394234 | LR: 0.0100 | Batch:  32 | Patience:  90/500 | Rate: 99.4 iter/s | Time: 47.67s\n",
      "Iter  4760 | Train Loss: 0.394193 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.4 iter/s | Time: 47.87s\n",
      "Iter  4780 | Train Loss: 0.394197 | LR: 0.0100 | Batch:  32 | Patience:   1/500 | Rate: 99.4 iter/s | Time: 48.07s\n",
      "Iter  4800 | Train Loss: 0.394212 | LR: 0.0100 | Batch:  32 | Patience:   5/500 | Rate: 99.4 iter/s | Time: 48.27s\n",
      "Iter  4820 | Train Loss: 0.394213 | LR: 0.0100 | Batch:  32 | Patience:   9/500 | Rate: 99.4 iter/s | Time: 48.49s\n",
      "Iter  4840 | Train Loss: 0.394232 | LR: 0.0100 | Batch:  32 | Patience:  13/500 | Rate: 99.4 iter/s | Time: 48.69s\n",
      "Iter  4860 | Train Loss: 0.394225 | LR: 0.0100 | Batch:  32 | Patience:  17/500 | Rate: 99.4 iter/s | Time: 48.90s\n",
      "Iter  4880 | Train Loss: 0.394219 | LR: 0.0100 | Batch:  32 | Patience:  21/500 | Rate: 99.4 iter/s | Time: 49.10s\n",
      "Iter  4900 | Train Loss: 0.394213 | LR: 0.0100 | Batch:  32 | Patience:  25/500 | Rate: 99.4 iter/s | Time: 49.30s\n",
      "Iter  4920 | Train Loss: 0.394248 | LR: 0.0100 | Batch:  32 | Patience:  29/500 | Rate: 99.4 iter/s | Time: 49.51s\n",
      "Iter  4940 | Train Loss: 0.394211 | LR: 0.0100 | Batch:  32 | Patience:  33/500 | Rate: 99.4 iter/s | Time: 49.72s\n",
      "Iter  4960 | Train Loss: 0.394218 | LR: 0.0100 | Batch:  32 | Patience:  37/500 | Rate: 99.3 iter/s | Time: 49.94s\n",
      "Iter  4980 | Train Loss: 0.394188 | LR: 0.0100 | Batch:  32 | Patience:  41/500 | Rate: 99.3 iter/s | Time: 50.15s\n",
      "Iter  5000 | Train Loss: 0.394204 | LR: 0.0100 | Batch:  32 | Patience:  45/500 | Rate: 99.3 iter/s | Time: 50.36s\n",
      "Iter  5020 | Train Loss: 0.394187 | LR: 0.0100 | Batch:  32 | Patience:  49/500 | Rate: 99.3 iter/s | Time: 50.58s\n",
      "Iter  5040 | Train Loss: 0.394219 | LR: 0.0100 | Batch:  32 | Patience:  53/500 | Rate: 99.2 iter/s | Time: 50.78s\n",
      "Iter  5060 | Train Loss: 0.394199 | LR: 0.0100 | Batch:  32 | Patience:  57/500 | Rate: 99.2 iter/s | Time: 51.00s\n",
      "Iter  5080 | Train Loss: 0.394188 | LR: 0.0100 | Batch:  32 | Patience:  61/500 | Rate: 99.2 iter/s | Time: 51.21s\n",
      "Iter  5100 | Train Loss: 0.394195 | LR: 0.0100 | Batch:  32 | Patience:  65/500 | Rate: 99.2 iter/s | Time: 51.42s\n",
      "Iter  5120 | Train Loss: 0.394195 | LR: 0.0100 | Batch:  32 | Patience:  69/500 | Rate: 99.2 iter/s | Time: 51.63s\n",
      "Iter  5140 | Train Loss: 0.394215 | LR: 0.0100 | Batch:  32 | Patience:  73/500 | Rate: 99.2 iter/s | Time: 51.83s\n",
      "Iter  5160 | Train Loss: 0.394212 | LR: 0.0100 | Batch:  32 | Patience:  77/500 | Rate: 99.2 iter/s | Time: 52.03s\n",
      "Iter  5180 | Train Loss: 0.394212 | LR: 0.0100 | Batch:  32 | Patience:  81/500 | Rate: 99.2 iter/s | Time: 52.23s\n",
      "Iter  5200 | Train Loss: 0.394199 | LR: 0.0100 | Batch:  32 | Patience:  85/500 | Rate: 99.2 iter/s | Time: 52.44s\n",
      "Iter  5220 | Train Loss: 0.394215 | LR: 0.0100 | Batch:  32 | Patience:  89/500 | Rate: 99.1 iter/s | Time: 52.67s\n",
      "Iter  5240 | Train Loss: 0.394222 | LR: 0.0100 | Batch:  32 | Patience:  93/500 | Rate: 99.1 iter/s | Time: 52.88s\n",
      "Iter  5260 | Train Loss: 0.394209 | LR: 0.0100 | Batch:  32 | Patience:  97/500 | Rate: 99.1 iter/s | Time: 53.09s\n",
      "Iter  5280 | Train Loss: 0.394213 | LR: 0.0100 | Batch:  32 | Patience: 101/500 | Rate: 99.1 iter/s | Time: 53.29s\n",
      "Iter  5300 | Train Loss: 0.394190 | LR: 0.0100 | Batch:  32 | Patience: 105/500 | Rate: 99.1 iter/s | Time: 53.48s\n",
      "Iter  5320 | Train Loss: 0.394215 | LR: 0.0100 | Batch:  32 | Patience: 109/500 | Rate: 99.1 iter/s | Time: 53.67s\n",
      "Iter  5340 | Train Loss: 0.394206 | LR: 0.0100 | Batch:  32 | Patience: 113/500 | Rate: 99.1 iter/s | Time: 53.87s\n",
      "Iter  5360 | Train Loss: 0.394219 | LR: 0.0100 | Batch:  32 | Patience: 117/500 | Rate: 99.1 iter/s | Time: 54.07s\n",
      "Iter  5380 | Train Loss: 0.394197 | LR: 0.0100 | Batch:  32 | Patience: 121/500 | Rate: 99.1 iter/s | Time: 54.26s\n",
      "Iter  5400 | Train Loss: 0.394195 | LR: 0.0100 | Batch:  32 | Patience: 125/500 | Rate: 99.2 iter/s | Time: 54.46s\n",
      "Iter  5420 | Train Loss: 0.394232 | LR: 0.0100 | Batch:  32 | Patience: 129/500 | Rate: 99.2 iter/s | Time: 54.66s\n",
      "Iter  5440 | Train Loss: 0.394258 | LR: 0.0100 | Batch:  32 | Patience: 133/500 | Rate: 99.2 iter/s | Time: 54.86s\n",
      "Iter  5460 | Train Loss: 0.394250 | LR: 0.0100 | Batch:  32 | Patience: 137/500 | Rate: 99.1 iter/s | Time: 55.07s\n",
      "Iter  5480 | Train Loss: 0.394243 | LR: 0.0100 | Batch:  32 | Patience: 141/500 | Rate: 99.2 iter/s | Time: 55.27s\n",
      "Iter  5500 | Train Loss: 0.394221 | LR: 0.0100 | Batch:  32 | Patience: 145/500 | Rate: 99.2 iter/s | Time: 55.47s\n",
      "Iter  5520 | Train Loss: 0.394221 | LR: 0.0100 | Batch:  32 | Patience: 149/500 | Rate: 99.2 iter/s | Time: 55.67s\n",
      "Iter  5540 | Train Loss: 0.394224 | LR: 0.0100 | Batch:  32 | Patience: 153/500 | Rate: 99.2 iter/s | Time: 55.87s\n",
      "Iter  5560 | Train Loss: 0.394243 | LR: 0.0100 | Batch:  32 | Patience: 157/500 | Rate: 99.2 iter/s | Time: 56.08s\n",
      "Iter  5580 | Train Loss: 0.394233 | LR: 0.0100 | Batch:  32 | Patience: 161/500 | Rate: 99.2 iter/s | Time: 56.28s\n",
      "Iter  5600 | Train Loss: 0.394237 | LR: 0.0100 | Batch:  32 | Patience: 165/500 | Rate: 99.2 iter/s | Time: 56.48s\n",
      "Iter  5620 | Train Loss: 0.394220 | LR: 0.0100 | Batch:  32 | Patience: 169/500 | Rate: 99.1 iter/s | Time: 56.69s\n",
      "Iter  5640 | Train Loss: 0.394205 | LR: 0.0100 | Batch:  32 | Patience: 173/500 | Rate: 99.1 iter/s | Time: 56.90s\n",
      "Iter  5660 | Train Loss: 0.394260 | LR: 0.0100 | Batch:  32 | Patience: 177/500 | Rate: 99.1 iter/s | Time: 57.12s\n",
      "Iter  5680 | Train Loss: 0.394304 | LR: 0.0100 | Batch:  32 | Patience: 181/500 | Rate: 99.1 iter/s | Time: 57.33s\n",
      "Iter  5700 | Train Loss: 0.394308 | LR: 0.0100 | Batch:  32 | Patience: 185/500 | Rate: 99.1 iter/s | Time: 57.54s\n",
      "Iter  5720 | Train Loss: 0.394336 | LR: 0.0100 | Batch:  32 | Patience: 189/500 | Rate: 99.1 iter/s | Time: 57.74s\n",
      "Iter  5740 | Train Loss: 0.394373 | LR: 0.0100 | Batch:  32 | Patience: 193/500 | Rate: 99.0 iter/s | Time: 57.96s\n",
      "Iter  5760 | Train Loss: 0.394271 | LR: 0.0100 | Batch:  32 | Patience: 197/500 | Rate: 99.0 iter/s | Time: 58.17s\n",
      "Iter  5780 | Train Loss: 0.394270 | LR: 0.0100 | Batch:  32 | Patience: 201/500 | Rate: 99.0 iter/s | Time: 58.37s\n",
      "Iter  5800 | Train Loss: 0.394259 | LR: 0.0100 | Batch:  32 | Patience: 205/500 | Rate: 99.0 iter/s | Time: 58.59s\n",
      "Iter  5820 | Train Loss: 0.394299 | LR: 0.0100 | Batch:  32 | Patience: 209/500 | Rate: 99.0 iter/s | Time: 58.81s\n",
      "Iter  5840 | Train Loss: 0.394368 | LR: 0.0100 | Batch:  32 | Patience: 213/500 | Rate: 99.0 iter/s | Time: 59.02s\n",
      "Iter  5860 | Train Loss: 0.394387 | LR: 0.0100 | Batch:  32 | Patience: 217/500 | Rate: 98.9 iter/s | Time: 59.23s\n",
      "Iter  5880 | Train Loss: 0.394283 | LR: 0.0100 | Batch:  32 | Patience: 221/500 | Rate: 98.9 iter/s | Time: 59.44s\n",
      "Iter  5900 | Train Loss: 0.394257 | LR: 0.0100 | Batch:  32 | Patience: 225/500 | Rate: 98.9 iter/s | Time: 59.65s\n",
      "Iter  5920 | Train Loss: 0.394234 | LR: 0.0100 | Batch:  32 | Patience: 229/500 | Rate: 98.9 iter/s | Time: 59.85s\n",
      "Iter  5940 | Train Loss: 0.394219 | LR: 0.0100 | Batch:  32 | Patience: 233/500 | Rate: 98.9 iter/s | Time: 60.07s\n",
      "Iter  5960 | Train Loss: 0.394212 | LR: 0.0100 | Batch:  32 | Patience: 237/500 | Rate: 98.9 iter/s | Time: 60.29s\n",
      "Iter  5980 | Train Loss: 0.394161 | LR: 0.0100 | Batch:  32 | Patience:   1/500 | Rate: 98.8 iter/s | Time: 60.50s\n",
      "Iter  6000 | Train Loss: 0.394155 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 98.8 iter/s | Time: 60.72s\n",
      "Iter  6020 | Train Loss: 0.394151 | LR: 0.0100 | Batch:  32 | Patience:   6/500 | Rate: 98.8 iter/s | Time: 60.95s\n",
      "Iter  6040 | Train Loss: 0.394187 | LR: 0.0100 | Batch:  32 | Patience:  10/500 | Rate: 98.7 iter/s | Time: 61.19s\n",
      "Iter  6060 | Train Loss: 0.394218 | LR: 0.0100 | Batch:  32 | Patience:  14/500 | Rate: 98.7 iter/s | Time: 61.41s\n",
      "Iter  6080 | Train Loss: 0.394250 | LR: 0.0100 | Batch:  32 | Patience:  18/500 | Rate: 98.7 iter/s | Time: 61.61s\n",
      "Iter  6100 | Train Loss: 0.394219 | LR: 0.0100 | Batch:  32 | Patience:  22/500 | Rate: 98.7 iter/s | Time: 61.82s\n",
      "Iter  6120 | Train Loss: 0.394233 | LR: 0.0100 | Batch:  32 | Patience:  26/500 | Rate: 98.7 iter/s | Time: 62.03s\n",
      "Iter  6140 | Train Loss: 0.394271 | LR: 0.0100 | Batch:  32 | Patience:  30/500 | Rate: 98.7 iter/s | Time: 62.23s\n",
      "Iter  6160 | Train Loss: 0.394286 | LR: 0.0100 | Batch:  32 | Patience:  34/500 | Rate: 98.6 iter/s | Time: 62.45s\n",
      "Iter  6180 | Train Loss: 0.394299 | LR: 0.0100 | Batch:  32 | Patience:  38/500 | Rate: 98.7 iter/s | Time: 62.64s\n",
      "Iter  6200 | Train Loss: 0.394284 | LR: 0.0100 | Batch:  32 | Patience:  42/500 | Rate: 98.7 iter/s | Time: 62.84s\n",
      "Iter  6220 | Train Loss: 0.394254 | LR: 0.0100 | Batch:  32 | Patience:  46/500 | Rate: 98.7 iter/s | Time: 63.03s\n",
      "Iter  6240 | Train Loss: 0.394301 | LR: 0.0100 | Batch:  32 | Patience:  50/500 | Rate: 98.7 iter/s | Time: 63.23s\n",
      "Iter  6260 | Train Loss: 0.394318 | LR: 0.0100 | Batch:  32 | Patience:  54/500 | Rate: 98.7 iter/s | Time: 63.42s\n",
      "Iter  6280 | Train Loss: 0.394348 | LR: 0.0100 | Batch:  32 | Patience:  58/500 | Rate: 98.7 iter/s | Time: 63.61s\n",
      "Iter  6300 | Train Loss: 0.394385 | LR: 0.0100 | Batch:  32 | Patience:  62/500 | Rate: 98.7 iter/s | Time: 63.80s\n",
      "Iter  6320 | Train Loss: 0.394303 | LR: 0.0100 | Batch:  32 | Patience:  66/500 | Rate: 98.8 iter/s | Time: 64.00s\n",
      "Iter  6340 | Train Loss: 0.394296 | LR: 0.0100 | Batch:  32 | Patience:  70/500 | Rate: 98.6 iter/s | Time: 64.29s\n",
      "Iter  6360 | Train Loss: 0.394287 | LR: 0.0100 | Batch:  32 | Patience:  74/500 | Rate: 98.6 iter/s | Time: 64.49s\n",
      "Iter  6380 | Train Loss: 0.394302 | LR: 0.0100 | Batch:  32 | Patience:  78/500 | Rate: 98.6 iter/s | Time: 64.68s\n",
      "Iter  6400 | Train Loss: 0.394278 | LR: 0.0100 | Batch:  32 | Patience:  82/500 | Rate: 98.7 iter/s | Time: 64.87s\n",
      "Iter  6420 | Train Loss: 0.394306 | LR: 0.0100 | Batch:  32 | Patience:  86/500 | Rate: 98.7 iter/s | Time: 65.07s\n",
      "Iter  6440 | Train Loss: 0.394280 | LR: 0.0100 | Batch:  32 | Patience:  90/500 | Rate: 98.7 iter/s | Time: 65.27s\n",
      "Iter  6460 | Train Loss: 0.394243 | LR: 0.0100 | Batch:  32 | Patience:  94/500 | Rate: 98.7 iter/s | Time: 65.46s\n",
      "Iter  6480 | Train Loss: 0.394264 | LR: 0.0100 | Batch:  32 | Patience:  98/500 | Rate: 98.7 iter/s | Time: 65.65s\n",
      "Iter  6500 | Train Loss: 0.394250 | LR: 0.0100 | Batch:  32 | Patience: 102/500 | Rate: 98.7 iter/s | Time: 65.84s\n",
      "Iter  6520 | Train Loss: 0.394270 | LR: 0.0100 | Batch:  32 | Patience: 106/500 | Rate: 98.7 iter/s | Time: 66.03s\n",
      "Iter  6540 | Train Loss: 0.394270 | LR: 0.0100 | Batch:  32 | Patience: 110/500 | Rate: 98.7 iter/s | Time: 66.24s\n",
      "Iter  6560 | Train Loss: 0.394305 | LR: 0.0100 | Batch:  32 | Patience: 114/500 | Rate: 98.7 iter/s | Time: 66.44s\n",
      "Iter  6580 | Train Loss: 0.394270 | LR: 0.0100 | Batch:  32 | Patience: 118/500 | Rate: 98.8 iter/s | Time: 66.63s\n",
      "Iter  6600 | Train Loss: 0.394247 | LR: 0.0100 | Batch:  32 | Patience: 122/500 | Rate: 98.8 iter/s | Time: 66.82s\n",
      "Iter  6620 | Train Loss: 0.394235 | LR: 0.0100 | Batch:  32 | Patience: 126/500 | Rate: 98.8 iter/s | Time: 67.03s\n",
      "Iter  6640 | Train Loss: 0.394272 | LR: 0.0100 | Batch:  32 | Patience: 130/500 | Rate: 98.8 iter/s | Time: 67.23s\n",
      "Iter  6660 | Train Loss: 0.394294 | LR: 0.0100 | Batch:  32 | Patience: 134/500 | Rate: 98.8 iter/s | Time: 67.43s\n",
      "Iter  6680 | Train Loss: 0.394289 | LR: 0.0100 | Batch:  32 | Patience: 138/500 | Rate: 98.8 iter/s | Time: 67.63s\n",
      "Iter  6700 | Train Loss: 0.394309 | LR: 0.0100 | Batch:  32 | Patience: 142/500 | Rate: 98.8 iter/s | Time: 67.82s\n",
      "Iter  6720 | Train Loss: 0.394360 | LR: 0.0100 | Batch:  32 | Patience: 146/500 | Rate: 98.8 iter/s | Time: 68.02s\n",
      "Iter  6740 | Train Loss: 0.394417 | LR: 0.0100 | Batch:  32 | Patience: 150/500 | Rate: 98.8 iter/s | Time: 68.21s\n",
      "Iter  6760 | Train Loss: 0.394348 | LR: 0.0100 | Batch:  32 | Patience: 154/500 | Rate: 98.8 iter/s | Time: 68.41s\n",
      "Iter  6780 | Train Loss: 0.394342 | LR: 0.0100 | Batch:  32 | Patience: 158/500 | Rate: 98.8 iter/s | Time: 68.60s\n",
      "Iter  6800 | Train Loss: 0.394326 | LR: 0.0100 | Batch:  32 | Patience: 162/500 | Rate: 98.8 iter/s | Time: 68.80s\n",
      "Iter  6820 | Train Loss: 0.394306 | LR: 0.0100 | Batch:  32 | Patience: 166/500 | Rate: 98.9 iter/s | Time: 68.99s\n",
      "Iter  6840 | Train Loss: 0.394300 | LR: 0.0100 | Batch:  32 | Patience: 170/500 | Rate: 98.9 iter/s | Time: 69.18s\n",
      "Iter  6860 | Train Loss: 0.394225 | LR: 0.0100 | Batch:  32 | Patience: 174/500 | Rate: 98.9 iter/s | Time: 69.37s\n",
      "Iter  6880 | Train Loss: 0.394230 | LR: 0.0100 | Batch:  32 | Patience: 178/500 | Rate: 98.9 iter/s | Time: 69.57s\n",
      "Iter  6900 | Train Loss: 0.394232 | LR: 0.0100 | Batch:  32 | Patience: 182/500 | Rate: 98.9 iter/s | Time: 69.76s\n",
      "Iter  6920 | Train Loss: 0.394236 | LR: 0.0100 | Batch:  32 | Patience: 186/500 | Rate: 98.9 iter/s | Time: 69.95s\n",
      "Iter  6940 | Train Loss: 0.394236 | LR: 0.0100 | Batch:  32 | Patience: 190/500 | Rate: 98.9 iter/s | Time: 70.15s\n",
      "Iter  6960 | Train Loss: 0.394260 | LR: 0.0100 | Batch:  32 | Patience: 194/500 | Rate: 99.0 iter/s | Time: 70.34s\n",
      "Iter  6980 | Train Loss: 0.394267 | LR: 0.0100 | Batch:  32 | Patience: 198/500 | Rate: 99.0 iter/s | Time: 70.53s\n",
      "Iter  7000 | Train Loss: 0.394242 | LR: 0.0100 | Batch:  32 | Patience: 202/500 | Rate: 99.0 iter/s | Time: 70.74s\n",
      "Iter  7020 | Train Loss: 0.394182 | LR: 0.0100 | Batch:  32 | Patience: 206/500 | Rate: 99.0 iter/s | Time: 70.93s\n",
      "Iter  7040 | Train Loss: 0.394226 | LR: 0.0100 | Batch:  32 | Patience: 210/500 | Rate: 99.0 iter/s | Time: 71.13s\n",
      "Iter  7060 | Train Loss: 0.394183 | LR: 0.0100 | Batch:  32 | Patience: 214/500 | Rate: 99.0 iter/s | Time: 71.33s\n",
      "Iter  7080 | Train Loss: 0.394166 | LR: 0.0100 | Batch:  32 | Patience: 218/500 | Rate: 99.0 iter/s | Time: 71.53s\n",
      "Iter  7100 | Train Loss: 0.394192 | LR: 0.0100 | Batch:  32 | Patience: 222/500 | Rate: 99.0 iter/s | Time: 71.72s\n",
      "Iter  7120 | Train Loss: 0.394183 | LR: 0.0100 | Batch:  32 | Patience: 226/500 | Rate: 99.0 iter/s | Time: 71.91s\n",
      "Iter  7140 | Train Loss: 0.394168 | LR: 0.0100 | Batch:  32 | Patience: 230/500 | Rate: 99.0 iter/s | Time: 72.13s\n",
      "Iter  7160 | Train Loss: 0.394135 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.0 iter/s | Time: 72.33s\n",
      "Iter  7180 | Train Loss: 0.394132 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 99.0 iter/s | Time: 72.52s\n",
      "Iter  7200 | Train Loss: 0.394161 | LR: 0.0100 | Batch:  32 | Patience:   3/500 | Rate: 99.0 iter/s | Time: 72.72s\n",
      "Iter  7220 | Train Loss: 0.394159 | LR: 0.0100 | Batch:  32 | Patience:   7/500 | Rate: 99.0 iter/s | Time: 72.91s\n",
      "Iter  7240 | Train Loss: 0.394152 | LR: 0.0100 | Batch:  32 | Patience:  11/500 | Rate: 99.0 iter/s | Time: 73.10s\n",
      "Iter  7260 | Train Loss: 0.394132 | LR: 0.0100 | Batch:  32 | Patience:   2/500 | Rate: 99.0 iter/s | Time: 73.30s\n",
      "Iter  7280 | Train Loss: 0.394130 | LR: 0.0100 | Batch:  32 | Patience:   1/500 | Rate: 99.0 iter/s | Time: 73.50s\n",
      "Iter  7300 | Train Loss: 0.394116 | LR: 0.0100 | Batch:  32 | Patience:   3/500 | Rate: 99.1 iter/s | Time: 73.69s\n",
      "Iter  7320 | Train Loss: 0.394102 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.1 iter/s | Time: 73.88s\n",
      "Iter  7340 | Train Loss: 0.394105 | LR: 0.0100 | Batch:  32 | Patience:   4/500 | Rate: 99.1 iter/s | Time: 74.08s\n",
      "Iter  7360 | Train Loss: 0.394108 | LR: 0.0100 | Batch:  32 | Patience:   8/500 | Rate: 99.1 iter/s | Time: 74.28s\n",
      "Iter  7380 | Train Loss: 0.394111 | LR: 0.0100 | Batch:  32 | Patience:  12/500 | Rate: 99.1 iter/s | Time: 74.47s\n",
      "Iter  7400 | Train Loss: 0.394105 | LR: 0.0100 | Batch:  32 | Patience:  16/500 | Rate: 99.1 iter/s | Time: 74.67s\n",
      "Iter  7420 | Train Loss: 0.394104 | LR: 0.0100 | Batch:  32 | Patience:  20/500 | Rate: 99.1 iter/s | Time: 74.86s\n",
      "Iter  7440 | Train Loss: 0.394104 | LR: 0.0100 | Batch:  32 | Patience:  24/500 | Rate: 99.1 iter/s | Time: 75.06s\n",
      "Iter  7460 | Train Loss: 0.394143 | LR: 0.0100 | Batch:  32 | Patience:  28/500 | Rate: 99.1 iter/s | Time: 75.26s\n",
      "Iter  7480 | Train Loss: 0.394150 | LR: 0.0100 | Batch:  32 | Patience:  32/500 | Rate: 99.1 iter/s | Time: 75.45s\n",
      "Iter  7500 | Train Loss: 0.394129 | LR: 0.0100 | Batch:  32 | Patience:  36/500 | Rate: 99.1 iter/s | Time: 75.65s\n",
      "Iter  7520 | Train Loss: 0.394112 | LR: 0.0100 | Batch:  32 | Patience:  40/500 | Rate: 99.2 iter/s | Time: 75.84s\n",
      "Iter  7540 | Train Loss: 0.394155 | LR: 0.0100 | Batch:  32 | Patience:  44/500 | Rate: 99.2 iter/s | Time: 76.04s\n",
      "Iter  7560 | Train Loss: 0.394164 | LR: 0.0100 | Batch:  32 | Patience:  48/500 | Rate: 99.2 iter/s | Time: 76.24s\n",
      "Iter  7580 | Train Loss: 0.394122 | LR: 0.0100 | Batch:  32 | Patience:  52/500 | Rate: 99.2 iter/s | Time: 76.44s\n",
      "Iter  7600 | Train Loss: 0.394123 | LR: 0.0100 | Batch:  32 | Patience:  56/500 | Rate: 99.2 iter/s | Time: 76.63s\n",
      "Iter  7620 | Train Loss: 0.394125 | LR: 0.0100 | Batch:  32 | Patience:  60/500 | Rate: 99.2 iter/s | Time: 76.82s\n",
      "Iter  7640 | Train Loss: 0.394108 | LR: 0.0100 | Batch:  32 | Patience:  64/500 | Rate: 99.2 iter/s | Time: 77.02s\n",
      "Iter  7660 | Train Loss: 0.394108 | LR: 0.0100 | Batch:  32 | Patience:  68/500 | Rate: 99.2 iter/s | Time: 77.23s\n",
      "Iter  7680 | Train Loss: 0.394119 | LR: 0.0100 | Batch:  32 | Patience:  72/500 | Rate: 99.2 iter/s | Time: 77.43s\n",
      "Iter  7700 | Train Loss: 0.394118 | LR: 0.0100 | Batch:  32 | Patience:  76/500 | Rate: 99.2 iter/s | Time: 77.63s\n",
      "Iter  7720 | Train Loss: 0.394120 | LR: 0.0100 | Batch:  32 | Patience:  80/500 | Rate: 99.2 iter/s | Time: 77.83s\n",
      "Iter  7740 | Train Loss: 0.394136 | LR: 0.0100 | Batch:  32 | Patience:  84/500 | Rate: 99.2 iter/s | Time: 78.03s\n",
      "Iter  7760 | Train Loss: 0.394146 | LR: 0.0100 | Batch:  32 | Patience:  88/500 | Rate: 99.2 iter/s | Time: 78.23s\n",
      "Iter  7780 | Train Loss: 0.394130 | LR: 0.0100 | Batch:  32 | Patience:  92/500 | Rate: 99.2 iter/s | Time: 78.43s\n",
      "Iter  7800 | Train Loss: 0.394141 | LR: 0.0100 | Batch:  32 | Patience:  96/500 | Rate: 99.2 iter/s | Time: 78.64s\n",
      "Iter  7820 | Train Loss: 0.394177 | LR: 0.0100 | Batch:  32 | Patience: 100/500 | Rate: 99.2 iter/s | Time: 78.84s\n",
      "Iter  7840 | Train Loss: 0.394205 | LR: 0.0100 | Batch:  32 | Patience: 104/500 | Rate: 99.2 iter/s | Time: 79.04s\n",
      "Iter  7860 | Train Loss: 0.394171 | LR: 0.0100 | Batch:  32 | Patience: 108/500 | Rate: 99.2 iter/s | Time: 79.24s\n",
      "Iter  7880 | Train Loss: 0.394181 | LR: 0.0100 | Batch:  32 | Patience: 112/500 | Rate: 99.2 iter/s | Time: 79.44s\n",
      "Iter  7900 | Train Loss: 0.394201 | LR: 0.0100 | Batch:  32 | Patience: 116/500 | Rate: 99.2 iter/s | Time: 79.64s\n",
      "Iter  7920 | Train Loss: 0.394231 | LR: 0.0100 | Batch:  32 | Patience: 120/500 | Rate: 99.2 iter/s | Time: 79.84s\n",
      "Iter  7940 | Train Loss: 0.394211 | LR: 0.0100 | Batch:  32 | Patience: 124/500 | Rate: 99.2 iter/s | Time: 80.04s\n",
      "Iter  7960 | Train Loss: 0.394227 | LR: 0.0100 | Batch:  32 | Patience: 128/500 | Rate: 99.2 iter/s | Time: 80.25s\n",
      "Iter  7980 | Train Loss: 0.394198 | LR: 0.0100 | Batch:  32 | Patience: 132/500 | Rate: 99.2 iter/s | Time: 80.45s\n",
      "Iter  8000 | Train Loss: 0.394150 | LR: 0.0100 | Batch:  32 | Patience: 136/500 | Rate: 99.2 iter/s | Time: 80.65s\n",
      "Iter  8020 | Train Loss: 0.394111 | LR: 0.0100 | Batch:  32 | Patience: 140/500 | Rate: 99.2 iter/s | Time: 80.85s\n",
      "Iter  8040 | Train Loss: 0.394106 | LR: 0.0100 | Batch:  32 | Patience: 144/500 | Rate: 99.2 iter/s | Time: 81.05s\n",
      "Iter  8060 | Train Loss: 0.394092 | LR: 0.0100 | Batch:  32 | Patience: 148/500 | Rate: 99.2 iter/s | Time: 81.25s\n",
      "Iter  8080 | Train Loss: 0.394106 | LR: 0.0100 | Batch:  32 | Patience: 152/500 | Rate: 99.2 iter/s | Time: 81.46s\n",
      "Iter  8100 | Train Loss: 0.394103 | LR: 0.0100 | Batch:  32 | Patience: 156/500 | Rate: 99.2 iter/s | Time: 81.66s\n",
      "Iter  8120 | Train Loss: 0.394104 | LR: 0.0100 | Batch:  32 | Patience: 160/500 | Rate: 99.2 iter/s | Time: 81.85s\n",
      "Iter  8140 | Train Loss: 0.394105 | LR: 0.0100 | Batch:  32 | Patience: 164/500 | Rate: 99.2 iter/s | Time: 82.05s\n",
      "Iter  8160 | Train Loss: 0.394104 | LR: 0.0100 | Batch:  32 | Patience: 168/500 | Rate: 99.2 iter/s | Time: 82.25s\n",
      "Iter  8180 | Train Loss: 0.394108 | LR: 0.0100 | Batch:  32 | Patience: 172/500 | Rate: 99.2 iter/s | Time: 82.45s\n",
      "Iter  8200 | Train Loss: 0.394120 | LR: 0.0100 | Batch:  32 | Patience: 176/500 | Rate: 99.2 iter/s | Time: 82.65s\n",
      "Iter  8220 | Train Loss: 0.394110 | LR: 0.0100 | Batch:  32 | Patience: 180/500 | Rate: 99.2 iter/s | Time: 82.85s\n",
      "Iter  8240 | Train Loss: 0.394119 | LR: 0.0100 | Batch:  32 | Patience: 184/500 | Rate: 99.2 iter/s | Time: 83.05s\n",
      "Iter  8260 | Train Loss: 0.394115 | LR: 0.0100 | Batch:  32 | Patience: 188/500 | Rate: 99.2 iter/s | Time: 83.25s\n",
      "Iter  8280 | Train Loss: 0.394155 | LR: 0.0100 | Batch:  32 | Patience: 192/500 | Rate: 99.2 iter/s | Time: 83.45s\n",
      "Iter  8300 | Train Loss: 0.394152 | LR: 0.0100 | Batch:  32 | Patience: 196/500 | Rate: 99.2 iter/s | Time: 83.65s\n",
      "Iter  8320 | Train Loss: 0.394172 | LR: 0.0100 | Batch:  32 | Patience: 200/500 | Rate: 99.2 iter/s | Time: 83.85s\n",
      "Iter  8340 | Train Loss: 0.394162 | LR: 0.0100 | Batch:  32 | Patience: 204/500 | Rate: 99.2 iter/s | Time: 84.05s\n",
      "Iter  8360 | Train Loss: 0.394180 | LR: 0.0100 | Batch:  32 | Patience: 208/500 | Rate: 99.2 iter/s | Time: 84.25s\n",
      "Iter  8380 | Train Loss: 0.394242 | LR: 0.0100 | Batch:  32 | Patience: 212/500 | Rate: 99.2 iter/s | Time: 84.45s\n",
      "Iter  8400 | Train Loss: 0.394223 | LR: 0.0100 | Batch:  32 | Patience: 216/500 | Rate: 99.2 iter/s | Time: 84.65s\n",
      "Iter  8420 | Train Loss: 0.394223 | LR: 0.0100 | Batch:  32 | Patience: 220/500 | Rate: 99.2 iter/s | Time: 84.85s\n",
      "Iter  8440 | Train Loss: 0.394223 | LR: 0.0100 | Batch:  32 | Patience: 224/500 | Rate: 99.2 iter/s | Time: 85.05s\n",
      "Iter  8460 | Train Loss: 0.394209 | LR: 0.0100 | Batch:  32 | Patience: 228/500 | Rate: 99.2 iter/s | Time: 85.25s\n",
      "Iter  8480 | Train Loss: 0.394169 | LR: 0.0100 | Batch:  32 | Patience: 232/500 | Rate: 99.2 iter/s | Time: 85.46s\n",
      "Iter  8500 | Train Loss: 0.394141 | LR: 0.0100 | Batch:  32 | Patience: 236/500 | Rate: 99.2 iter/s | Time: 85.66s\n",
      "Iter  8520 | Train Loss: 0.394111 | LR: 0.0100 | Batch:  32 | Patience: 240/500 | Rate: 99.2 iter/s | Time: 85.86s\n",
      "Iter  8540 | Train Loss: 0.394142 | LR: 0.0100 | Batch:  32 | Patience: 244/500 | Rate: 99.2 iter/s | Time: 86.06s\n",
      "Iter  8560 | Train Loss: 0.394170 | LR: 0.0100 | Batch:  32 | Patience: 248/500 | Rate: 99.2 iter/s | Time: 86.26s\n",
      "Iter  8580 | Train Loss: 0.394219 | LR: 0.0100 | Batch:  32 | Patience: 252/500 | Rate: 99.2 iter/s | Time: 86.46s\n",
      "Iter  8600 | Train Loss: 0.394205 | LR: 0.0100 | Batch:  32 | Patience: 256/500 | Rate: 99.2 iter/s | Time: 86.66s\n",
      "Iter  8620 | Train Loss: 0.394198 | LR: 0.0100 | Batch:  32 | Patience: 260/500 | Rate: 99.2 iter/s | Time: 86.86s\n",
      "Iter  8640 | Train Loss: 0.394254 | LR: 0.0100 | Batch:  32 | Patience: 264/500 | Rate: 99.2 iter/s | Time: 87.06s\n",
      "Iter  8660 | Train Loss: 0.394222 | LR: 0.0100 | Batch:  32 | Patience: 268/500 | Rate: 99.2 iter/s | Time: 87.26s\n",
      "Iter  8680 | Train Loss: 0.394205 | LR: 0.0100 | Batch:  32 | Patience: 272/500 | Rate: 99.2 iter/s | Time: 87.46s\n",
      "Iter  8700 | Train Loss: 0.394218 | LR: 0.0100 | Batch:  32 | Patience: 276/500 | Rate: 99.3 iter/s | Time: 87.66s\n",
      "Iter  8720 | Train Loss: 0.394236 | LR: 0.0100 | Batch:  32 | Patience: 280/500 | Rate: 99.2 iter/s | Time: 87.86s\n",
      "Iter  8740 | Train Loss: 0.394245 | LR: 0.0100 | Batch:  32 | Patience: 284/500 | Rate: 99.2 iter/s | Time: 88.06s\n",
      "Iter  8760 | Train Loss: 0.394287 | LR: 0.0100 | Batch:  32 | Patience: 288/500 | Rate: 99.3 iter/s | Time: 88.26s\n",
      "Iter  8780 | Train Loss: 0.394199 | LR: 0.0100 | Batch:  32 | Patience: 292/500 | Rate: 99.3 iter/s | Time: 88.46s\n",
      "Iter  8800 | Train Loss: 0.394180 | LR: 0.0100 | Batch:  32 | Patience: 296/500 | Rate: 99.3 iter/s | Time: 88.66s\n",
      "Iter  8820 | Train Loss: 0.394192 | LR: 0.0100 | Batch:  32 | Patience: 300/500 | Rate: 99.3 iter/s | Time: 88.86s\n",
      "Iter  8840 | Train Loss: 0.394127 | LR: 0.0100 | Batch:  32 | Patience: 304/500 | Rate: 99.2 iter/s | Time: 89.07s\n",
      "Iter  8860 | Train Loss: 0.394121 | LR: 0.0100 | Batch:  32 | Patience: 308/500 | Rate: 99.2 iter/s | Time: 89.27s\n",
      "Iter  8880 | Train Loss: 0.394101 | LR: 0.0100 | Batch:  32 | Patience: 312/500 | Rate: 99.2 iter/s | Time: 89.47s\n",
      "Iter  8900 | Train Loss: 0.394161 | LR: 0.0100 | Batch:  32 | Patience: 316/500 | Rate: 99.2 iter/s | Time: 89.67s\n",
      "Iter  8920 | Train Loss: 0.394140 | LR: 0.0100 | Batch:  32 | Patience: 320/500 | Rate: 99.2 iter/s | Time: 89.88s\n",
      "Iter  8940 | Train Loss: 0.394148 | LR: 0.0100 | Batch:  32 | Patience: 324/500 | Rate: 99.2 iter/s | Time: 90.08s\n",
      "Iter  8960 | Train Loss: 0.394137 | LR: 0.0100 | Batch:  32 | Patience: 328/500 | Rate: 99.2 iter/s | Time: 90.29s\n",
      "Iter  8980 | Train Loss: 0.394148 | LR: 0.0100 | Batch:  32 | Patience: 332/500 | Rate: 99.2 iter/s | Time: 90.49s\n",
      "Iter  9000 | Train Loss: 0.394127 | LR: 0.0100 | Batch:  32 | Patience: 336/500 | Rate: 99.2 iter/s | Time: 90.69s\n",
      "Iter  9020 | Train Loss: 0.394143 | LR: 0.0100 | Batch:  32 | Patience: 340/500 | Rate: 99.2 iter/s | Time: 90.89s\n",
      "Iter  9040 | Train Loss: 0.394151 | LR: 0.0100 | Batch:  32 | Patience: 344/500 | Rate: 99.2 iter/s | Time: 91.09s\n",
      "Iter  9060 | Train Loss: 0.394138 | LR: 0.0100 | Batch:  32 | Patience: 348/500 | Rate: 99.2 iter/s | Time: 91.29s\n",
      "Iter  9080 | Train Loss: 0.394135 | LR: 0.0100 | Batch:  32 | Patience: 352/500 | Rate: 99.2 iter/s | Time: 91.49s\n",
      "Iter  9100 | Train Loss: 0.394130 | LR: 0.0100 | Batch:  32 | Patience: 356/500 | Rate: 99.3 iter/s | Time: 91.69s\n",
      "Iter  9120 | Train Loss: 0.394112 | LR: 0.0100 | Batch:  32 | Patience: 360/500 | Rate: 99.3 iter/s | Time: 91.89s\n",
      "Iter  9140 | Train Loss: 0.394105 | LR: 0.0100 | Batch:  32 | Patience: 364/500 | Rate: 99.3 iter/s | Time: 92.08s\n",
      "Iter  9160 | Train Loss: 0.394123 | LR: 0.0100 | Batch:  32 | Patience: 368/500 | Rate: 99.3 iter/s | Time: 92.29s\n",
      "Iter  9180 | Train Loss: 0.394125 | LR: 0.0100 | Batch:  32 | Patience: 372/500 | Rate: 99.3 iter/s | Time: 92.49s\n",
      "Iter  9200 | Train Loss: 0.394140 | LR: 0.0100 | Batch:  32 | Patience: 376/500 | Rate: 99.3 iter/s | Time: 92.69s\n",
      "Iter  9220 | Train Loss: 0.394158 | LR: 0.0100 | Batch:  32 | Patience: 380/500 | Rate: 99.3 iter/s | Time: 92.89s\n",
      "Iter  9240 | Train Loss: 0.394150 | LR: 0.0100 | Batch:  32 | Patience: 384/500 | Rate: 99.3 iter/s | Time: 93.09s\n",
      "Iter  9260 | Train Loss: 0.394205 | LR: 0.0100 | Batch:  32 | Patience: 388/500 | Rate: 99.3 iter/s | Time: 93.29s\n",
      "Iter  9280 | Train Loss: 0.394327 | LR: 0.0100 | Batch:  32 | Patience: 392/500 | Rate: 99.3 iter/s | Time: 93.49s\n",
      "Iter  9300 | Train Loss: 0.394238 | LR: 0.0100 | Batch:  32 | Patience: 396/500 | Rate: 99.3 iter/s | Time: 93.69s\n",
      "Iter  9320 | Train Loss: 0.394199 | LR: 0.0100 | Batch:  32 | Patience: 400/500 | Rate: 99.3 iter/s | Time: 93.89s\n",
      "Iter  9340 | Train Loss: 0.394162 | LR: 0.0100 | Batch:  32 | Patience: 404/500 | Rate: 99.3 iter/s | Time: 94.08s\n",
      "Iter  9360 | Train Loss: 0.394163 | LR: 0.0100 | Batch:  32 | Patience: 408/500 | Rate: 99.3 iter/s | Time: 94.29s\n",
      "Iter  9380 | Train Loss: 0.394156 | LR: 0.0100 | Batch:  32 | Patience: 412/500 | Rate: 99.3 iter/s | Time: 94.48s\n",
      "Iter  9400 | Train Loss: 0.394162 | LR: 0.0100 | Batch:  32 | Patience: 416/500 | Rate: 99.3 iter/s | Time: 94.67s\n",
      "Iter  9420 | Train Loss: 0.394154 | LR: 0.0100 | Batch:  32 | Patience: 420/500 | Rate: 99.3 iter/s | Time: 94.86s\n",
      "Iter  9440 | Train Loss: 0.394145 | LR: 0.0100 | Batch:  32 | Patience: 424/500 | Rate: 99.3 iter/s | Time: 95.06s\n",
      "Iter  9460 | Train Loss: 0.394148 | LR: 0.0100 | Batch:  32 | Patience: 428/500 | Rate: 99.3 iter/s | Time: 95.25s\n",
      "Iter  9480 | Train Loss: 0.394149 | LR: 0.0100 | Batch:  32 | Patience: 432/500 | Rate: 99.3 iter/s | Time: 95.45s\n",
      "Iter  9500 | Train Loss: 0.394139 | LR: 0.0100 | Batch:  32 | Patience: 436/500 | Rate: 99.3 iter/s | Time: 95.64s\n",
      "Iter  9520 | Train Loss: 0.394132 | LR: 0.0100 | Batch:  32 | Patience: 440/500 | Rate: 99.3 iter/s | Time: 95.84s\n",
      "Iter  9540 | Train Loss: 0.394123 | LR: 0.0100 | Batch:  32 | Patience: 444/500 | Rate: 99.3 iter/s | Time: 96.04s\n",
      "Iter  9560 | Train Loss: 0.394132 | LR: 0.0100 | Batch:  32 | Patience: 448/500 | Rate: 99.3 iter/s | Time: 96.23s\n",
      "Iter  9580 | Train Loss: 0.394131 | LR: 0.0100 | Batch:  32 | Patience: 452/500 | Rate: 99.4 iter/s | Time: 96.42s\n",
      "Iter  9600 | Train Loss: 0.394124 | LR: 0.0100 | Batch:  32 | Patience: 456/500 | Rate: 99.4 iter/s | Time: 96.62s\n",
      "Iter  9620 | Train Loss: 0.394099 | LR: 0.0100 | Batch:  32 | Patience: 460/500 | Rate: 99.4 iter/s | Time: 96.82s\n",
      "Iter  9640 | Train Loss: 0.394068 | LR: 0.0100 | Batch:  32 | Patience:   0/500 | Rate: 99.4 iter/s | Time: 97.01s\n",
      "Iter  9660 | Train Loss: 0.394073 | LR: 0.0100 | Batch:  32 | Patience:   3/500 | Rate: 99.4 iter/s | Time: 97.20s\n",
      "Iter  9680 | Train Loss: 0.394080 | LR: 0.0100 | Batch:  32 | Patience:   7/500 | Rate: 99.4 iter/s | Time: 97.40s\n",
      "Iter  9700 | Train Loss: 0.394070 | LR: 0.0100 | Batch:  32 | Patience:  11/500 | Rate: 99.4 iter/s | Time: 97.59s\n",
      "Iter  9720 | Train Loss: 0.394111 | LR: 0.0100 | Batch:  32 | Patience:  15/500 | Rate: 99.4 iter/s | Time: 97.79s\n",
      "Iter  9740 | Train Loss: 0.394134 | LR: 0.0100 | Batch:  32 | Patience:  19/500 | Rate: 99.4 iter/s | Time: 97.99s\n",
      "Iter  9760 | Train Loss: 0.394132 | LR: 0.0100 | Batch:  32 | Patience:  23/500 | Rate: 99.4 iter/s | Time: 98.18s\n",
      "Iter  9780 | Train Loss: 0.394116 | LR: 0.0100 | Batch:  32 | Patience:  27/500 | Rate: 99.4 iter/s | Time: 98.38s\n",
      "Iter  9800 | Train Loss: 0.394115 | LR: 0.0100 | Batch:  32 | Patience:  31/500 | Rate: 99.4 iter/s | Time: 98.57s\n",
      "Iter  9820 | Train Loss: 0.394109 | LR: 0.0100 | Batch:  32 | Patience:  35/500 | Rate: 99.4 iter/s | Time: 98.77s\n",
      "Iter  9840 | Train Loss: 0.394091 | LR: 0.0100 | Batch:  32 | Patience:  39/500 | Rate: 99.4 iter/s | Time: 98.97s\n",
      "Iter  9860 | Train Loss: 0.394106 | LR: 0.0100 | Batch:  32 | Patience:  43/500 | Rate: 99.4 iter/s | Time: 99.17s\n",
      "Iter  9880 | Train Loss: 0.394100 | LR: 0.0100 | Batch:  32 | Patience:  47/500 | Rate: 99.4 iter/s | Time: 99.37s\n",
      "Iter  9900 | Train Loss: 0.394140 | LR: 0.0100 | Batch:  32 | Patience:  51/500 | Rate: 99.4 iter/s | Time: 99.57s\n",
      "Iter  9920 | Train Loss: 0.394149 | LR: 0.0100 | Batch:  32 | Patience:  55/500 | Rate: 99.4 iter/s | Time: 99.77s\n",
      "Iter  9940 | Train Loss: 0.394111 | LR: 0.0100 | Batch:  32 | Patience:  59/500 | Rate: 99.4 iter/s | Time: 99.99s\n",
      "Iter  9960 | Train Loss: 0.394133 | LR: 0.0100 | Batch:  32 | Patience:  63/500 | Rate: 99.4 iter/s | Time: 100.18s\n",
      "Iter  9980 | Train Loss: 0.394134 | LR: 0.0100 | Batch:  32 | Patience:  67/500 | Rate: 99.4 iter/s | Time: 100.38s\n",
      "Iter 10000 | Train Loss: 0.394133 | LR: 0.0100 | Batch:  32 | Patience:  71/500 | Rate: 99.4 iter/s | Time: 100.58s\n",
      "Iter 10020 | Train Loss: 0.394125 | LR: 0.0100 | Batch:  32 | Patience:  75/500 | Rate: 99.4 iter/s | Time: 100.77s\n",
      "Iter 10040 | Train Loss: 0.394131 | LR: 0.0100 | Batch:  32 | Patience:  79/500 | Rate: 99.4 iter/s | Time: 100.96s\n",
      "Iter 10060 | Train Loss: 0.394116 | LR: 0.0100 | Batch:  32 | Patience:  83/500 | Rate: 99.4 iter/s | Time: 101.18s\n",
      "Iter 10080 | Train Loss: 0.394118 | LR: 0.0100 | Batch:  32 | Patience:  87/500 | Rate: 99.3 iter/s | Time: 101.46s\n",
      "Iter 10100 | Train Loss: 0.394115 | LR: 0.0100 | Batch:  32 | Patience:  91/500 | Rate: 99.4 iter/s | Time: 101.65s\n",
      "Iter 10120 | Train Loss: 0.394119 | LR: 0.0100 | Batch:  32 | Patience:  95/500 | Rate: 99.4 iter/s | Time: 101.84s\n",
      "Iter 10140 | Train Loss: 0.394113 | LR: 0.0100 | Batch:  32 | Patience:  99/500 | Rate: 99.4 iter/s | Time: 102.03s\n",
      "Iter 10160 | Train Loss: 0.394148 | LR: 0.0100 | Batch:  32 | Patience: 103/500 | Rate: 99.4 iter/s | Time: 102.24s\n",
      "Iter 10180 | Train Loss: 0.394165 | LR: 0.0100 | Batch:  32 | Patience: 107/500 | Rate: 99.4 iter/s | Time: 102.45s\n",
      "Iter 10200 | Train Loss: 0.394144 | LR: 0.0100 | Batch:  32 | Patience: 111/500 | Rate: 99.4 iter/s | Time: 102.64s\n",
      "Iter 10220 | Train Loss: 0.394174 | LR: 0.0100 | Batch:  32 | Patience: 115/500 | Rate: 99.4 iter/s | Time: 102.83s\n",
      "Iter 10240 | Train Loss: 0.394159 | LR: 0.0100 | Batch:  32 | Patience: 119/500 | Rate: 99.4 iter/s | Time: 103.02s\n",
      "Iter 10260 | Train Loss: 0.394153 | LR: 0.0100 | Batch:  32 | Patience: 123/500 | Rate: 99.4 iter/s | Time: 103.21s\n",
      "Iter 10280 | Train Loss: 0.394133 | LR: 0.0100 | Batch:  32 | Patience: 127/500 | Rate: 99.4 iter/s | Time: 103.41s\n",
      "Iter 10300 | Train Loss: 0.394118 | LR: 0.0100 | Batch:  32 | Patience: 131/500 | Rate: 99.4 iter/s | Time: 103.61s\n",
      "Iter 10320 | Train Loss: 0.394097 | LR: 0.0100 | Batch:  32 | Patience: 135/500 | Rate: 99.4 iter/s | Time: 103.81s\n",
      "Iter 10340 | Train Loss: 0.394098 | LR: 0.0100 | Batch:  32 | Patience: 139/500 | Rate: 99.4 iter/s | Time: 104.01s\n",
      "Iter 10360 | Train Loss: 0.394108 | LR: 0.0100 | Batch:  32 | Patience: 143/500 | Rate: 99.4 iter/s | Time: 104.21s\n",
      "Iter 10380 | Train Loss: 0.394100 | LR: 0.0100 | Batch:  32 | Patience: 147/500 | Rate: 99.4 iter/s | Time: 104.41s\n",
      "Iter 10400 | Train Loss: 0.394089 | LR: 0.0100 | Batch:  32 | Patience: 151/500 | Rate: 99.4 iter/s | Time: 104.61s\n",
      "Iter 10420 | Train Loss: 0.394092 | LR: 0.0100 | Batch:  32 | Patience: 155/500 | Rate: 99.4 iter/s | Time: 104.81s\n",
      "Iter 10440 | Train Loss: 0.394135 | LR: 0.0100 | Batch:  32 | Patience: 159/500 | Rate: 99.4 iter/s | Time: 105.01s\n",
      "Iter 10460 | Train Loss: 0.394142 | LR: 0.0100 | Batch:  32 | Patience: 163/500 | Rate: 99.4 iter/s | Time: 105.20s\n",
      "Iter 10480 | Train Loss: 0.394168 | LR: 0.0100 | Batch:  32 | Patience: 167/500 | Rate: 99.4 iter/s | Time: 105.40s\n",
      "Iter 10500 | Train Loss: 0.394203 | LR: 0.0100 | Batch:  32 | Patience: 171/500 | Rate: 99.4 iter/s | Time: 105.67s\n",
      "Iter 10520 | Train Loss: 0.394186 | LR: 0.0100 | Batch:  32 | Patience: 175/500 | Rate: 99.4 iter/s | Time: 105.87s\n",
      "Iter 10540 | Train Loss: 0.394176 | LR: 0.0100 | Batch:  32 | Patience: 179/500 | Rate: 99.4 iter/s | Time: 106.07s\n",
      "Iter 10560 | Train Loss: 0.394193 | LR: 0.0100 | Batch:  32 | Patience: 183/500 | Rate: 99.4 iter/s | Time: 106.27s\n",
      "Iter 10580 | Train Loss: 0.394197 | LR: 0.0100 | Batch:  32 | Patience: 187/500 | Rate: 99.4 iter/s | Time: 106.49s\n",
      "Iter 10600 | Train Loss: 0.394219 | LR: 0.0100 | Batch:  32 | Patience: 191/500 | Rate: 99.4 iter/s | Time: 106.69s\n",
      "Iter 10620 | Train Loss: 0.394186 | LR: 0.0100 | Batch:  32 | Patience: 195/500 | Rate: 99.4 iter/s | Time: 106.88s\n",
      "Iter 10640 | Train Loss: 0.394205 | LR: 0.0100 | Batch:  32 | Patience: 199/500 | Rate: 99.4 iter/s | Time: 107.08s\n",
      "Iter 10660 | Train Loss: 0.394158 | LR: 0.0100 | Batch:  32 | Patience: 203/500 | Rate: 99.4 iter/s | Time: 107.28s\n",
      "Iter 10680 | Train Loss: 0.394139 | LR: 0.0100 | Batch:  32 | Patience: 207/500 | Rate: 99.4 iter/s | Time: 107.48s\n",
      "Iter 10700 | Train Loss: 0.394145 | LR: 0.0100 | Batch:  32 | Patience: 211/500 | Rate: 99.4 iter/s | Time: 107.68s\n",
      "Iter 10720 | Train Loss: 0.394170 | LR: 0.0100 | Batch:  32 | Patience: 215/500 | Rate: 99.4 iter/s | Time: 107.87s\n",
      "Iter 10740 | Train Loss: 0.394164 | LR: 0.0100 | Batch:  32 | Patience: 219/500 | Rate: 99.4 iter/s | Time: 108.05s\n",
      "Iter 10760 | Train Loss: 0.394182 | LR: 0.0100 | Batch:  32 | Patience: 223/500 | Rate: 99.4 iter/s | Time: 108.29s\n",
      "Iter 10780 | Train Loss: 0.394197 | LR: 0.0100 | Batch:  32 | Patience: 227/500 | Rate: 99.4 iter/s | Time: 108.49s\n",
      "Iter 10800 | Train Loss: 0.394209 | LR: 0.0100 | Batch:  32 | Patience: 231/500 | Rate: 99.4 iter/s | Time: 108.68s\n",
      "Iter 10820 | Train Loss: 0.394185 | LR: 0.0100 | Batch:  32 | Patience: 235/500 | Rate: 99.4 iter/s | Time: 108.87s\n",
      "Iter 10840 | Train Loss: 0.394236 | LR: 0.0100 | Batch:  32 | Patience: 239/500 | Rate: 99.4 iter/s | Time: 109.06s\n",
      "Iter 10860 | Train Loss: 0.394273 | LR: 0.0100 | Batch:  32 | Patience: 243/500 | Rate: 99.4 iter/s | Time: 109.25s\n",
      "Iter 10880 | Train Loss: 0.394279 | LR: 0.0100 | Batch:  32 | Patience: 247/500 | Rate: 99.4 iter/s | Time: 109.45s\n",
      "Iter 10900 | Train Loss: 0.394252 | LR: 0.0100 | Batch:  32 | Patience: 251/500 | Rate: 99.4 iter/s | Time: 109.65s\n",
      "Iter 10920 | Train Loss: 0.394238 | LR: 0.0100 | Batch:  32 | Patience: 255/500 | Rate: 99.4 iter/s | Time: 109.85s\n",
      "Iter 10940 | Train Loss: 0.394226 | LR: 0.0100 | Batch:  32 | Patience: 259/500 | Rate: 99.4 iter/s | Time: 110.04s\n",
      "Iter 10960 | Train Loss: 0.394215 | LR: 0.0100 | Batch:  32 | Patience: 263/500 | Rate: 99.4 iter/s | Time: 110.23s\n",
      "Iter 10980 | Train Loss: 0.394234 | LR: 0.0100 | Batch:  32 | Patience: 267/500 | Rate: 99.4 iter/s | Time: 110.43s\n",
      "Iter 11000 | Train Loss: 0.394244 | LR: 0.0100 | Batch:  32 | Patience: 271/500 | Rate: 99.4 iter/s | Time: 110.63s\n",
      "Iter 11020 | Train Loss: 0.394249 | LR: 0.0100 | Batch:  32 | Patience: 275/500 | Rate: 99.4 iter/s | Time: 110.82s\n",
      "Iter 11040 | Train Loss: 0.394252 | LR: 0.0100 | Batch:  32 | Patience: 279/500 | Rate: 99.4 iter/s | Time: 111.01s\n",
      "Iter 11060 | Train Loss: 0.394281 | LR: 0.0100 | Batch:  32 | Patience: 283/500 | Rate: 99.5 iter/s | Time: 111.21s\n",
      "Iter 11080 | Train Loss: 0.394263 | LR: 0.0100 | Batch:  32 | Patience: 287/500 | Rate: 99.5 iter/s | Time: 111.40s\n",
      "Iter 11100 | Train Loss: 0.394254 | LR: 0.0100 | Batch:  32 | Patience: 291/500 | Rate: 99.5 iter/s | Time: 111.59s\n",
      "Iter 11120 | Train Loss: 0.394255 | LR: 0.0100 | Batch:  32 | Patience: 295/500 | Rate: 99.5 iter/s | Time: 111.79s\n",
      "Iter 11140 | Train Loss: 0.394242 | LR: 0.0100 | Batch:  32 | Patience: 299/500 | Rate: 99.5 iter/s | Time: 111.98s\n",
      "Iter 11160 | Train Loss: 0.394231 | LR: 0.0100 | Batch:  32 | Patience: 303/500 | Rate: 99.5 iter/s | Time: 112.17s\n",
      "Iter 11180 | Train Loss: 0.394228 | LR: 0.0100 | Batch:  32 | Patience: 307/500 | Rate: 99.5 iter/s | Time: 112.38s\n",
      "Iter 11200 | Train Loss: 0.394241 | LR: 0.0100 | Batch:  32 | Patience: 311/500 | Rate: 99.5 iter/s | Time: 112.60s\n",
      "Iter 11220 | Train Loss: 0.394232 | LR: 0.0100 | Batch:  32 | Patience: 315/500 | Rate: 99.5 iter/s | Time: 112.79s\n",
      "Iter 11240 | Train Loss: 0.394198 | LR: 0.0100 | Batch:  32 | Patience: 319/500 | Rate: 99.5 iter/s | Time: 113.00s\n",
      "Iter 11260 | Train Loss: 0.394173 | LR: 0.0100 | Batch:  32 | Patience: 323/500 | Rate: 99.5 iter/s | Time: 113.19s\n",
      "Iter 11280 | Train Loss: 0.394159 | LR: 0.0100 | Batch:  32 | Patience: 327/500 | Rate: 99.5 iter/s | Time: 113.38s\n",
      "Iter 11300 | Train Loss: 0.394179 | LR: 0.0100 | Batch:  32 | Patience: 331/500 | Rate: 99.5 iter/s | Time: 113.58s\n",
      "Iter 11320 | Train Loss: 0.394175 | LR: 0.0100 | Batch:  32 | Patience: 335/500 | Rate: 99.5 iter/s | Time: 113.77s\n",
      "Iter 11340 | Train Loss: 0.394187 | LR: 0.0100 | Batch:  32 | Patience: 339/500 | Rate: 99.5 iter/s | Time: 113.96s\n",
      "Iter 11360 | Train Loss: 0.394198 | LR: 0.0100 | Batch:  32 | Patience: 343/500 | Rate: 99.5 iter/s | Time: 114.16s\n",
      "Iter 11380 | Train Loss: 0.394188 | LR: 0.0100 | Batch:  32 | Patience: 347/500 | Rate: 99.5 iter/s | Time: 114.35s\n",
      "Iter 11400 | Train Loss: 0.394178 | LR: 0.0100 | Batch:  32 | Patience: 351/500 | Rate: 99.5 iter/s | Time: 114.55s\n",
      "Iter 11420 | Train Loss: 0.394210 | LR: 0.0100 | Batch:  32 | Patience: 355/500 | Rate: 99.5 iter/s | Time: 114.74s\n",
      "Iter 11440 | Train Loss: 0.394282 | LR: 0.0100 | Batch:  32 | Patience: 359/500 | Rate: 99.5 iter/s | Time: 114.93s\n",
      "Iter 11460 | Train Loss: 0.394338 | LR: 0.0100 | Batch:  32 | Patience: 363/500 | Rate: 99.5 iter/s | Time: 115.12s\n",
      "Iter 11480 | Train Loss: 0.394420 | LR: 0.0100 | Batch:  32 | Patience: 367/500 | Rate: 99.6 iter/s | Time: 115.31s\n",
      "Iter 11500 | Train Loss: 0.394370 | LR: 0.0100 | Batch:  32 | Patience: 371/500 | Rate: 99.6 iter/s | Time: 115.50s\n",
      "Iter 11520 | Train Loss: 0.394292 | LR: 0.0100 | Batch:  32 | Patience: 375/500 | Rate: 99.6 iter/s | Time: 115.70s\n",
      "Iter 11540 | Train Loss: 0.394351 | LR: 0.0100 | Batch:  32 | Patience: 379/500 | Rate: 99.6 iter/s | Time: 115.89s\n",
      "Iter 11560 | Train Loss: 0.394312 | LR: 0.0100 | Batch:  32 | Patience: 383/500 | Rate: 99.6 iter/s | Time: 116.09s\n",
      "Iter 11580 | Train Loss: 0.394310 | LR: 0.0100 | Batch:  32 | Patience: 387/500 | Rate: 99.6 iter/s | Time: 116.28s\n",
      "Iter 11600 | Train Loss: 0.394282 | LR: 0.0100 | Batch:  32 | Patience: 391/500 | Rate: 99.6 iter/s | Time: 116.47s\n",
      "Iter 11620 | Train Loss: 0.394249 | LR: 0.0100 | Batch:  32 | Patience: 395/500 | Rate: 99.6 iter/s | Time: 116.67s\n",
      "Iter 11640 | Train Loss: 0.394222 | LR: 0.0100 | Batch:  32 | Patience: 399/500 | Rate: 99.6 iter/s | Time: 116.86s\n",
      "Iter 11660 | Train Loss: 0.394178 | LR: 0.0100 | Batch:  32 | Patience: 403/500 | Rate: 99.6 iter/s | Time: 117.06s\n",
      "Iter 11680 | Train Loss: 0.394197 | LR: 0.0100 | Batch:  32 | Patience: 407/500 | Rate: 99.6 iter/s | Time: 117.25s\n",
      "Iter 11700 | Train Loss: 0.394174 | LR: 0.0100 | Batch:  32 | Patience: 411/500 | Rate: 99.6 iter/s | Time: 117.44s\n",
      "Iter 11720 | Train Loss: 0.394221 | LR: 0.0100 | Batch:  32 | Patience: 415/500 | Rate: 99.6 iter/s | Time: 117.64s\n",
      "Iter 11740 | Train Loss: 0.394265 | LR: 0.0100 | Batch:  32 | Patience: 419/500 | Rate: 99.6 iter/s | Time: 117.83s\n",
      "Iter 11760 | Train Loss: 0.394281 | LR: 0.0100 | Batch:  32 | Patience: 423/500 | Rate: 99.6 iter/s | Time: 118.02s\n",
      "Iter 11780 | Train Loss: 0.394284 | LR: 0.0100 | Batch:  32 | Patience: 427/500 | Rate: 99.6 iter/s | Time: 118.21s\n",
      "Iter 11800 | Train Loss: 0.394265 | LR: 0.0100 | Batch:  32 | Patience: 431/500 | Rate: 99.7 iter/s | Time: 118.40s\n",
      "Iter 11820 | Train Loss: 0.394245 | LR: 0.0100 | Batch:  32 | Patience: 435/500 | Rate: 99.6 iter/s | Time: 118.62s\n",
      "Iter 11840 | Train Loss: 0.394211 | LR: 0.0100 | Batch:  32 | Patience: 439/500 | Rate: 99.6 iter/s | Time: 118.82s\n",
      "Iter 11860 | Train Loss: 0.394227 | LR: 0.0100 | Batch:  32 | Patience: 443/500 | Rate: 99.7 iter/s | Time: 119.01s\n",
      "Iter 11880 | Train Loss: 0.394192 | LR: 0.0100 | Batch:  32 | Patience: 447/500 | Rate: 99.6 iter/s | Time: 119.22s\n",
      "Iter 11900 | Train Loss: 0.394234 | LR: 0.0100 | Batch:  32 | Patience: 451/500 | Rate: 99.7 iter/s | Time: 119.41s\n",
      "Iter 11920 | Train Loss: 0.394207 | LR: 0.0100 | Batch:  32 | Patience: 455/500 | Rate: 99.7 iter/s | Time: 119.61s\n",
      "Iter 11940 | Train Loss: 0.394164 | LR: 0.0100 | Batch:  32 | Patience: 459/500 | Rate: 99.7 iter/s | Time: 119.81s\n",
      "Iter 11960 | Train Loss: 0.394212 | LR: 0.0100 | Batch:  32 | Patience: 463/500 | Rate: 99.7 iter/s | Time: 120.00s\n",
      "Iter 11980 | Train Loss: 0.394230 | LR: 0.0100 | Batch:  32 | Patience: 467/500 | Rate: 99.7 iter/s | Time: 120.19s\n",
      "Iter 12000 | Train Loss: 0.394272 | LR: 0.0100 | Batch:  32 | Patience: 471/500 | Rate: 99.7 iter/s | Time: 120.38s\n",
      "Iter 12020 | Train Loss: 0.394315 | LR: 0.0100 | Batch:  32 | Patience: 475/500 | Rate: 99.7 iter/s | Time: 120.59s\n",
      "Iter 12040 | Train Loss: 0.394309 | LR: 0.0100 | Batch:  32 | Patience: 479/500 | Rate: 99.7 iter/s | Time: 120.80s\n",
      "Iter 12060 | Train Loss: 0.394300 | LR: 0.0100 | Batch:  32 | Patience: 483/500 | Rate: 99.7 iter/s | Time: 120.99s\n",
      "Iter 12080 | Train Loss: 0.394267 | LR: 0.0100 | Batch:  32 | Patience: 487/500 | Rate: 99.7 iter/s | Time: 121.19s\n",
      "Iter 12100 | Train Loss: 0.394274 | LR: 0.0100 | Batch:  32 | Patience: 491/500 | Rate: 99.7 iter/s | Time: 121.38s\n",
      "Iter 12120 | Train Loss: 0.394257 | LR: 0.0100 | Batch:  32 | Patience: 495/500 | Rate: 99.7 iter/s | Time: 121.60s\n",
      "Iter 12140 | Train Loss: 0.394234 | LR: 0.0100 | Batch:  32 | Patience: 499/500 | Rate: 99.7 iter/s | Time: 121.80s\n",
      "\n",
      "Early stopping triggered at iteration 12141\n",
      "No improvement in loss for 500 checks (threshold: 1e-06)\n",
      "Final loss: 0.394230\n",
      "Total training time: 121.83s\n",
      "Average time per iteration: 0.0100s\n",
      "Total iterations: 12141\n",
      "Approximate epochs: 0.89\n",
      "Starting SGD Iterations training\n",
      "Learning rate: 0.05, Batch size: 32\n",
      "Max iterations: 100000\n",
      "Early stopping: patience=500, threshold=1e-06\n",
      "Using weighted loss with training weights shape: (438744,)\n",
      "Initial full dataset loss: 0.691674\n",
      "----------------------------------------------------------------------\n",
      "Iter    20 | Train Loss: 0.470302 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 105.5 iter/s | Time: 0.19s\n",
      "Iter    40 | Train Loss: 0.432926 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 106.4 iter/s | Time: 0.38s\n",
      "Iter    60 | Train Loss: 0.418716 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 106.5 iter/s | Time: 0.56s\n",
      "Iter    80 | Train Loss: 0.412973 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 106.1 iter/s | Time: 0.75s\n",
      "Iter   100 | Train Loss: 0.409168 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 103.5 iter/s | Time: 0.97s\n",
      "Iter   120 | Train Loss: 0.406117 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 103.4 iter/s | Time: 1.16s\n",
      "Iter   140 | Train Loss: 0.404383 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 102.6 iter/s | Time: 1.36s\n",
      "Iter   160 | Train Loss: 0.402756 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 102.6 iter/s | Time: 1.56s\n",
      "Iter   180 | Train Loss: 0.400784 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 102.2 iter/s | Time: 1.76s\n",
      "Iter   200 | Train Loss: 0.399567 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 102.1 iter/s | Time: 1.96s\n",
      "Iter   220 | Train Loss: 0.399368 | LR: 0.0500 | Batch:  32 | Patience:   3/500 | Rate: 102.0 iter/s | Time: 2.16s\n",
      "Iter   240 | Train Loss: 0.398252 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 101.7 iter/s | Time: 2.36s\n",
      "Iter   260 | Train Loss: 0.397481 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 100.1 iter/s | Time: 2.60s\n",
      "Iter   280 | Train Loss: 0.397071 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 100.1 iter/s | Time: 2.80s\n",
      "Iter   300 | Train Loss: 0.396907 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 99.9 iter/s | Time: 3.00s\n",
      "Iter   320 | Train Loss: 0.396461 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 100.3 iter/s | Time: 3.19s\n",
      "Iter   340 | Train Loss: 0.396489 | LR: 0.0500 | Batch:  32 | Patience:   3/500 | Rate: 99.9 iter/s | Time: 3.40s\n",
      "Iter   360 | Train Loss: 0.396372 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 100.2 iter/s | Time: 3.59s\n",
      "Iter   380 | Train Loss: 0.396390 | LR: 0.0500 | Batch:  32 | Patience:   1/500 | Rate: 100.2 iter/s | Time: 3.79s\n",
      "Iter   400 | Train Loss: 0.395883 | LR: 0.0500 | Batch:  32 | Patience:   1/500 | Rate: 100.4 iter/s | Time: 3.98s\n",
      "Iter   420 | Train Loss: 0.395616 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 100.5 iter/s | Time: 4.18s\n",
      "Iter   440 | Train Loss: 0.395300 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 100.7 iter/s | Time: 4.37s\n",
      "Iter   460 | Train Loss: 0.395812 | LR: 0.0500 | Batch:  32 | Patience:   4/500 | Rate: 100.8 iter/s | Time: 4.56s\n",
      "Iter   480 | Train Loss: 0.395854 | LR: 0.0500 | Batch:  32 | Patience:   8/500 | Rate: 101.0 iter/s | Time: 4.75s\n",
      "Iter   500 | Train Loss: 0.396214 | LR: 0.0500 | Batch:  32 | Patience:  12/500 | Rate: 100.7 iter/s | Time: 4.96s\n",
      "Iter   520 | Train Loss: 0.395919 | LR: 0.0500 | Batch:  32 | Patience:  16/500 | Rate: 100.8 iter/s | Time: 5.16s\n",
      "Iter   540 | Train Loss: 0.395857 | LR: 0.0500 | Batch:  32 | Patience:  20/500 | Rate: 100.9 iter/s | Time: 5.35s\n",
      "Iter   560 | Train Loss: 0.396134 | LR: 0.0500 | Batch:  32 | Patience:  24/500 | Rate: 101.0 iter/s | Time: 5.54s\n",
      "Iter   580 | Train Loss: 0.396277 | LR: 0.0500 | Batch:  32 | Patience:  28/500 | Rate: 101.0 iter/s | Time: 5.74s\n",
      "Iter   600 | Train Loss: 0.396760 | LR: 0.0500 | Batch:  32 | Patience:  32/500 | Rate: 101.0 iter/s | Time: 5.94s\n",
      "Iter   620 | Train Loss: 0.395823 | LR: 0.0500 | Batch:  32 | Patience:  36/500 | Rate: 101.0 iter/s | Time: 6.14s\n",
      "Iter   640 | Train Loss: 0.395612 | LR: 0.0500 | Batch:  32 | Patience:  40/500 | Rate: 101.1 iter/s | Time: 6.33s\n",
      "Iter   660 | Train Loss: 0.395627 | LR: 0.0500 | Batch:  32 | Patience:  44/500 | Rate: 101.2 iter/s | Time: 6.52s\n",
      "Iter   680 | Train Loss: 0.395585 | LR: 0.0500 | Batch:  32 | Patience:  48/500 | Rate: 101.3 iter/s | Time: 6.71s\n",
      "Iter   700 | Train Loss: 0.395358 | LR: 0.0500 | Batch:  32 | Patience:  52/500 | Rate: 101.2 iter/s | Time: 6.92s\n",
      "Iter   720 | Train Loss: 0.395774 | LR: 0.0500 | Batch:  32 | Patience:  56/500 | Rate: 101.2 iter/s | Time: 7.11s\n",
      "Iter   740 | Train Loss: 0.395245 | LR: 0.0500 | Batch:  32 | Patience:  60/500 | Rate: 101.2 iter/s | Time: 7.31s\n",
      "Iter   760 | Train Loss: 0.394966 | LR: 0.0500 | Batch:  32 | Patience:   2/500 | Rate: 101.3 iter/s | Time: 7.50s\n",
      "Iter   780 | Train Loss: 0.394764 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 101.3 iter/s | Time: 7.70s\n",
      "Iter   800 | Train Loss: 0.394769 | LR: 0.0500 | Batch:  32 | Patience:   1/500 | Rate: 101.4 iter/s | Time: 7.89s\n",
      "Iter   820 | Train Loss: 0.395079 | LR: 0.0500 | Batch:  32 | Patience:   5/500 | Rate: 101.4 iter/s | Time: 8.09s\n",
      "Iter   840 | Train Loss: 0.395227 | LR: 0.0500 | Batch:  32 | Patience:   9/500 | Rate: 101.5 iter/s | Time: 8.28s\n",
      "Iter   860 | Train Loss: 0.394800 | LR: 0.0500 | Batch:  32 | Patience:  13/500 | Rate: 101.5 iter/s | Time: 8.48s\n",
      "Iter   880 | Train Loss: 0.395049 | LR: 0.0500 | Batch:  32 | Patience:  17/500 | Rate: 101.5 iter/s | Time: 8.67s\n",
      "Iter   900 | Train Loss: 0.394864 | LR: 0.0500 | Batch:  32 | Patience:  21/500 | Rate: 101.5 iter/s | Time: 8.87s\n",
      "Iter   920 | Train Loss: 0.394655 | LR: 0.0500 | Batch:  32 | Patience:  25/500 | Rate: 101.5 iter/s | Time: 9.07s\n",
      "Iter   940 | Train Loss: 0.394435 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 101.4 iter/s | Time: 9.27s\n",
      "Iter   960 | Train Loss: 0.394660 | LR: 0.0500 | Batch:  32 | Patience:   3/500 | Rate: 101.5 iter/s | Time: 9.46s\n",
      "Iter   980 | Train Loss: 0.394876 | LR: 0.0500 | Batch:  32 | Patience:   7/500 | Rate: 101.5 iter/s | Time: 9.65s\n",
      "Iter  1000 | Train Loss: 0.394569 | LR: 0.0500 | Batch:  32 | Patience:  11/500 | Rate: 101.6 iter/s | Time: 9.84s\n",
      "Iter  1020 | Train Loss: 0.394628 | LR: 0.0500 | Batch:  32 | Patience:  15/500 | Rate: 101.4 iter/s | Time: 10.06s\n",
      "Iter  1040 | Train Loss: 0.394587 | LR: 0.0500 | Batch:  32 | Patience:  19/500 | Rate: 101.5 iter/s | Time: 10.25s\n",
      "Iter  1060 | Train Loss: 0.394670 | LR: 0.0500 | Batch:  32 | Patience:  23/500 | Rate: 101.5 iter/s | Time: 10.45s\n",
      "Iter  1080 | Train Loss: 0.394750 | LR: 0.0500 | Batch:  32 | Patience:  27/500 | Rate: 101.5 iter/s | Time: 10.64s\n",
      "Iter  1100 | Train Loss: 0.394963 | LR: 0.0500 | Batch:  32 | Patience:  31/500 | Rate: 101.5 iter/s | Time: 10.84s\n",
      "Iter  1120 | Train Loss: 0.395021 | LR: 0.0500 | Batch:  32 | Patience:  35/500 | Rate: 101.4 iter/s | Time: 11.04s\n",
      "Iter  1140 | Train Loss: 0.395064 | LR: 0.0500 | Batch:  32 | Patience:  39/500 | Rate: 101.4 iter/s | Time: 11.25s\n",
      "Iter  1160 | Train Loss: 0.394909 | LR: 0.0500 | Batch:  32 | Patience:  43/500 | Rate: 101.3 iter/s | Time: 11.45s\n",
      "Iter  1180 | Train Loss: 0.395222 | LR: 0.0500 | Batch:  32 | Patience:  47/500 | Rate: 101.3 iter/s | Time: 11.65s\n",
      "Iter  1200 | Train Loss: 0.394747 | LR: 0.0500 | Batch:  32 | Patience:  51/500 | Rate: 101.2 iter/s | Time: 11.85s\n",
      "Iter  1220 | Train Loss: 0.394849 | LR: 0.0500 | Batch:  32 | Patience:  55/500 | Rate: 101.2 iter/s | Time: 12.05s\n",
      "Iter  1240 | Train Loss: 0.394796 | LR: 0.0500 | Batch:  32 | Patience:  59/500 | Rate: 101.2 iter/s | Time: 12.25s\n",
      "Iter  1260 | Train Loss: 0.395440 | LR: 0.0500 | Batch:  32 | Patience:  63/500 | Rate: 101.2 iter/s | Time: 12.45s\n",
      "Iter  1280 | Train Loss: 0.395055 | LR: 0.0500 | Batch:  32 | Patience:  67/500 | Rate: 101.2 iter/s | Time: 12.65s\n",
      "Iter  1300 | Train Loss: 0.395093 | LR: 0.0500 | Batch:  32 | Patience:  71/500 | Rate: 101.2 iter/s | Time: 12.85s\n",
      "Iter  1320 | Train Loss: 0.395084 | LR: 0.0500 | Batch:  32 | Patience:  75/500 | Rate: 101.2 iter/s | Time: 13.05s\n",
      "Iter  1340 | Train Loss: 0.394871 | LR: 0.0500 | Batch:  32 | Patience:  79/500 | Rate: 101.2 iter/s | Time: 13.24s\n",
      "Iter  1360 | Train Loss: 0.395254 | LR: 0.0500 | Batch:  32 | Patience:  83/500 | Rate: 101.2 iter/s | Time: 13.44s\n",
      "Iter  1380 | Train Loss: 0.394567 | LR: 0.0500 | Batch:  32 | Patience:  87/500 | Rate: 101.3 iter/s | Time: 13.63s\n",
      "Iter  1400 | Train Loss: 0.394592 | LR: 0.0500 | Batch:  32 | Patience:  91/500 | Rate: 101.3 iter/s | Time: 13.82s\n",
      "Iter  1420 | Train Loss: 0.394768 | LR: 0.0500 | Batch:  32 | Patience:  95/500 | Rate: 101.3 iter/s | Time: 14.01s\n",
      "Iter  1440 | Train Loss: 0.394763 | LR: 0.0500 | Batch:  32 | Patience:  99/500 | Rate: 101.3 iter/s | Time: 14.21s\n",
      "Iter  1460 | Train Loss: 0.394395 | LR: 0.0500 | Batch:  32 | Patience: 103/500 | Rate: 101.4 iter/s | Time: 14.40s\n",
      "Iter  1480 | Train Loss: 0.394497 | LR: 0.0500 | Batch:  32 | Patience:   3/500 | Rate: 101.4 iter/s | Time: 14.60s\n",
      "Iter  1500 | Train Loss: 0.394612 | LR: 0.0500 | Batch:  32 | Patience:   7/500 | Rate: 101.4 iter/s | Time: 14.79s\n",
      "Iter  1520 | Train Loss: 0.394466 | LR: 0.0500 | Batch:  32 | Patience:  11/500 | Rate: 101.4 iter/s | Time: 14.99s\n",
      "Iter  1540 | Train Loss: 0.394963 | LR: 0.0500 | Batch:  32 | Patience:  15/500 | Rate: 101.4 iter/s | Time: 15.18s\n",
      "Iter  1560 | Train Loss: 0.394446 | LR: 0.0500 | Batch:  32 | Patience:   0/500 | Rate: 101.4 iter/s | Time: 15.38s\n",
      "Iter  1580 | Train Loss: 0.394491 | LR: 0.0500 | Batch:  32 | Patience:   4/500 | Rate: 101.5 iter/s | Time: 15.57s\n",
      "Iter  1600 | Train Loss: 0.394393 | LR: 0.0500 | Batch:  32 | Patience:   1/500 | Rate: 101.4 iter/s | Time: 15.78s\n",
      "Iter  1620 | Train Loss: 0.394570 | LR: 0.0500 | Batch:  32 | Patience:   5/500 | Rate: 101.4 iter/s | Time: 15.97s\n",
      "Iter  1640 | Train Loss: 0.394799 | LR: 0.0500 | Batch:  32 | Patience:   9/500 | Rate: 101.4 iter/s | Time: 16.17s\n",
      "Iter  1660 | Train Loss: 0.395040 | LR: 0.0500 | Batch:  32 | Patience:  13/500 | Rate: 101.5 iter/s | Time: 16.36s\n",
      "Iter  1680 | Train Loss: 0.395252 | LR: 0.0500 | Batch:  32 | Patience:  17/500 | Rate: 101.4 iter/s | Time: 16.57s\n",
      "Iter  1700 | Train Loss: 0.395050 | LR: 0.0500 | Batch:  32 | Patience:  21/500 | Rate: 101.4 iter/s | Time: 16.76s\n",
      "Iter  1720 | Train Loss: 0.394860 | LR: 0.0500 | Batch:  32 | Patience:  25/500 | Rate: 101.4 iter/s | Time: 16.96s\n",
      "Iter  1740 | Train Loss: 0.394791 | LR: 0.0500 | Batch:  32 | Patience:  29/500 | Rate: 101.4 iter/s | Time: 17.16s\n",
      "Iter  1760 | Train Loss: 0.394652 | LR: 0.0500 | Batch:  32 | Patience:  33/500 | Rate: 101.4 iter/s | Time: 17.36s\n",
      "Iter  1780 | Train Loss: 0.394841 | LR: 0.0500 | Batch:  32 | Patience:  37/500 | Rate: 101.4 iter/s | Time: 17.55s\n",
      "Iter  1800 | Train Loss: 0.395152 | LR: 0.0500 | Batch:  32 | Patience:  41/500 | Rate: 101.4 iter/s | Time: 17.75s\n",
      "Iter  1820 | Train Loss: 0.395065 | LR: 0.0500 | Batch:  32 | Patience:  45/500 | Rate: 101.4 iter/s | Time: 17.95s\n",
      "Iter  1840 | Train Loss: 0.395234 | LR: 0.0500 | Batch:  32 | Patience:  49/500 | Rate: 101.4 iter/s | Time: 18.15s\n",
      "Iter  1860 | Train Loss: 0.394770 | LR: 0.0500 | Batch:  32 | Patience:  53/500 | Rate: 101.4 iter/s | Time: 18.35s\n",
      "Iter  1880 | Train Loss: 0.394706 | LR: 0.0500 | Batch:  32 | Patience:  57/500 | Rate: 101.4 iter/s | Time: 18.54s\n",
      "Iter  1900 | Train Loss: 0.395013 | LR: 0.0500 | Batch:  32 | Patience:  61/500 | Rate: 101.4 iter/s | Time: 18.73s\n",
      "Iter  1920 | Train Loss: 0.394986 | LR: 0.0500 | Batch:  32 | Patience:  65/500 | Rate: 101.5 iter/s | Time: 18.92s\n",
      "Iter  1940 | Train Loss: 0.395349 | LR: 0.0500 | Batch:  32 | Patience:  69/500 | Rate: 101.5 iter/s | Time: 19.11s\n",
      "Iter  1960 | Train Loss: 0.395101 | LR: 0.0500 | Batch:  32 | Patience:  73/500 | Rate: 101.5 iter/s | Time: 19.30s\n",
      "Iter  1980 | Train Loss: 0.395319 | LR: 0.0500 | Batch:  32 | Patience:  77/500 | Rate: 101.6 iter/s | Time: 19.49s\n",
      "Iter  2000 | Train Loss: 0.394878 | LR: 0.0500 | Batch:  32 | Patience:  81/500 | Rate: 101.6 iter/s | Time: 19.68s\n",
      "Iter  2020 | Train Loss: 0.395283 | LR: 0.0500 | Batch:  32 | Patience:  85/500 | Rate: 101.6 iter/s | Time: 19.88s\n",
      "Iter  2040 | Train Loss: 0.394916 | LR: 0.0500 | Batch:  32 | Patience:  89/500 | Rate: 101.6 iter/s | Time: 20.07s\n",
      "Iter  2060 | Train Loss: 0.395263 | LR: 0.0500 | Batch:  32 | Patience:  93/500 | Rate: 101.7 iter/s | Time: 20.26s\n",
      "Iter  2080 | Train Loss: 0.395629 | LR: 0.0500 | Batch:  32 | Patience:  97/500 | Rate: 101.6 iter/s | Time: 20.47s\n",
      "Iter  2100 | Train Loss: 0.395456 | LR: 0.0500 | Batch:  32 | Patience: 101/500 | Rate: 101.4 iter/s | Time: 20.71s\n",
      "Iter  2120 | Train Loss: 0.394941 | LR: 0.0500 | Batch:  32 | Patience: 105/500 | Rate: 101.3 iter/s | Time: 20.94s\n",
      "Iter  2140 | Train Loss: 0.394936 | LR: 0.0500 | Batch:  32 | Patience: 109/500 | Rate: 100.9 iter/s | Time: 21.22s\n",
      "Iter  2160 | Train Loss: 0.394891 | LR: 0.0500 | Batch:  32 | Patience: 113/500 | Rate: 100.7 iter/s | Time: 21.45s\n",
      "Iter  2180 | Train Loss: 0.394990 | LR: 0.0500 | Batch:  32 | Patience: 117/500 | Rate: 100.7 iter/s | Time: 21.66s\n",
      "Iter  2200 | Train Loss: 0.394898 | LR: 0.0500 | Batch:  32 | Patience: 121/500 | Rate: 100.5 iter/s | Time: 21.88s\n",
      "Iter  2220 | Train Loss: 0.394711 | LR: 0.0500 | Batch:  32 | Patience: 125/500 | Rate: 100.5 iter/s | Time: 22.09s\n",
      "Iter  2240 | Train Loss: 0.394789 | LR: 0.0500 | Batch:  32 | Patience: 129/500 | Rate: 100.5 iter/s | Time: 22.30s\n",
      "Iter  2260 | Train Loss: 0.394530 | LR: 0.0500 | Batch:  32 | Patience: 133/500 | Rate: 100.4 iter/s | Time: 22.51s\n",
      "Iter  2280 | Train Loss: 0.394584 | LR: 0.0500 | Batch:  32 | Patience: 137/500 | Rate: 100.4 iter/s | Time: 22.71s\n",
      "Iter  2300 | Train Loss: 0.394878 | LR: 0.0500 | Batch:  32 | Patience: 141/500 | Rate: 100.4 iter/s | Time: 22.91s\n",
      "Iter  2320 | Train Loss: 0.394714 | LR: 0.0500 | Batch:  32 | Patience: 145/500 | Rate: 100.4 iter/s | Time: 23.11s\n",
      "Iter  2340 | Train Loss: 0.394792 | LR: 0.0500 | Batch:  32 | Patience: 149/500 | Rate: 100.4 iter/s | Time: 23.31s\n",
      "Iter  2360 | Train Loss: 0.394930 | LR: 0.0500 | Batch:  32 | Patience: 153/500 | Rate: 100.3 iter/s | Time: 23.53s\n",
      "Iter  2380 | Train Loss: 0.395041 | LR: 0.0500 | Batch:  32 | Patience: 157/500 | Rate: 100.2 iter/s | Time: 23.74s\n",
      "Iter  2400 | Train Loss: 0.395012 | LR: 0.0500 | Batch:  32 | Patience: 161/500 | Rate: 100.2 iter/s | Time: 23.96s\n",
      "Iter  2420 | Train Loss: 0.394809 | LR: 0.0500 | Batch:  32 | Patience: 165/500 | Rate: 100.2 iter/s | Time: 24.15s\n",
      "Iter  2440 | Train Loss: 0.394854 | LR: 0.0500 | Batch:  32 | Patience: 169/500 | Rate: 100.2 iter/s | Time: 24.35s\n",
      "Iter  2460 | Train Loss: 0.394556 | LR: 0.0500 | Batch:  32 | Patience: 173/500 | Rate: 100.2 iter/s | Time: 24.55s\n",
      "Iter  2480 | Train Loss: 0.394931 | LR: 0.0500 | Batch:  32 | Patience: 177/500 | Rate: 100.2 iter/s | Time: 24.75s\n",
      "Iter  2500 | Train Loss: 0.394943 | LR: 0.0500 | Batch:  32 | Patience: 181/500 | Rate: 100.3 iter/s | Time: 24.94s\n",
      "Iter  2520 | Train Loss: 0.394718 | LR: 0.0500 | Batch:  32 | Patience: 185/500 | Rate: 100.3 iter/s | Time: 25.14s\n",
      "Iter  2540 | Train Loss: 0.394765 | LR: 0.0500 | Batch:  32 | Patience: 189/500 | Rate: 100.2 iter/s | Time: 25.34s\n",
      "Iter  2560 | Train Loss: 0.395312 | LR: 0.0500 | Batch:  32 | Patience: 193/500 | Rate: 100.2 iter/s | Time: 25.54s\n",
      "Iter  2580 | Train Loss: 0.395243 | LR: 0.0500 | Batch:  32 | Patience: 197/500 | Rate: 100.2 iter/s | Time: 25.74s\n",
      "Iter  2600 | Train Loss: 0.394901 | LR: 0.0500 | Batch:  32 | Patience: 201/500 | Rate: 100.2 iter/s | Time: 25.94s\n",
      "Iter  2620 | Train Loss: 0.394660 | LR: 0.0500 | Batch:  32 | Patience: 205/500 | Rate: 100.3 iter/s | Time: 26.13s\n",
      "Iter  2640 | Train Loss: 0.394776 | LR: 0.0500 | Batch:  32 | Patience: 209/500 | Rate: 100.3 iter/s | Time: 26.33s\n",
      "Iter  2660 | Train Loss: 0.394794 | LR: 0.0500 | Batch:  32 | Patience: 213/500 | Rate: 100.3 iter/s | Time: 26.53s\n",
      "Iter  2680 | Train Loss: 0.394885 | LR: 0.0500 | Batch:  32 | Patience: 217/500 | Rate: 100.3 iter/s | Time: 26.72s\n",
      "Iter  2700 | Train Loss: 0.394770 | LR: 0.0500 | Batch:  32 | Patience: 221/500 | Rate: 100.3 iter/s | Time: 26.92s\n",
      "Iter  2720 | Train Loss: 0.394561 | LR: 0.0500 | Batch:  32 | Patience: 225/500 | Rate: 100.3 iter/s | Time: 27.12s\n",
      "Iter  2740 | Train Loss: 0.394846 | LR: 0.0500 | Batch:  32 | Patience: 229/500 | Rate: 100.2 iter/s | Time: 27.33s\n",
      "Iter  2760 | Train Loss: 0.394897 | LR: 0.0500 | Batch:  32 | Patience: 233/500 | Rate: 100.2 iter/s | Time: 27.54s\n",
      "Iter  2780 | Train Loss: 0.395550 | LR: 0.0500 | Batch:  32 | Patience: 237/500 | Rate: 100.2 iter/s | Time: 27.73s\n",
      "Iter  2800 | Train Loss: 0.395164 | LR: 0.0500 | Batch:  32 | Patience: 241/500 | Rate: 100.3 iter/s | Time: 27.93s\n",
      "Iter  2820 | Train Loss: 0.395460 | LR: 0.0500 | Batch:  32 | Patience: 245/500 | Rate: 100.3 iter/s | Time: 28.12s\n",
      "Iter  2840 | Train Loss: 0.395486 | LR: 0.0500 | Batch:  32 | Patience: 249/500 | Rate: 100.3 iter/s | Time: 28.32s\n",
      "Iter  2860 | Train Loss: 0.395028 | LR: 0.0500 | Batch:  32 | Patience: 253/500 | Rate: 100.3 iter/s | Time: 28.51s\n",
      "Iter  2880 | Train Loss: 0.394798 | LR: 0.0500 | Batch:  32 | Patience: 257/500 | Rate: 100.3 iter/s | Time: 28.70s\n",
      "Iter  2900 | Train Loss: 0.394670 | LR: 0.0500 | Batch:  32 | Patience: 261/500 | Rate: 100.4 iter/s | Time: 28.89s\n",
      "Iter  2920 | Train Loss: 0.394753 | LR: 0.0500 | Batch:  32 | Patience: 265/500 | Rate: 100.4 iter/s | Time: 29.09s\n",
      "Iter  2940 | Train Loss: 0.394638 | LR: 0.0500 | Batch:  32 | Patience: 269/500 | Rate: 100.3 iter/s | Time: 29.30s\n",
      "Iter  2960 | Train Loss: 0.394430 | LR: 0.0500 | Batch:  32 | Patience: 273/500 | Rate: 100.0 iter/s | Time: 29.60s\n",
      "Iter  2980 | Train Loss: 0.394673 | LR: 0.0500 | Batch:  32 | Patience: 277/500 | Rate: 100.0 iter/s | Time: 29.79s\n",
      "Iter  3000 | Train Loss: 0.394677 | LR: 0.0500 | Batch:  32 | Patience: 281/500 | Rate: 100.1 iter/s | Time: 29.98s\n",
      "Iter  3020 | Train Loss: 0.394833 | LR: 0.0500 | Batch:  32 | Patience: 285/500 | Rate: 100.1 iter/s | Time: 30.18s\n",
      "Iter  3040 | Train Loss: 0.394614 | LR: 0.0500 | Batch:  32 | Patience: 289/500 | Rate: 100.1 iter/s | Time: 30.38s\n",
      "Iter  3060 | Train Loss: 0.394817 | LR: 0.0500 | Batch:  32 | Patience: 293/500 | Rate: 100.1 iter/s | Time: 30.57s\n",
      "Iter  3080 | Train Loss: 0.394756 | LR: 0.0500 | Batch:  32 | Patience: 297/500 | Rate: 100.1 iter/s | Time: 30.76s\n",
      "Iter  3100 | Train Loss: 0.394762 | LR: 0.0500 | Batch:  32 | Patience: 301/500 | Rate: 100.1 iter/s | Time: 30.95s\n",
      "Iter  3120 | Train Loss: 0.394874 | LR: 0.0500 | Batch:  32 | Patience: 305/500 | Rate: 100.2 iter/s | Time: 31.15s\n",
      "Iter  3140 | Train Loss: 0.394654 | LR: 0.0500 | Batch:  32 | Patience: 309/500 | Rate: 100.2 iter/s | Time: 31.35s\n",
      "Iter  3160 | Train Loss: 0.394695 | LR: 0.0500 | Batch:  32 | Patience: 313/500 | Rate: 100.2 iter/s | Time: 31.55s\n",
      "Iter  3180 | Train Loss: 0.394790 | LR: 0.0500 | Batch:  32 | Patience: 317/500 | Rate: 100.2 iter/s | Time: 31.74s\n",
      "Iter  3200 | Train Loss: 0.394720 | LR: 0.0500 | Batch:  32 | Patience: 321/500 | Rate: 100.2 iter/s | Time: 31.93s\n",
      "Iter  3220 | Train Loss: 0.394815 | LR: 0.0500 | Batch:  32 | Patience: 325/500 | Rate: 100.2 iter/s | Time: 32.12s\n",
      "Iter  3240 | Train Loss: 0.394790 | LR: 0.0500 | Batch:  32 | Patience: 329/500 | Rate: 100.2 iter/s | Time: 32.32s\n",
      "Iter  3260 | Train Loss: 0.394845 | LR: 0.0500 | Batch:  32 | Patience: 333/500 | Rate: 100.3 iter/s | Time: 32.51s\n",
      "Iter  3280 | Train Loss: 0.395376 | LR: 0.0500 | Batch:  32 | Patience: 337/500 | Rate: 100.3 iter/s | Time: 32.71s\n",
      "Iter  3300 | Train Loss: 0.395257 | LR: 0.0500 | Batch:  32 | Patience: 341/500 | Rate: 100.3 iter/s | Time: 32.90s\n",
      "Iter  3320 | Train Loss: 0.395027 | LR: 0.0500 | Batch:  32 | Patience: 345/500 | Rate: 100.3 iter/s | Time: 33.10s\n",
      "Iter  3340 | Train Loss: 0.394958 | LR: 0.0500 | Batch:  32 | Patience: 349/500 | Rate: 100.3 iter/s | Time: 33.29s\n",
      "Iter  3360 | Train Loss: 0.394876 | LR: 0.0500 | Batch:  32 | Patience: 353/500 | Rate: 100.3 iter/s | Time: 33.49s\n",
      "Iter  3380 | Train Loss: 0.394658 | LR: 0.0500 | Batch:  32 | Patience: 357/500 | Rate: 100.3 iter/s | Time: 33.68s\n",
      "Iter  3400 | Train Loss: 0.394726 | LR: 0.0500 | Batch:  32 | Patience: 361/500 | Rate: 100.4 iter/s | Time: 33.88s\n",
      "Iter  3420 | Train Loss: 0.395234 | LR: 0.0500 | Batch:  32 | Patience: 365/500 | Rate: 100.4 iter/s | Time: 34.07s\n",
      "Iter  3440 | Train Loss: 0.395444 | LR: 0.0500 | Batch:  32 | Patience: 369/500 | Rate: 100.4 iter/s | Time: 34.27s\n",
      "Iter  3460 | Train Loss: 0.394888 | LR: 0.0500 | Batch:  32 | Patience: 373/500 | Rate: 100.4 iter/s | Time: 34.46s\n",
      "Iter  3480 | Train Loss: 0.394759 | LR: 0.0500 | Batch:  32 | Patience: 377/500 | Rate: 100.4 iter/s | Time: 34.65s\n",
      "Iter  3500 | Train Loss: 0.394757 | LR: 0.0500 | Batch:  32 | Patience: 381/500 | Rate: 100.4 iter/s | Time: 34.85s\n",
      "Iter  3520 | Train Loss: 0.395035 | LR: 0.0500 | Batch:  32 | Patience: 385/500 | Rate: 100.4 iter/s | Time: 35.05s\n",
      "Iter  3540 | Train Loss: 0.394670 | LR: 0.0500 | Batch:  32 | Patience: 389/500 | Rate: 100.4 iter/s | Time: 35.25s\n",
      "Iter  3560 | Train Loss: 0.394535 | LR: 0.0500 | Batch:  32 | Patience: 393/500 | Rate: 100.4 iter/s | Time: 35.46s\n",
      "Iter  3580 | Train Loss: 0.394357 | LR: 0.0500 | Batch:  32 | Patience: 397/500 | Rate: 100.4 iter/s | Time: 35.66s\n",
      "Iter  3600 | Train Loss: 0.394389 | LR: 0.0500 | Batch:  32 | Patience: 401/500 | Rate: 100.4 iter/s | Time: 35.86s\n",
      "Iter  3620 | Train Loss: 0.394357 | LR: 0.0500 | Batch:  32 | Patience:   1/500 | Rate: 100.4 iter/s | Time: 36.05s\n",
      "Iter  3640 | Train Loss: 0.394374 | LR: 0.0500 | Batch:  32 | Patience:   5/500 | Rate: 100.4 iter/s | Time: 36.24s\n",
      "Iter  3660 | Train Loss: 0.394465 | LR: 0.0500 | Batch:  32 | Patience:   9/500 | Rate: 100.4 iter/s | Time: 36.44s\n",
      "Iter  3680 | Train Loss: 0.394531 | LR: 0.0500 | Batch:  32 | Patience:  13/500 | Rate: 100.5 iter/s | Time: 36.63s\n",
      "Iter  3700 | Train Loss: 0.394604 | LR: 0.0500 | Batch:  32 | Patience:  17/500 | Rate: 100.5 iter/s | Time: 36.82s\n",
      "Iter  3720 | Train Loss: 0.394456 | LR: 0.0500 | Batch:  32 | Patience:  21/500 | Rate: 100.5 iter/s | Time: 37.01s\n",
      "Iter  3740 | Train Loss: 0.394330 | LR: 0.0500 | Batch:  32 | Patience:  25/500 | Rate: 100.5 iter/s | Time: 37.21s\n",
      "Iter  3760 | Train Loss: 0.394321 | LR: 0.0500 | Batch:  32 | Patience:   2/500 | Rate: 100.5 iter/s | Time: 37.40s\n",
      "Iter  3780 | Train Loss: 0.394697 | LR: 0.0500 | Batch:  32 | Patience:   6/500 | Rate: 100.5 iter/s | Time: 37.60s\n",
      "Iter  3800 | Train Loss: 0.394666 | LR: 0.0500 | Batch:  32 | Patience:  10/500 | Rate: 100.6 iter/s | Time: 37.79s\n",
      "Iter  3820 | Train Loss: 0.395859 | LR: 0.0500 | Batch:  32 | Patience:  14/500 | Rate: 100.6 iter/s | Time: 37.98s\n",
      "Iter  3840 | Train Loss: 0.394895 | LR: 0.0500 | Batch:  32 | Patience:  18/500 | Rate: 100.6 iter/s | Time: 38.17s\n",
      "Iter  3860 | Train Loss: 0.394604 | LR: 0.0500 | Batch:  32 | Patience:  22/500 | Rate: 100.6 iter/s | Time: 38.37s\n",
      "Iter  3880 | Train Loss: 0.394897 | LR: 0.0500 | Batch:  32 | Patience:  26/500 | Rate: 100.6 iter/s | Time: 38.56s\n",
      "Iter  3900 | Train Loss: 0.395011 | LR: 0.0500 | Batch:  32 | Patience:  30/500 | Rate: 100.6 iter/s | Time: 38.75s\n",
      "Iter  3920 | Train Loss: 0.394865 | LR: 0.0500 | Batch:  32 | Patience:  34/500 | Rate: 100.7 iter/s | Time: 38.94s\n",
      "Iter  3940 | Train Loss: 0.394713 | LR: 0.0500 | Batch:  32 | Patience:  38/500 | Rate: 100.7 iter/s | Time: 39.13s\n",
      "Iter  3960 | Train Loss: 0.394773 | LR: 0.0500 | Batch:  32 | Patience:  42/500 | Rate: 100.7 iter/s | Time: 39.32s\n",
      "Iter  3980 | Train Loss: 0.394700 | LR: 0.0500 | Batch:  32 | Patience:  46/500 | Rate: 100.7 iter/s | Time: 39.52s\n",
      "Iter  4000 | Train Loss: 0.394670 | LR: 0.0500 | Batch:  32 | Patience:  50/500 | Rate: 100.6 iter/s | Time: 39.74s\n",
      "Iter  4020 | Train Loss: 0.394705 | LR: 0.0500 | Batch:  32 | Patience:  54/500 | Rate: 100.6 iter/s | Time: 39.96s\n",
      "Iter  4040 | Train Loss: 0.394982 | LR: 0.0500 | Batch:  32 | Patience:  58/500 | Rate: 100.5 iter/s | Time: 40.18s\n",
      "Iter  4060 | Train Loss: 0.394807 | LR: 0.0500 | Batch:  32 | Patience:  62/500 | Rate: 100.5 iter/s | Time: 40.40s\n",
      "Iter  4080 | Train Loss: 0.394932 | LR: 0.0500 | Batch:  32 | Patience:  66/500 | Rate: 100.4 iter/s | Time: 40.63s\n",
      "Iter  4100 | Train Loss: 0.394802 | LR: 0.0500 | Batch:  32 | Patience:  70/500 | Rate: 100.4 iter/s | Time: 40.85s\n",
      "Iter  4120 | Train Loss: 0.394885 | LR: 0.0500 | Batch:  32 | Patience:  74/500 | Rate: 100.3 iter/s | Time: 41.07s\n",
      "Iter  4140 | Train Loss: 0.395296 | LR: 0.0500 | Batch:  32 | Patience:  78/500 | Rate: 100.3 iter/s | Time: 41.29s\n",
      "Iter  4160 | Train Loss: 0.395446 | LR: 0.0500 | Batch:  32 | Patience:  82/500 | Rate: 100.2 iter/s | Time: 41.51s\n",
      "Iter  4180 | Train Loss: 0.395732 | LR: 0.0500 | Batch:  32 | Patience:  86/500 | Rate: 100.2 iter/s | Time: 41.73s\n",
      "Iter  4200 | Train Loss: 0.395227 | LR: 0.0500 | Batch:  32 | Patience:  90/500 | Rate: 100.1 iter/s | Time: 41.95s\n",
      "Iter  4220 | Train Loss: 0.395183 | LR: 0.0500 | Batch:  32 | Patience:  94/500 | Rate: 100.1 iter/s | Time: 42.17s\n",
      "Iter  4240 | Train Loss: 0.395227 | LR: 0.0500 | Batch:  32 | Patience:  98/500 | Rate: 100.1 iter/s | Time: 42.38s\n",
      "Iter  4260 | Train Loss: 0.395188 | LR: 0.0500 | Batch:  32 | Patience: 102/500 | Rate: 100.1 iter/s | Time: 42.57s\n",
      "Iter  4280 | Train Loss: 0.395044 | LR: 0.0500 | Batch:  32 | Patience: 106/500 | Rate: 100.1 iter/s | Time: 42.77s\n",
      "Iter  4300 | Train Loss: 0.395032 | LR: 0.0500 | Batch:  32 | Patience: 110/500 | Rate: 100.1 iter/s | Time: 42.96s\n",
      "Iter  4320 | Train Loss: 0.395133 | LR: 0.0500 | Batch:  32 | Patience: 114/500 | Rate: 100.1 iter/s | Time: 43.16s\n",
      "Iter  4340 | Train Loss: 0.395116 | LR: 0.0500 | Batch:  32 | Patience: 118/500 | Rate: 100.1 iter/s | Time: 43.35s\n",
      "Iter  4360 | Train Loss: 0.395265 | LR: 0.0500 | Batch:  32 | Patience: 122/500 | Rate: 100.1 iter/s | Time: 43.55s\n",
      "Iter  4380 | Train Loss: 0.395331 | LR: 0.0500 | Batch:  32 | Patience: 126/500 | Rate: 100.1 iter/s | Time: 43.74s\n",
      "Iter  4400 | Train Loss: 0.395354 | LR: 0.0500 | Batch:  32 | Patience: 130/500 | Rate: 100.0 iter/s | Time: 44.00s\n",
      "Iter  4420 | Train Loss: 0.395371 | LR: 0.0500 | Batch:  32 | Patience: 134/500 | Rate: 100.0 iter/s | Time: 44.20s\n",
      "Iter  4440 | Train Loss: 0.395302 | LR: 0.0500 | Batch:  32 | Patience: 138/500 | Rate: 100.0 iter/s | Time: 44.40s\n",
      "Iter  4460 | Train Loss: 0.395089 | LR: 0.0500 | Batch:  32 | Patience: 142/500 | Rate: 100.0 iter/s | Time: 44.59s\n",
      "Iter  4480 | Train Loss: 0.394833 | LR: 0.0500 | Batch:  32 | Patience: 146/500 | Rate: 100.0 iter/s | Time: 44.78s\n",
      "Iter  4500 | Train Loss: 0.394870 | LR: 0.0500 | Batch:  32 | Patience: 150/500 | Rate: 100.0 iter/s | Time: 45.01s\n",
      "Iter  4520 | Train Loss: 0.394984 | LR: 0.0500 | Batch:  32 | Patience: 154/500 | Rate: 99.9 iter/s | Time: 45.23s\n",
      "Iter  4540 | Train Loss: 0.394933 | LR: 0.0500 | Batch:  32 | Patience: 158/500 | Rate: 99.9 iter/s | Time: 45.44s\n",
      "Iter  4560 | Train Loss: 0.395165 | LR: 0.0500 | Batch:  32 | Patience: 162/500 | Rate: 99.9 iter/s | Time: 45.66s\n",
      "Iter  4580 | Train Loss: 0.394762 | LR: 0.0500 | Batch:  32 | Patience: 166/500 | Rate: 99.9 iter/s | Time: 45.86s\n",
      "Iter  4600 | Train Loss: 0.394793 | LR: 0.0500 | Batch:  32 | Patience: 170/500 | Rate: 99.9 iter/s | Time: 46.05s\n",
      "Iter  4620 | Train Loss: 0.394919 | LR: 0.0500 | Batch:  32 | Patience: 174/500 | Rate: 99.9 iter/s | Time: 46.25s\n",
      "Iter  4640 | Train Loss: 0.395290 | LR: 0.0500 | Batch:  32 | Patience: 178/500 | Rate: 99.9 iter/s | Time: 46.44s\n",
      "Iter  4660 | Train Loss: 0.394801 | LR: 0.0500 | Batch:  32 | Patience: 182/500 | Rate: 99.9 iter/s | Time: 46.66s\n",
      "Iter  4680 | Train Loss: 0.394456 | LR: 0.0500 | Batch:  32 | Patience: 186/500 | Rate: 99.9 iter/s | Time: 46.85s\n",
      "Iter  4700 | Train Loss: 0.394822 | LR: 0.0500 | Batch:  32 | Patience: 190/500 | Rate: 99.9 iter/s | Time: 47.06s\n",
      "Iter  4720 | Train Loss: 0.394302 | LR: 0.0500 | Batch:  32 | Patience: 194/500 | Rate: 99.8 iter/s | Time: 47.28s\n",
      "Iter  4740 | Train Loss: 0.394356 | LR: 0.0500 | Batch:  32 | Patience: 198/500 | Rate: 99.8 iter/s | Time: 47.48s\n",
      "Iter  4760 | Train Loss: 0.394749 | LR: 0.0500 | Batch:  32 | Patience: 202/500 | Rate: 99.8 iter/s | Time: 47.68s\n",
      "Iter  4780 | Train Loss: 0.394580 | LR: 0.0500 | Batch:  32 | Patience: 206/500 | Rate: 99.8 iter/s | Time: 47.89s\n",
      "Iter  4800 | Train Loss: 0.394558 | LR: 0.0500 | Batch:  32 | Patience: 210/500 | Rate: 99.8 iter/s | Time: 48.10s\n",
      "Iter  4820 | Train Loss: 0.394451 | LR: 0.0500 | Batch:  32 | Patience: 214/500 | Rate: 99.8 iter/s | Time: 48.30s\n",
      "Iter  4840 | Train Loss: 0.394610 | LR: 0.0500 | Batch:  32 | Patience: 218/500 | Rate: 99.8 iter/s | Time: 48.50s\n",
      "Iter  4860 | Train Loss: 0.394543 | LR: 0.0500 | Batch:  32 | Patience: 222/500 | Rate: 99.8 iter/s | Time: 48.69s\n",
      "Iter  4880 | Train Loss: 0.394563 | LR: 0.0500 | Batch:  32 | Patience: 226/500 | Rate: 99.8 iter/s | Time: 48.89s\n",
      "Iter  4900 | Train Loss: 0.394814 | LR: 0.0500 | Batch:  32 | Patience: 230/500 | Rate: 99.8 iter/s | Time: 49.09s\n",
      "Iter  4920 | Train Loss: 0.394389 | LR: 0.0500 | Batch:  32 | Patience: 234/500 | Rate: 99.8 iter/s | Time: 49.30s\n",
      "Iter  4940 | Train Loss: 0.394574 | LR: 0.0500 | Batch:  32 | Patience: 238/500 | Rate: 99.8 iter/s | Time: 49.49s\n",
      "Iter  4960 | Train Loss: 0.394757 | LR: 0.0500 | Batch:  32 | Patience: 242/500 | Rate: 99.8 iter/s | Time: 49.69s\n",
      "Iter  4980 | Train Loss: 0.394664 | LR: 0.0500 | Batch:  32 | Patience: 246/500 | Rate: 99.8 iter/s | Time: 49.89s\n",
      "Iter  5000 | Train Loss: 0.394441 | LR: 0.0500 | Batch:  32 | Patience: 250/500 | Rate: 99.8 iter/s | Time: 50.09s\n",
      "Iter  5020 | Train Loss: 0.394543 | LR: 0.0500 | Batch:  32 | Patience: 254/500 | Rate: 99.8 iter/s | Time: 50.29s\n",
      "Iter  5040 | Train Loss: 0.394837 | LR: 0.0500 | Batch:  32 | Patience: 258/500 | Rate: 99.8 iter/s | Time: 50.49s\n",
      "Iter  5060 | Train Loss: 0.394540 | LR: 0.0500 | Batch:  32 | Patience: 262/500 | Rate: 99.8 iter/s | Time: 50.69s\n",
      "Iter  5080 | Train Loss: 0.394616 | LR: 0.0500 | Batch:  32 | Patience: 266/500 | Rate: 99.8 iter/s | Time: 50.89s\n",
      "Iter  5100 | Train Loss: 0.395257 | LR: 0.0500 | Batch:  32 | Patience: 270/500 | Rate: 99.8 iter/s | Time: 51.09s\n",
      "Iter  5120 | Train Loss: 0.394847 | LR: 0.0500 | Batch:  32 | Patience: 274/500 | Rate: 99.8 iter/s | Time: 51.28s\n",
      "Iter  5140 | Train Loss: 0.394810 | LR: 0.0500 | Batch:  32 | Patience: 278/500 | Rate: 99.8 iter/s | Time: 51.48s\n",
      "Iter  5160 | Train Loss: 0.394608 | LR: 0.0500 | Batch:  32 | Patience: 282/500 | Rate: 99.9 iter/s | Time: 51.68s\n",
      "Iter  5180 | Train Loss: 0.394862 | LR: 0.0500 | Batch:  32 | Patience: 286/500 | Rate: 99.9 iter/s | Time: 51.87s\n",
      "Iter  5200 | Train Loss: 0.394397 | LR: 0.0500 | Batch:  32 | Patience: 290/500 | Rate: 99.9 iter/s | Time: 52.07s\n",
      "Iter  5220 | Train Loss: 0.394524 | LR: 0.0500 | Batch:  32 | Patience: 294/500 | Rate: 99.8 iter/s | Time: 52.28s\n",
      "Iter  5240 | Train Loss: 0.394982 | LR: 0.0500 | Batch:  32 | Patience: 298/500 | Rate: 99.8 iter/s | Time: 52.51s\n",
      "Iter  5260 | Train Loss: 0.394795 | LR: 0.0500 | Batch:  32 | Patience: 302/500 | Rate: 99.8 iter/s | Time: 52.71s\n",
      "Iter  5280 | Train Loss: 0.394803 | LR: 0.0500 | Batch:  32 | Patience: 306/500 | Rate: 99.8 iter/s | Time: 52.91s\n",
      "Iter  5300 | Train Loss: 0.395223 | LR: 0.0500 | Batch:  32 | Patience: 310/500 | Rate: 99.8 iter/s | Time: 53.11s\n",
      "Iter  5320 | Train Loss: 0.394834 | LR: 0.0500 | Batch:  32 | Patience: 314/500 | Rate: 99.8 iter/s | Time: 53.32s\n",
      "Iter  5340 | Train Loss: 0.394469 | LR: 0.0500 | Batch:  32 | Patience: 318/500 | Rate: 99.8 iter/s | Time: 53.52s\n",
      "Iter  5360 | Train Loss: 0.394445 | LR: 0.0500 | Batch:  32 | Patience: 322/500 | Rate: 99.8 iter/s | Time: 53.71s\n",
      "Iter  5380 | Train Loss: 0.395891 | LR: 0.0500 | Batch:  32 | Patience: 326/500 | Rate: 99.8 iter/s | Time: 53.91s\n",
      "Iter  5400 | Train Loss: 0.395563 | LR: 0.0500 | Batch:  32 | Patience: 330/500 | Rate: 99.8 iter/s | Time: 54.11s\n",
      "Iter  5420 | Train Loss: 0.396078 | LR: 0.0500 | Batch:  32 | Patience: 334/500 | Rate: 99.8 iter/s | Time: 54.30s\n",
      "Iter  5440 | Train Loss: 0.394881 | LR: 0.0500 | Batch:  32 | Patience: 338/500 | Rate: 99.8 iter/s | Time: 54.50s\n",
      "Iter  5460 | Train Loss: 0.395119 | LR: 0.0500 | Batch:  32 | Patience: 342/500 | Rate: 99.8 iter/s | Time: 54.70s\n",
      "Iter  5480 | Train Loss: 0.394704 | LR: 0.0500 | Batch:  32 | Patience: 346/500 | Rate: 99.8 iter/s | Time: 54.89s\n",
      "Iter  5500 | Train Loss: 0.394542 | LR: 0.0500 | Batch:  32 | Patience: 350/500 | Rate: 99.8 iter/s | Time: 55.09s\n",
      "Iter  5520 | Train Loss: 0.394834 | LR: 0.0500 | Batch:  32 | Patience: 354/500 | Rate: 99.8 iter/s | Time: 55.29s\n",
      "Iter  5540 | Train Loss: 0.394563 | LR: 0.0500 | Batch:  32 | Patience: 358/500 | Rate: 99.8 iter/s | Time: 55.50s\n",
      "Iter  5560 | Train Loss: 0.394538 | LR: 0.0500 | Batch:  32 | Patience: 362/500 | Rate: 99.8 iter/s | Time: 55.69s\n",
      "Iter  5580 | Train Loss: 0.394478 | LR: 0.0500 | Batch:  32 | Patience: 366/500 | Rate: 99.8 iter/s | Time: 55.89s\n",
      "Iter  5600 | Train Loss: 0.394614 | LR: 0.0500 | Batch:  32 | Patience: 370/500 | Rate: 99.8 iter/s | Time: 56.09s\n",
      "Iter  5620 | Train Loss: 0.394815 | LR: 0.0500 | Batch:  32 | Patience: 374/500 | Rate: 99.9 iter/s | Time: 56.28s\n",
      "Iter  5640 | Train Loss: 0.394898 | LR: 0.0500 | Batch:  32 | Patience: 378/500 | Rate: 99.9 iter/s | Time: 56.48s\n",
      "Iter  5660 | Train Loss: 0.395268 | LR: 0.0500 | Batch:  32 | Patience: 382/500 | Rate: 99.9 iter/s | Time: 56.68s\n",
      "Iter  5680 | Train Loss: 0.395148 | LR: 0.0500 | Batch:  32 | Patience: 386/500 | Rate: 99.9 iter/s | Time: 56.88s\n",
      "Iter  5700 | Train Loss: 0.394888 | LR: 0.0500 | Batch:  32 | Patience: 390/500 | Rate: 99.9 iter/s | Time: 57.08s\n",
      "Iter  5720 | Train Loss: 0.394938 | LR: 0.0500 | Batch:  32 | Patience: 394/500 | Rate: 99.9 iter/s | Time: 57.27s\n",
      "Iter  5740 | Train Loss: 0.394951 | LR: 0.0500 | Batch:  32 | Patience: 398/500 | Rate: 99.9 iter/s | Time: 57.48s\n",
      "Iter  5760 | Train Loss: 0.394436 | LR: 0.0500 | Batch:  32 | Patience: 402/500 | Rate: 99.9 iter/s | Time: 57.67s\n",
      "Iter  5780 | Train Loss: 0.394388 | LR: 0.0500 | Batch:  32 | Patience: 406/500 | Rate: 99.9 iter/s | Time: 57.87s\n",
      "Iter  5800 | Train Loss: 0.394498 | LR: 0.0500 | Batch:  32 | Patience: 410/500 | Rate: 99.9 iter/s | Time: 58.07s\n",
      "Iter  5820 | Train Loss: 0.394435 | LR: 0.0500 | Batch:  32 | Patience: 414/500 | Rate: 99.9 iter/s | Time: 58.27s\n",
      "Iter  5840 | Train Loss: 0.394488 | LR: 0.0500 | Batch:  32 | Patience: 418/500 | Rate: 99.9 iter/s | Time: 58.48s\n",
      "Iter  5860 | Train Loss: 0.394323 | LR: 0.0500 | Batch:  32 | Patience: 422/500 | Rate: 99.9 iter/s | Time: 58.67s\n",
      "Iter  5880 | Train Loss: 0.394280 | LR: 0.0500 | Batch:  32 | Patience: 426/500 | Rate: 99.9 iter/s | Time: 58.87s\n",
      "Iter  5900 | Train Loss: 0.394316 | LR: 0.0500 | Batch:  32 | Patience: 430/500 | Rate: 99.9 iter/s | Time: 59.06s\n",
      "Iter  5920 | Train Loss: 0.394469 | LR: 0.0500 | Batch:  32 | Patience: 434/500 | Rate: 99.9 iter/s | Time: 59.26s\n",
      "Iter  5940 | Train Loss: 0.394541 | LR: 0.0500 | Batch:  32 | Patience: 438/500 | Rate: 99.8 iter/s | Time: 59.49s\n",
      "Iter  5960 | Train Loss: 0.394704 | LR: 0.0500 | Batch:  32 | Patience: 442/500 | Rate: 99.9 iter/s | Time: 59.68s\n",
      "Iter  5980 | Train Loss: 0.395628 | LR: 0.0500 | Batch:  32 | Patience: 446/500 | Rate: 99.9 iter/s | Time: 59.89s\n",
      "Iter  6000 | Train Loss: 0.395592 | LR: 0.0500 | Batch:  32 | Patience: 450/500 | Rate: 99.9 iter/s | Time: 60.09s\n",
      "Iter  6020 | Train Loss: 0.395358 | LR: 0.0500 | Batch:  32 | Patience: 454/500 | Rate: 99.9 iter/s | Time: 60.29s\n",
      "Iter  6040 | Train Loss: 0.394819 | LR: 0.0500 | Batch:  32 | Patience: 458/500 | Rate: 99.8 iter/s | Time: 60.50s\n",
      "Iter  6060 | Train Loss: 0.394784 | LR: 0.0500 | Batch:  32 | Patience: 462/500 | Rate: 99.8 iter/s | Time: 60.70s\n",
      "Iter  6080 | Train Loss: 0.394841 | LR: 0.0500 | Batch:  32 | Patience: 466/500 | Rate: 99.8 iter/s | Time: 60.91s\n",
      "Iter  6100 | Train Loss: 0.394659 | LR: 0.0500 | Batch:  32 | Patience: 470/500 | Rate: 99.8 iter/s | Time: 61.11s\n",
      "Iter  6120 | Train Loss: 0.394999 | LR: 0.0500 | Batch:  32 | Patience: 474/500 | Rate: 99.8 iter/s | Time: 61.32s\n",
      "Iter  6140 | Train Loss: 0.395054 | LR: 0.0500 | Batch:  32 | Patience: 478/500 | Rate: 99.8 iter/s | Time: 61.53s\n",
      "Iter  6160 | Train Loss: 0.394804 | LR: 0.0500 | Batch:  32 | Patience: 482/500 | Rate: 99.7 iter/s | Time: 61.77s\n",
      "Iter  6180 | Train Loss: 0.394564 | LR: 0.0500 | Batch:  32 | Patience: 486/500 | Rate: 99.7 iter/s | Time: 61.98s\n",
      "Iter  6200 | Train Loss: 0.394584 | LR: 0.0500 | Batch:  32 | Patience: 490/500 | Rate: 99.7 iter/s | Time: 62.19s\n",
      "Iter  6220 | Train Loss: 0.394576 | LR: 0.0500 | Batch:  32 | Patience: 494/500 | Rate: 99.7 iter/s | Time: 62.40s\n",
      "Iter  6240 | Train Loss: 0.394388 | LR: 0.0500 | Batch:  32 | Patience: 498/500 | Rate: 99.6 iter/s | Time: 62.68s\n",
      "\n",
      "Early stopping triggered at iteration 6246\n",
      "No improvement in loss for 500 checks (threshold: 1e-06)\n",
      "Final loss: 0.394295\n",
      "Total training time: 62.74s\n",
      "Average time per iteration: 0.0100s\n",
      "Total iterations: 6246\n",
      "Approximate epochs: 0.46\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SGD Iterations comparison saved!\n",
      "{'final_loss': np.float64(0.3989799090076122), 'final_iteration': 5296, 'training_time': 53.9142701625824, 'learning_rate': 0.01, 'batch_size': 1}\n",
      "{'final_loss': np.float64(0.4173632583900321), 'final_iteration': 3846, 'training_time': 38.53975009918213, 'learning_rate': 0.05, 'batch_size': 1}\n",
      "{'final_loss': np.float64(0.3942298478604509), 'final_iteration': 12141, 'training_time': 121.82505941390991, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "{'final_loss': np.float64(0.3942945867163068), 'final_iteration': 6246, 'training_time': 62.742690563201904, 'learning_rate': 0.05, 'batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "# SGD Iterations (Mini-batch) - Loss Convergence Comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "sgd_iter_results: List[Dict[str, float]] = []\n",
    "\n",
    "sgd_iter_configs = [\n",
    "    {\"lr\": 0.01, \"batch_size\": 1, \"color\": \"blue\", \"label\": \"LR 0.01, Batch 1\"},\n",
    "    {\"lr\": 0.05, \"batch_size\": 1, \"color\": \"orange\", \"label\": \"LR 0.05, Batch 1\"},\n",
    "    {\"lr\": 0.01, \"batch_size\": 32, \"color\": \"green\", \"label\": \"LR 0.01, Batch 32\"},\n",
    "    {\"lr\": 0.05, \"batch_size\": 32, \"color\": \"red\", \"label\": \"LR 0.05, Batch 32\"},\n",
    "]\n",
    "\n",
    "for config in sgd_iter_configs:\n",
    "    model = WeightedLogisticRegression()\n",
    "    optimizer = SGDIterationsOptimizer(\n",
    "        learning_rate=config[\"lr\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        early_stop_patience=500,\n",
    "        max_iterations=100000,\n",
    "        log_interval=20,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    result = optimizer.optimize(\n",
    "        model, X_train_scaled, y_train,\n",
    "        weights_train=weights_train\n",
    "    )\n",
    "    data: Dict[str, float] = {}\n",
    "    data[\"final_loss\"] = result[\"final_loss\"]\n",
    "    data[\"final_iteration\"] = result[\"final_iteration\"]\n",
    "    data[\"training_time\"] = result[\"training_time\"]\n",
    "    data[\"learning_rate\"] = result[\"learning_rate\"]\n",
    "    data[\"batch_size\"] = result[\"batch_size\"]\n",
    "    sgd_iter_results.append(data)\n",
    "    \n",
    "    plt.plot(result['iteration_history'], result['loss_history'], \n",
    "             color=config[\"color\"], linewidth=2, label=config[\"label\"])\n",
    "plt.xlabel(\"Iteration\", fontsize=12)\n",
    "plt.ylabel(\"Training Loss\", fontsize=12)\n",
    "plt.title(\n",
    "    \"SGD Iterations (Mini-batch) - Loss Convergence\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(images_dir / \"sgd_iterations_convergence.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"✅ SGD Iterations comparison saved!\")\n",
    "for result in sgd_iter_results:\n",
    "\tprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0f9a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: Display all saved images\n",
    "print(\"=\" * 80)\n",
    "print(\"VISUALIZATION SUMMARY - WEIGHTED LOSS OPTIMIZATION COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"📁 All images saved to: {images_dir}\")\n",
    "print(\"📊 Generated visualizations:\")\n",
    "print(\"   1. sgd_fixed_lr_convergence.png - SGD with different learning rates\")\n",
    "print(\"   2. sgd_backtracking_convergence.png - SGD Backtracking with different initial rates\")  \n",
    "print(\"   3. sgd_iterations_convergence.png - SGD Iterations with different batch sizes\")\n",
    "print(\"   4. agd_fixed_lr_convergence.png - AGD with different LR/momentum combinations\")\n",
    "print(\"   5. agd_backtracking_convergence.png - AGD Backtracking with different initial rates\")\n",
    "print(\"\\n✅ All optimization methods visualized with weighted loss function!\")\n",
    "print(f\"✅ Using 'minute' field ({weights_train.min():.0f}-{weights_train.max():.0f}) as sample weights\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
